[{
  "id": "KITTI",
  "href": "http://www.cvlibs.net/datasets/kitti/",
  "size_hours": "6",
  "size_storage": "180",
  "frames": "-",
  "numberOfScenes": "50",
  "samplingRate": "10",
  "lengthOfScenes": "-",
  "sensors": "camera, lidar, gps/imu",
  "sensorDetail": "2 greyscale cameras 1.4 MP, 2 color cameras 1.4 MP, 1 lidar 64 beams 360° 10Hz, 1 inertial and GPS navigation system",
  "benchmark": "stereo, optical flow, visual odometry, slam, 3d object detection, 3d object tracking",
  "annotations": "3d bounding boxes",
  "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0",
  "relatedDatasets": "Semantic KITTI, KITTI-360",
  "publishDate": "2012-03-01",
  "lastUpdate": "2021-02-01",
  "paperTitle": "Vision meets Robotics: The KITTI Dataset",
  "relatedPaper": "http://www.cvlibs.net/publications/Geiger2013IJRR.pdf",
  "location": "Karlsruhe, Germany",
  "rawData": "Yes",
  "DOI": "10.1177%2F0278364913491297"
},
{
"id": "nuScenes",
"href": "https://www.nuscenes.org/",
"size_hours": "15",
"size_storage": "-",
"frames": "1400000",
"numberOfScenes": "1000",
"samplingRate": "-",
"lengthOfScenes": "20",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "1x lidar 32 channels 360° 20Hz, 5x long range radar 13Hz, 6x camera 1600x1200 12Hz, 1x gps/imu 1000Hz",
"benchmark": "3d object detection, tracking, trajectory (prediction), lidar segmentation, panoptic segmentation & tracking",
"annotations": "semantic category, attributes, 3d bounding boxes ",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",
"relatedDatasets": "nuImages",
"publishDate": "2019-03-01",
"lastUpdate": "2020-12-01",
"paperTitle": "nuScenes: A multimodal dataset for autonomous driving",
"relatedPaper": "https://arxiv.org/pdf/1903.11027.pdf",
"location": "Boston, USA and Singapore",
"rawData": "Yes",
"DOI": "10.1109/cvpr42600.2020.01164"
},
{
"id": "Oxford Robot Car", 
"href": "https://robotcar-dataset.robots.ox.ac.uk/",
"size_hours": "210",
"size_storage": "23150",
"frames": "-",
"numberOfScenes": "100",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, ins/gps",
"sensorDetail": "1x camera Bumblebee XB3 1280x960x3 16Hz, 3x camera Grasshopper2 1024x1024 12Hz, 2x lidar SICK LMS-151 270° 50Hz, 1x lidar SICK LD-MRS 90° 4 plane 12.5Hz, 1x NovAtel SPAN-CPT ALIGN 50Hz GPS+INS",
"benchmark": "-",
"annotations": "-",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International",
"relatedDatasets": "Oxford Radar Robot Car",
"publishDate": "2016-11-01",
"lastUpdate": "2020-02-01",
"paperTitle": "1 Year, 1000km: The Oxford RobotCar Dataset",
"relatedPaper": "https://robotcar-dataset.robots.ox.ac.uk/images/robotcar_ijrr.pdf",
"location": "Oxford, UK",
"rawData": "Yes",
"DOI": "10.1177%2F0278364916679498"
},
{
"id": "Waymo Open Perception", 
"href": "https://waymo.com/open/data/perception/",
"size_hours":  "10.83", 
"size_storage": "-",
"frames": "390000",
"numberOfScenes": "1950",
"samplingRate": "10",
"lengthOfScenes": "20",
"sensors": "camera, lidar",
"sensorDetail": "5x cameras (front and sides) 1920x1280 & 1920x1040, 1x mid-range lidar, 4x short-range lidars",
"benchmark": "2d detection, 3d detection, 2d tracking, 3d tracking",
"annotations": "3d bounding boxes (lidar), 2d bounding boxes (camera)",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "Waymo Open Motion",
"publishDate": "2019-08-01",
"lastUpdate": "2020-03-01",
"paperTitle": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
"relatedPaper": "https://arxiv.org/pdf/1912.04838.pdf",
"location": "San Francisco, Mountain View, Los Angeles, Detroit, Seattle and Phoenix, USA",
"rawData": "Yes",
"DOI": "10.1109/CVPR42600.2020.00252"
},
{
"id": "Argoverse Motion Forecasting", 
"href": "https://www.argoverse.org/",
"size_storage": "4.81",
"size_hours": "320",
"frames": "16227850",
"numberOfScenes": "324557",
"samplingRate": "10",
"lengthOfScenes": "5",
"sensors": "camera, lidar, gps",
"sensorDetail": "2x lidar 32 beam 40° 10Hz, 7x ring cameras 1920x1200 combined 360° 30Hz, 2x front-view facing stereo cameras 0.2986m baseline 2056x2464 5Hz",
"benchmark": "forecasting",
"annotations": "semantic vector map, rasterized map, trajectories",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",
"relatedDatasets": "Argoverse 3D Tracking",
"publishDate": "2019-06-01",
"lastUpdate": "-",
"paperTitle": "Argoverse: 3D Tracking and Forecasting with Rich Maps",
"relatedPaper": "https://arxiv.org/pdf/1911.02620.pdf",
"location": "Miami and Pittsburgh, USA",
"rawData": "No",
"DOI": "10.1109/CVPR.2019.00895"
},
{
"id": "Argoverse 3D Tracking", 
"href": "https://www.argoverse.org/",
"size_storage": "254.4",
"size_hours": "1",
"frames": "44000",
"numberOfScenes": "113",
"samplingRate": "30",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps",
"sensorDetail": "2x lidar 40° 10Hz, 7x ring cameras 1920x1200 combined 360° 30Hz, 2x front-view facing stereo cameras 2056x2464 5Hz",
"benchmark": "tracking",
"annotations": "semantic vector map, rasterized map, 3d bounding boxes",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",
"relatedDatasets": "Argoverse Motion Forecasting",
"publishDate": "2019-06-01",
"lastUpdate": "-",
"paperTitle": "Argoverse: 3D Tracking and Forecasting with Rich Maps",
"relatedPaper": "https://arxiv.org/pdf/1911.02620.pdf",
"location": "Miami and Pittsburgh, USA",
"rawData": "Yes",
"DOI": "10.1109/CVPR.2019.00895"
},
{
"id": "Semantic KITTI", 
"href": "http://www.semantic-kitti.org/",
"size_storage": "-",
"size_hours": "-",
"frames": "43552", 
"numberOfScenes": "21",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "lidar",
"sensorDetail": "Velodyne HDL-64E from sequences of the odometry 'benchmark' of the KITTI Vision Benchmark with 360° view",
"benchmark": "semantic segmentation, panoptic segmentation, 4D panoptic segmentation, moving object segmentation, semantic scene completion",
"annotations": "semantic segmentation",
"licensing": "Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) ",
"relatedDatasets": "KITTI",
"publishDate": "2019-07-01",
"lastUpdate": "2021-02-01",
"relatedPaper": "https://arxiv.org/abs/1904.01416.pdf",
"location": "Karlsruhe, Germany",
"rawData": "No"
},
{
"id": "ApolloScape", 
"href": "http://apolloscape.auto/",
"size_hours": "100", 
"size_storage": "-",
"frames": "143906",
"numberOfScenes": "-",
"samplingRate": "30",
"lengthOfScenes": "-",
"sensors": "camera, lidar, imu/gnss",
"sensorDetail": "2x VUX-1HA laser scanners 360°, 1x VMX-CS6 camera system, 1x measuring head with gnss/imu, 2x high frontal cameras 3384 ×2710",
"benchmark": "2d image parsing, 3d car instance understanding, landmark segmentation, self-localization, trajectory prediction, 3d detection, 3d tracking, stereo",
"annotations": "high density 3d point cloud map, per-pixel, per-frame semantic image label, lane mark label semantic instance segmentation, geo-tagged",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "-",
"publishDate": "2018-03-01",
"lastUpdate": "2020-09-01",
"paperTitle": "The ApolloScape Open Dataset for Autonomous Driving and its Application",
"relatedPaper": "https://arxiv.org/pdf/1803.06184.pdf",
"location": "Beijing, Shanghai and Shenzhen, China",
"rawData": "Yes"
},
{
"id": "BDD100k", 
"href": "https://www.bdd100k.com/",
"size_storage": "1800",
"size_hours": "1111",
"frames": "120000000",
"numberOfScenes": "100000",
"samplingRate": "30",
"lengthOfScenes": "40",
"sensors": "camera, gps/imu",
"sensorDetail": "crowd-sourced therefore no fixed setup, camera (720p) and gps/imu",
"benchmark": "object detection, instance segmentation, multiple object tracking, segmentation tracking, semantic segmentation, lane marking, drivable area, image tagging, imitation learning, domain adaption",
"annotations": "bounding boxes, instance segmentation, semantic segmentation, box tracking, semantic tracking, drivable area",
"licensing": "BSD 3-Clause",
"relatedDatasets": "-",
"publishDate": "2020-04-01",
"lastUpdate": "-",
"paperTitle": "BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning",
"relatedPaper": "https://arxiv.org/pdf/1805.04687.pdf",
"location": "New York, Berkeley, San Francisco and Bay Area, USA",
"rawData": "Yes"
},
{
"id": "WildDash", 
"href": "https://wilddash.cc/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "156",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "various sources, e.g. YouTube",
"benchmark": "semantic segmentation, instance segmentation, panoptic segmentation",
"annotations": "semantic segmentation, instance segmentation",
"licensing": "CC-BY-NC 4.0 ",
"relatedDatasets": "-",
"publishDate": "2018-02-01",
"lastUpdate": "2020-06-01",
"relatedPaper": "https://openaccess.thecvf.com/content_ECCV_2018/papers/Oliver_Zendel_WildDash_-_Creating_ECCV_2018_paper.pdf",
"location": "All over the world",
"rawData": "Yes",
"DOI": "10.1007/978-3-030-01231-1_25"
},
{
"id": "Lyft Level5 Prediction",
"href": "https://level-5.global/data/prediction/",
"size_hours": "1118",
"size_storage": "-",
"frames": "42500000",
"numberOfScenes": "170000",
"samplingRate": "10",
"lengthOfScenes": "25",
"sensors": "camera, lidar, radar",
"sensorDetail": "7 cameras with 360° view, 3 lidars with 40-64 channels at 10Hz, 5 radars",
"benchmark": "-",
"annotations": "semantic map \"annotations\", trajectories",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 (CC-BY-NC-SA-4.0)",
"relatedDatasets": "Lyft Level5 Perception",
"publishDate": "2020-06-01",
"lastUpdate": "-",
"relatedPaper": "https://arxiv.org/pdf/2006.14480v1.pdf",
"location": "Palo Alto, USA",
"rawData": "No"
},
{
"id":   "Cityscapes 3D",
"href": "https://www.cityscapes-dataset.com/",
"size_hours": "-",
"size_storage": "63.141",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "17",
"lengthOfScenes": "1.8",
"sensors": "camera, gps, thermometer",
"sensorDetail": "stereo cameras 22 cm baseline 17Hz, odometry from in-vehicle \"sensors\" & outs\"id\"e temperature & GPS tracks",
"benchmark": "pixel-level semantic labeling, instance-level semantic labeling, panoptic semantic sabeling 3d vehicle detection",
"annotations": "dense semantic segmentation, instance segmentation for vehicles & people, 3d bounding boxes",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "-",
"publishDate": "2016-02-01",
"lastUpdate": "2020-10-01",
"relatedPaper": "https://arxiv.org/pdf/2006.07864.pdf",
"location": "50 cities in Germany and neighboring countries",
"rawData": "Yes"
},
{
"id": "Lyft Level5 Perception",
"href": "https://level-5.global/data/perception/",
"size_hours": "2.5",
"size_storage": "-",
"frames": "-",
"numberOfScenes": "366",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "-",
"benchmark": "-",
"annotations": "3d bounding boxes, rasterised road geometry",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 (CC-BY-NC-SA-4.0)",
"relatedDatasets": "Lyft Level5 Prediction",
"publishDate": "2019-07-01",
"lastUpdate": "-",
"relatedPaper": "https://arxiv.org/pdf/2006.14480v1.pdf",
"location": "Palo Alto, USA",
"rawData": "Yes"
},
{
"id": "nuImages",
"href": "https://www.nuscenes.org/nuimages",
"size_hours": "150",
"size_storage": "-",
"frames": "1200000",
"numberOfScenes": "93000",
"samplingRate": "2",
"lengthOfScenes": "-",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "1x lidar 32 channels 360° 20Hz, 5x long range radar 13Hz, 6x camera 1600x1200 12Hz, 1x gps/imu 1000Hz",
"benchmark": "-",
"annotations": "instance masks, 2d bounding boxes, semantic segmentation masks, attribute annotations",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",
"relatedDatasets": "nuScenes",
"publishDate": "2020-07-01",
"lastUpdate": "-",
"location": "Boston, USA and Singapore",
"rawData": "Yes",
"DOI": "10.1109/cvpr42600.2020.01164"
},
{
"id": "PandaSet",
"href": "https://pandaset.org/",
"size_hours": "0.23",
"size_storage": "-",
"frames": "48000",
"numberOfScenes": "103",
"samplingRate": "-",
"lengthOfScenes": "8",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "5x wide angle cameras 1920x1080 10Hz, 1x long focus camera 1920x1080 10Hz, 1x mechanical spinning LiDAR 64 channels 360° 10Hz, 1x forward-facing LiDAR 150 channels 60° 10Hz1x mechanical spinning LiDAR, 1x forward-facing LiDAR, 6x cameras, on-board GPS/IMU",
"benchmark": "-",
"annotations": "3d bounding boxes, attributes, point cloud segmentation ",
"licensing": "Creative Commons Attribution 4.0 International Public (CC BY 4.0)",
"relatedDatasets": "-",
"publishDate": "2020-04-01",
"lastUpdate": "-",
"location": "San Francisco and El Camina Real, USA",
"rawData": "Yes"
},
{
"id": "Waymo Open Motion",
"href": "https://waymo.com/open/data/motion/",
"size_hours":  "574",
"size_storage": "-",
"frames": "20670800",
"numberOfScenes": "103354",
"samplingRate": "10",
"lengthOfScenes": "20",
"sensors": "camera, lidar",
"sensorDetail": "5x cameras, 5x lidar, ",
"benchmark": "motion prediction, interaction prediction",
"annotations": "3d bounding boxes, 3d hd map information",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "Waymo Open Perception",
"publishDate": "2021-03-01",
"lastUpdate": "2021-09-01",
"relatedPaper": "https://arxiv.org/pdf/2104.10133.pdf",
"location": "San Francisco, Mountain View, Los Angeles, Detroit, Seattle and Phoenix, USA",
"rawData": "No"
},
{
"id": "openDD",
"href": "https://l3pilot.eu/data/opendd",
"size_storage": "-",
"size_hours": "62.7",
"frames": "6771600",
"numberOfScenes": "501",
"samplingRate": "30",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "DJI Phantom 4 3840×2160 camera drone",
"benchmark": "trajectory predictions",
"annotations": "2d bounding boxes, trajectories",
"licensing": "Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) ",
"relatedDatasets": "-",
"publishData": "2020-09-01",
"lastUpdate": "-",
"relatedPaper": "https://arxiv.org/pdf/2007.08463.pdf",
"location": "Wolfsburg and Ingolstadt, Germany",
"rawData": "Yes"
},

{
"id": "RoadAnomaly21",
"href": "https://segmentmeifyoucan.com/datasets",
"size_storage": "0.05",
"size_hours": "-",
"frames": "100",
"numberOfScenes": "100",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "images from web resources 2048x1024 & 1280x720",
"benchmark": "anomaly detection",
"annotations": "semantic segmentation",
"licensing": "various, see \"https://github.com/SegmentMeIfYouCan/road-anomaly-\"benchmark\"/blob/master/doc/RoadAnomaly/credits.txt\" for detail",
"relatedDatasets": "RoadObstacle21",
"publishDate": "2021-04-01",
"lastUpdate": "-",
"relatedPaper": "https://arxiv.org/pdf/2104.14812.pdf",
"location": "-",
"rawData": "Yes"

},
{
"id": "Comma2k19",
"href": "https://github.com/commaai/comma2k19",
"size_storage": "100",
"size_hours": "33.65",
"frames": "-",
"numberOfScenes": "2019",
"samplingRate": "-",
"lengthOfScenes": "60",
"sensors": "camera, radar, gnss/imu ",
"sensorDetail": "two different car types, 1x road-facing camera Sony IMX2984 20Hz, 1x gnss u-blox M8 chip5 10Hz, gyro and accelerometer data LSM6DS3 100Hz, magnetometer data AK09911 10Hz",
"benchmark": "-",
"annotations": "-",
"licensing": "MIT",
"relatedDatasets": "-",
"publishDate": "2018-12-01",
"lastUpdate": "-",
"relatedPaper": "http://export.arxiv.org/pdf/1812.05752",
"location": "California's 280 highway, USA",
"rawData": "Yes"
},
{
"id": "KITTI-360",
"href": "http://www.cvlibs.net/datasets/kitti-360/",
"size_storage": "-",
"size_hours": "-",
"frames": "400000",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "2x 180° fisheye camera, 1x 90° perspective stereo camera, 1x Velodyne HDL-64E & SICK LMS 200 laser scanning unit in pushbroom configuration",
"benchmark": "-",
"annotations": "semantic instance segmentation",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0",
"relatedDatasets": "KITTI",
"publishDate": "2015-11-01",
"lastUpdate": "2021-04-01",
"relatedPaper": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Xie_Semantic_Instance_Annotation_CVPR_2016_paper.pdf",
"location": "Karlsruhe, Germany",
"rawData": "Yes",
"DOI": "10.1109/CVPR.2016.401"
},
{
"id": "Fishyscapes",
"href": "https://fishyscapes.com/",
"size_storage":"-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "based on the validation set of Cityscapes overlayed with anomalous objects and the original LostAndFound with extended pixel-wise annotations",
"benchmark": "anomaly detection, semantic segmentation",
"annotations": "semantic segmentation",
"licensing": "-",
"relatedDatasets": "Cityscapes, LostAndFound",
"publishDate": "2019-09-01",
"lastUpdate": "-",
"relatedPaper": "https://arxiv.org/pdf/1904.03215.pdf",
"location": "-",
"rawData": "No"
},
{
"id": "LostAndFound",
"href": "http://www.6d-vision.com/lostandfounddataset",
"size_storage": "-",
"size_hours": "-",
"frames": "21040",
"numberOfScenes": "112",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "stereo camera setup baseline 21cm 2048x1024",
"benchmark": "anomaly detection",
"annotations": "semantic segmentation",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "-",
"publishDate": "2016-09-01",
"lastUpdate": "-",
"relatedPaper": "https://arxiv.org/pdf/1609.04653.pdf",
"location": "-",
"rawData": "Yes"
},
{
"id": "KAIST Multi-Spectral Day/Night",
"href": "http://multispectral.kaist.ac.kr",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "25",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu, thermal camera",
"sensorDetail": "2x PointGrey Flea3 RGB camera 1280 × 960, 1x FLIR A655Sc thermal camera 640x480 50Hz, 1x Velodyne HDL-32E 3D LiDAR 360° 32 beams 10Hz, 1x OXTS RT2002 gps/ins 100Hz",
"benchmark": "object detection, vision sensor enhancement, depth estimation, multi-spectral colorization",
"annotations": "dense depth map, bounding boxes",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0",
"relatedDatasets": "-",
"publishDate": "2017-12",
"lastUpdate": "-",
"relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8293689",
"location": "-",
"rawData": "Yes",
"DOI": "10.1109/TITS.2018.2791533"
},
{
"id": "A2D2",
"href": "https://www.a2d2.audi/a2d2/en.html",
"size_storage": "2300",
"size_hours": "-",
"frames": "433833",
"numberOfScenes": "3",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "5x lidar 16 channels 360° 10Hz, 1x front centre camera 1920x1208 30Hz, 5x surround cameras1920x1208 30Hz, vehicle bus data",
"benchmark": "-",
"annotations": "semantic segmentation, point cloud segmentation, instance segmentation, 3d bounding boxes",
"licensing": "Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0)",
"relatedDatasets": "-",
"publishDate": "2020-04-01",
"lastUpdate": "-",
"relatedPaper": "https://arxiv.org/pdf/2004.06320.pdf",
"location": "Three cities in the south of Germany",
"rawData": "Yes"
},
{
"id": "Caltech Pedestrian",
"href": "http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/",
"size_storage": "-",
"size_hours": "10",
"frames": "1000000",
"numberOfScenes": "137",
"samplingRate": "30",
"lengthOfScenes": "60",
"sensors": "camera",
"sensorDetail": "1x camera 640x480 30Hz",
"benchmark": "pedestrian detection",
"annotations": "bounding boxes",
"licensing": "-",
"relatedDatasets": "-",
"publishDate": "2010-03-01",
"lastUpdate": "2019-01-01",
"paperTitle": "Pedestrian Detection: A Benchmark",
"relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5206631",
"location": "Loa Angeles, USA",
"rawData": "Yes",
"DOI": "10.1109/CVPR.2009.5206631"
},
{
"id": "KAIST Urban",
"href": "https://irap.kaist.ac.kr/dataset/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "18",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "2x Velodyne VLP-16 16 channel lidar 360° 10Hz, 2x SICK LMS-511 1 channel lidar 190° 100Hz, 1x stereo camera 1280x560 10Hz",
"benchmark": "-",
"annotations": "-",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0",
"relatedDatasets": "-",
"publishDate": "2017-09-01",
"lastUpdate": "2019-06-01",
"relatedPaper": "https://irap.kaist.ac.kr/dataset/papers/IJRR2019_dataset.pdf",
"location": "Seoul, Pangyo, Daejeon, Suwon and Dongtan, Korea",
"rawData": "Yes",
"DOI": "10.1177%2F0278364919843996"
},
{
"id": "Udacity",
"href": "https://github.com/udacity/self-driving-car/",
"size_storage": "223",
"size_hours": "10",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "monocular color camera 1920x1200, velodyne 32 lidar, gps/imu",
"benchmark": "-",
"annotations": "2d bounding boxes",
"licensing": "MIT",
"relatedDatasets": "-",
"publishDate": "2016-09-01",
"lastUpdate": "-",
"location": "-",
"rawData": "True"
},
{
"id": "Ford Autonomous Vehicle Dataset",
"href": "https://avdata.ford.com/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "4x HDL-32E Lidars, 4x Flea3 GigE Point Grey Cameras in stereo pairs (front & back) 80° 15Hz,2x Flea3 GigE Point Grey Cameras (sides) 80° 15Hz, 1x Flea3 GigE Point Grey Camera 40° 7Hz, 1x Applanix POS LV gps/imu",
"benchmark": "-",
"annotations": "3d point cloud maps, ground reflectivity map",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International",
"relatedDatasets": "-",
"publishDate": "2020-03-01",
"lastUpdate": "-",
"relatedPaper": "https://s23.q4cdn.com/258866874/files/doc_downloads/2020/03/2003.07969.pdf",
"location": "Michigan, USA",
"rawData": "True",
"DOI": "10.1177/0278364920961451"
},
{
"id": "INTERACTION dataset",
"href": "https://interaction-dataset.com/",
"size_storage": "-",
"size_hours": "16.5",
"frames": "594588",
"numberOfScenes": "-",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "drones & traffic cameras 3840x2160 30Hz downscaled to 10Hz",
"benchmark": "motion prediction",
"annotations": "2d bounding boxes, semantic map, motion/trajectories",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "-",
"publishDate": "2019-09-01",
"lastUpdate": "-",
"relatedPaper": "https://arxiv.org/pdf/1910.03088.pdf",
"location": "USA, China, Germany and Bulgaria",
"rawData": "Yes"
},
{
"id": "MCity Data Collection",
"href": "https://arxiv.org/pdf/1912.06258.pdf",
"size_storage": "11000",
"size_hours": "50",
"frames": "-",
"numberOfScenes": "255",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "3x Velodyne Ultra Puck VLP-32C lidar 10Hz, 2x forward-facing cameras 30° 1080P 30Hz,1x backward-facing camera 90° 1080P 30Hz, 1x cabin pose camera 1280×1080 30Hz, 1x cabin head/eyeball camera 640P 30Hz, 1x Ibeo four beam LUX sensor 25Hz, 1x Delphi ESR 2.5 Radar 90° 20Hz,1x NovAtel FlexPak6 with IMU-IGM-S1 and 4G cellular for RTK GPS single antenna 1Hz",
"benchmark": "-",
"annotations": "semantic segmentation of objects, traffic lights, traffic signs, lanes",
"licensing": "-",
"relatedDatasets": "-",
"publishDate": "2019-12-01",
"lastUpdate": "-",
"relatedPaper": "https://arxiv.org/pdf/1912.06258.pdf",
"location": "Ann Arbor, USA",
"rawData": "Yes"
},
{
"id": "Oxford Radar Robot Car",
"href": "https://oxford-robotics-institute.github.io/radar-robotcar-dataset/",
"size_storage": "4700",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "32",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "1 x Navtech CTS350-X Millimetre-Wave FMCW radar 4 Hz, 2 x Velodyne HDL-32E LIDAR 360°32 planes 20 Hz, 1 x Point Grey Bumblebee XB3 trinocular stereo camera 1280×960×3 16 Hz 66°3 x Point Grey Grasshopper2 1024×1024 11.1 Hz 180°, 2 x SICK LMS-151 2D LIDAR 270° 50Hz, 1 x NovAtel SPAN-CPT ALIGN inertial and GPS navigation system 6 axis 50Hz,",
"benchmark": "-",
"annotations": "ground truth data",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International",
"relatedDatasets": "Oxford Robot Car",
"publishDate": "2020-02-01",
"lastUpdate": "-",
"relatedPaper": "https://arxiv.org/pdf/1909.01300.pdf",
"location": "Oxford",
"rawData": "Yes"
},
{
"id": "NightOwls",
"href": "https://www.nightowls-dataset.org/",
"size_storage": "-",
"size_hours": "5.17",
"frames": "279000",
"numberOfScenes": "40",
"samplingRate": "15",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x industry standard 1024x640 camera",
"benchmark": "pedestrian detection, object detection",
"annotations": "bounding boxes, attributes, temporal tracking annotations",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "-",
"publishDate": "2018-12-01",
"lastUpdate": "-",
"relatedPaper": "https://www.robots.ox.ac.uk/~vgg/publications/2018/Neumann18b/neumann18b.pdf",
"location": "Several cities across Europe",
"rawData": "Yes",
"DOI": "10.1007/978-3-030-20887-5_43"
},
{
"id": "DDD 20",
"href": "https://sites.google.com/view/davis-driving-dataset-2020/home",
"size_storage": "1300",
"size_hours": "51",
"frames": "-",
"numberOfScenes": "216",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, car parameters",
"sensorDetail": "1x DAVIS346B 346x260 up to 50Hz, vehicle bus data",
"benchmark": "-",
"annotations": "-",
"licensing": "Creative Commons Attribution-ShareAlike 4.0 International",
"relatedDatasets": "DDD 17",
"publishDate": "2020-02-01",
"lastUpdate": "-",
"relatedPaper": "https://arxiv.org/pdf/2005.08605.pdf",
"location": "California, USA",
"rawData": "Yes"
},
{
"id": "H3D",
"href": "https://paperswithcode.com/dataset/h3d",
"size_storage": "-",
"size_hours": "0.77",
"frames": "27721",
"numberOfScenes": "160",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "3x color PointGrey Grasshopper3 video cameras 1920x1200 90°/80° 30Hz, 1x Velodyne HDL-64E LiDAR 64 beams 360° 10Hz, 1x GeneSys Eletronik GmbH Automotive Dynamic Motion Analyzer 100Hz",
"benchmark": "-",
"annotations": "3d bounding boxes",
"licensing": "-",
"relatedDatasets": "-",
"publishDate": "2019-03-01",
"lastUpdate": "-",
"relatedPaper": "https://arxiv.org/pdf/1903.01568.pdf",
"location": "San Francisco Bay Area, USA",
"rawData": "Yes"
},
{
"id": "4Seasons",
"href": "https://www.4seasons-dataset.com/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "30",
"samplingRate": "30",
"lengthOfScenes": "-",
"sensors": "camera, imu/rtk-gnss",
"sensorDetail": "2x cameras stereo baseline 30cm 800x400 (after cropping)",
"benchmark": "globally consistent reference poses",
"annotations": "-",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",
"relatedDatasets": "-",
"publishDate": "2020-10-01",
"lastUpdate": "-",
"relatedPaper": "https://arxiv.org/pdf/2009.06364.pdf",
"location": "-",
"rawData": "Yes"
},
{
"id": "RadarScenes",
"href": "https://radar-scenes.com/",
"size_storage": "-",
"size_hours": "4",
"frames": "-",
"numberOfScenes": "158",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, radar, odometry",
"sensorDetail": "4x 77 GHz series production automotive 60° radar sensor, 1x documentary camera",
"benchmark": "-",
"annotations": "point-wise",
"licensing": "Creative Commons Attribution Non Commercial Share Alike 4.0 International",
"relatedDatasets": "-",
"publishDate":"2021-03-01",
"lastUpdate": "-",
"relatedPaper": "https://arxiv.org/pdf/2104.02493.pdf",
"location": "-",
"rawData": "Yes"
},
{
"id": "India Driving Dataset",
"href": "https://idd.insaan.iiit.ac.in/",
"size_storage": "-",
"size_hours": "-",
"frames": "10004",
"numberOfScenes": "182",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1080p & 720p stereo image",
"benchmark": "Pixel-Level Semantic Segmentation Task, Instance-Level Semantic Segmentation Task",
"annotations": "semantic segmentation",
"licensing": "-",
"relatedDatasets": "-",
"publishDate": "2018-11-01",
"lastUpdate": "-",
"relatedPaper": "https://idd.insaan.iiit.ac.in/media/publications/idd-650.pdf",
"location": "Bangalore and Hyderabad, India",
"rawData": "Yes",
"DOI": "10.1109/WACV.2019.00190"
},
{
"id": "Synscapes",
"href": "https://7dlabs.com/synscapes-overview",
"size_storage": "-",
"size_hours": "-",
"frames": "25000",
"numberOfScenes": "25000",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "RGB images in PNG format 1440x720 & upscaled version 2048x1024",
"benchmark": "-",
"annotations": "2d bounding boxes, 3d bounding boxes, occlusion, truncation, semantic segmentation,instance segmentation, depth segmentation, scene metadata",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "-",
"publishDate": "2018-10-01",
"lastUpdate": "-",
"relatedPaper": "https://arxiv.org/pdf/1810.08705v1.pdf",
"location": "-",
"rawData": "-"
},
{
"id": "RADIATE",
"href": "http://pro.hw.ac.uk/radiate/",
"size_storage": "-",
"size_hours": "5",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "1x ZED stereo camera 672x376 15Hz, 1x Velodyne HDL-32e LiDAR 32 channel 360° 10Hz, 1x Navtech CTS350-X radar 360°, 1x Advanced Navigation Spatial Dual GPS/IMU",
"benchmark": "-",
"annotations": "2d bounding boxes",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International",
"relatedDatasets": "-",
"publishDate": "2020-10-01",
"lastUpdate": "-",
"relatedPaper": "https://arxiv.org/pdf/2010.09076.pdf",
"location": "-",
"rawData": "Yes"
},
{
"id": "Bosch Small Traffic Lights Dataset",
"href": "https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset",
"size_storage": "-",
"size_hours": "-",
"frames": "13427",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "raw 12bit HDR images with a red-clear-clear-blue filter 1280x720 & reconstructed 8-bit RGB color images 1280x720",
"benchmark": "-",
"annotations": "bounding boxes, state",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "-",
"publishDate": "2017-05-01",
"lastUpdate": "-",
"relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7989163",
"location": "-",
"rawData": "Yes",
"DOI": "10.1109/ICRA.2017.7989163"
},
{
"id": "PepScenes",
"href": "https://github.com/huawei-noah/PePScenes",
"relatedPaper": "https://arxiv.org/pdf/2012.07773.pdf"
},
{
"id": "WZ-traffic dataset",
"href": "https://github.com/Fangyu0505/traffic-scene-recognition"
},
{
"id": "Nighttime Driving",
"href": "http://people.ee.ethz.ch/~daid/NightDriving/"
},
{
"id": "Cooperative Driving Dataset (CODD)",
"href": "https://github.com/eduardohenriquearnold/CODD",
"size_hours": "-",
"size_storage": "16.7",
"frames": "13500",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "lidar",
"sensorDetail": "-",
"recordingPerspective": "Top Shot",
"dataType":"Real",
"benchmark": "-",
"annotations": "3d bounding boxes",
"licensing": "Creative Commons Attribution Share Alike 4.0 International",
"relatedDatasets": "-",
"publishDate": "2021-11-23",
"lastUpdate": "-",
"paperTitle": "Fast and Robust Registration of Partially Overlapping Point Clouds",
"relatedPaper": "https://arxiv.org/pdf/2112.09922.pdf",
"location": "CARLA environment",
"rawData": "-",
"DOI": "10.1109/LRA.2021.3137888"
},
{
"id": "AIODrive",
"href": "http://www.aiodrive.org/overview.html",
"size_hours": "-",
"size_storage": "3041.71",
"frames": "100000",
"numberOfScenes": "100",
"samplingRate": "-",
"lengthOfScenes": "100",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "5x color cameras 1920x720 10Hz, 5x depth cameras 1920x720 10Hz, 3x lidar 64/800/1200 channels 360° 10Hz, 1x SPAD-LiDAR, 4x radar 360° 10Hz, 1x gps/imu 10Hz",
"recordingPerspective": "Bird’s Eye",
"dataType":"Real",
"benchmark": "3d object detection, trajectory forecasting",
"annotations": "2d/3d bounding boxes, object category and attributes, 2d-3d semantic, instance and panoptic segmentation",
"licensing": "freely available for both commercial and non-commercial purposes",
"relatedDatasets": "-",
"publishDate": "2021-04-06",
"lastUpdate": "-",
"paperTitle": "All-In-One Drive: A Comprehensive Perception Dataset with High-Density Long-Range Point Clouds",
"relatedPaper": "https://www.xinshuoweng.com/papers/AIODrive/arXiv.pdf",
"location": "CARLA environment",
"rawData": "-",
"DOI": "10.13140/RG.2.2.21621.81122"
},
{
"id": "ROAD",
"href": "https://github.com/gurkirt/road-dataset",
"size_hours": "2.83",
"size_storage": "-",
"frames": "122000",
"numberOfScenes": "22",
"samplingRate": "12",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"benchmark": "agent detection, action detection and road event detection",
"annotations": "2d/3d bounding boxes, action label and location labels",
"licensing": "Creative Commons Attribution Share Alike 4.0 International",
"relatedDatasets": "Oxford Robot Car Dataset (OxRD)",
"publishDate": "-",
"lastUpdate": "-",
"paperTitle": "ROAD: The ROad event Awareness Dataset for Autonomous Driving",
"relatedPaper": "https://www.computer.org/csdl/api/v1/periodical/trans/tp/5555/01/09712346/1AZL0P4dL1e/download-article/pdf",
"location": "Oxford, UK",
"rawData": "-",
"DOI": "10.1109/TPAMI.2022.3150906"
},
{
"id": "ONCE",
"href": "https://once-for-auto-driving.github.io/index.html",
"size_storage": "-",
"size_hours": "144",
"frames": "1000000",
"numberOfScenes": "581",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "7x color cameras 1920x1020 10Hz, 1x 40-beam lidar 360° 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"benchmark": "3d object detection",
"annotations": "2d/3d bounding boxes",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (CC BY-NC-SA 4.0)",
"relatedDatasets": "ONCE-3DLanes",
"publishDate":"2021-05-18",
"lastUpdate": "2021-08-05",
"paperTitle": "One Million Scenes for Autonomous Driving: ONCE Dataset",
"relatedPaper": "https://arxiv.org/pdf/2106.11037.pdf",
"location": "China",
"rawData": "-",
"DOI": "10.48550/arXiv.2106.11037"
},
{
"id": "DriveU Traffic Light Dataset",
"href": "https://www.uni-ulm.de/en/in/driveu/projects/driveu-traffic-light-dataset/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "15",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x stereo camera 60°, 1x stereo camera 130°",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData":"Yes",
"benchmark": "-",
"annotations": "2d bounding boxes",
"licensing": "freely available for non-commercial research and teaching activities",
"relatedDatasets": "-",
"publishDate":"2018-11-27",
"lastUpdate": "2021-04",
"paperTitle": "The DriveU Traffic Light Dataset: Introduction and Comparison with Existing Datasets",
"relatedPaper": "https://www.researchgate.net/profile/Julian-Mueller-14/publication/327808220_The_DriveU_Traffic_Light_Dataset_Introduction_and_Comparison_with_Existing_Datasets/links/5c1910e4a6fdccfc7056b787/The-DriveU-Traffic-Light-Dataset-Introduction-and-Comparison-with-Existing-Datasets.pdf ",
"location": "10 cities across Germany",
"rawData": "-",
"DOI": "10.1109/ICRA.2018.8460737"
},
{
"id": "Bosch TL",
"href": "https://github.com/asimonov/Bosch-TL-Dataset",
"size_storage": "-",
"size_hours": "-",
"frames": "13427",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "camera 1280x720",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData":"No",
"benchmark": "-",
"annotations": "2d bounding boxes",
"licensing": "freely available for non-commercial use",
"relatedDatasets": "-",
"publishDate":"2017-07-24",
"lastUpdate": "-",
"paperTitle": "A deep learning approach to traffic lights: Detection, tracking, and classification",
"relatedPaper": "https://ieeexplore.ieee.org/document/7989163",
"location": "El Camino Real in the San Francisco Bay Area, California",
"rawData": "-",
"DOI": "10.1109/ICRA.2017.7989163"
},
{
"id": "nuPlan",
"href": "https://arxiv.org/abs/2106.11810",
"size_storage": "-",
"size_hours": "1500",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "8x cameras 2000x1200 10Hz, 5x lidar 20Hz, 1x imu 100Hz, 1x gnss 20Hz",
"recordingPerspective": "-",
"dataType":"Real",
"mapData":"No",
"benchmark": "autonomous vehicle planning",
"annotations": "2d/3d bounding boxes",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (CC BY-NC-SA 4.0)",
"relatedDatasets": "-",
"publishDate":"2021-12-10",
"lastUpdate": "-",
"paperTitle": "nuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles",
"relatedPaper": "https://arxiv.org/abs/2106.11810",
"location": "Boston, Pittsburgh, Las Vegas and Singapore",
"rawData": "-",
"DOI": "10.48550/arXiv.2106.11810"
},
{
"id": "Steet Hazards Dataset",
"href": "https://once-for-auto-driving.github.io/index.html"
},
{
"id": "JAAD",
"href": "https://paperswithcode.com/dataset/jaad",
"size_storage": "3.1",
"size_hours": "-",
"frames": "82032",
"numberOfScenes": "346",
"samplingRate": "30",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x camera 1920x1080 110°, 1x camera 1920x1080 170°, 1x camera 1280x720 100°",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData":"No",
"benchmark": "-",
"annotations": "2d bounding boxes, behavioral tags, contextual tags",
"licensing": "MIT License",
"relatedDatasets": "PIE Dataset",
"publishDate":"2016-09-15",
"lastUpdate": "-",
"paperTitle": "Are They Going to Cross? A Benchmark Dataset and Baseline for Pedestrian Crosswalk Behavior",
"relatedPaper": "https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w3/Rasouli_Are_They_Going_ICCV_2017_paper.pdf",
"location": "North America and Europe",
"rawData": "-",
"DOI": "10.1109/ICCVW.2017.33"
},
{
"id": "RoadObstacle21",
"href": "https://segmentmeifyoucan.com/datasets"
},
{
"id": "Beyond PASCAL",
"href": "https://yuxng.github.io/Xiang_WACV_03242014.pdf",
"size_storage": "8.5",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"recordingPerspective": "-",
"dataType":"Real",
"mapData":"No",
"sensorDetail": "-",
"benchmark": "anomaly detection, 3D object detection and pose estimation",
"annotations": "label landmarks of the CAD model on the 2D image",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2014-03-26",
"lastUpdate": "-",
"paperTitle": "Beyond PASCAL: A Benchmark for 3D Object Detection in the Wild",
"relatedPaper": "https://cvgl.stanford.edu/papers/xiang_wacv14.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/WACV.2014.6836101"
},
{
"id": "The EuroCity Persons Dataset",
"href": "https://arxiv.org/abs/1805.07193",
"size_storage": "-",
"size_hours": "-",
"frames": "47300",
"numberOfScenes": "-",
"samplingRate": "20",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x camera 1920x1080 20Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData":"Yes",
"benchmark": "object detection",
"annotations": "2d bounding boxes",
"licensing": "freely available for research use by eligiible persons",
"relatedDatasets": "ECP2.5D - Person Localization in Traffic Scenes",
"publishDate":"2019-03-31",
"lastUpdate": "2020-11-11",
"paperTitle": "The EuroCity Persons Dataset: A Novel Benchmark for Object Detection",
"relatedPaper": "https://arxiv.org/pdf/1805.07193.pdf",
"location": "12 countries and 31 cities across Europe",
"rawData": "-",
"DOI": "10.1109/TPAMI.2019.2897684"
},
{
"id": "CADC",
"href": "http://cadcd.uwaterloo.ca/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "75",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "cameras, lidar, gps",
"sensorDetail": "8x camera Ximea MQ013CG-E2 1280x1024 10Hz,  1x lidar Veldyne VLP-32C 360° 10Hz, 1x NovAtel OEM638 Triple-Frequency GPS, 1x Sensonor STIM300 MEMS 100Hz IMU, 2x Xsens 200Hz IMU",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData":"Yes",
"benchmark": "-",
"annotations": "2d/3d bounding boxes",
"licensing": "Creative Commons Attribution-NonCommercial 4.0 International Public License (CC BY-NC 4.0)",
"relatedDatasets": "-",
"publishDate":"2020-01-27",
"lastUpdate": "-",
"paperTitle": "Canadian Adverse Driving Conditions Dataset",
"relatedPaper": "https://arxiv.org/pdf/2001.10117.pdf",
"location": "Waterloo region in Ontorio, Canada",
"rawData": "-",
"DOI": "10.1177/0278364920979368"
},
{
"id": "CARRADA Dataset",
"href": "https://github.com/valeoai/carrada_dataset",
"size_storage": "288",
"size_hours": "-",
"frames": "12666",
"numberOfScenes": "30",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, radar",
"sensorDetail": "1x camera 1238x1024,  1x radar 180°",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData":"No",
"benchmark": "-",
"annotations": "range-angle Doppler annotations",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (CC BY-NC-SA 4.0)",
"relatedDatasets": "-",
"publishDate":"2020-05-04",
"lastUpdate": "2021-07",
"paperTitle": "CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations",
"relatedPaper": "https://arxiv.org/pdf/2005.01456.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/ICPR48806.2021.9413181"
},
{
"id": "Astyx Dataset",
"href": "https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/astyx_dataset.html"
},
{
"id": "PointCloudDeNoising",
"href": "https://github.com/rheinzler/PointCloudDeNoising",
"size_storage": "-",
"size_hours": "-",
"frames": "175941",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "lidar",
"sensorDetail": "1 Velodyne VLP32c lidar sensor",
"recordingPerspective": "Ego-perspective",
"dataType":"Real",
"mapData":"No",
"benchmark": "-",
"annotations": "pointwise annotations",
"licensing": "Freely available for research and teaching purposes",
"relatedDatasets": "Gated2Depth, Gated2Gated, SeeingThroughFog",
"publishDate":"2019-12-09",
"lastUpdate": "-",
"paperTitle": "CNN-based Lidar Point Cloud De-Noising in Adverse Weather",
"relatedPaper": "https://ieeexplore.ieee.org/document/8990038",
"location": "CEREMA's climatic chamber",
"rawData": "-",
"DOI": "10.1109/LRA.2020.2972865"
},
{
"id": "Talk2Car",
"href": "https://talk2car.github.io/",
"size_storage": "300",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "850",
"samplingRate": "-",
"lengthOfScenes": "20",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "1x lidar 32 channels 360° 20Hz, 5x long range radar 13Hz, 6x camera 1600x1200 12Hz, 1x gps/imu 1000Hz",
"recordingPerspective": "Ego-perspective",
"dataType":"Real",
"mapData":"No",
"benchmark": "find bounding box for objects based on user commands in natural language",
"annotations": "bounding box and command in natural language related to the bounding box",
"licensing": "MIT license",
"relatedDatasets": "-",
"publishDate":"2020-03-18",
"lastUpdate": "2021-10-06",
"paperTitle": "Talk2Car: Taking Control of Your Self-Driving Car",
"relatedPaper": "https://arxiv.org/pdf/1909.10838.pdf",
"location": "Boston and Singapore",
"rawData": "-",
"DOI": "10.18653/v1/D19-1215"
},
{
"id": "A Parametric Top-View Representation of Complex Road Scenes",
"href": "https://www.nec-labs.com/~mas/BEV/"
},
{
"id": "DRIV100",
"href": "https://zenodo.org/record/4389243#.YnvlruhBxD8",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "100",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData":"No",
"benchmark": "Domain adaptation techniques on in-the-wild road-scene videos collected from the Internet",
"annotations": "pixel level semantic segmentation",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License",
"relatedDatasets": "-",
"publishDate":"2021-01-30",
"lastUpdate": "-",
"paperTitle": "DRIV100: In-The-Wild Multi-Domain Dataset and Evaluation for Real-World Domain Adaptation of Semantic Segmentation",
"relatedPaper": "https://arxiv.org/pdf/2102.00150.pdf",
"location": "Random videos from YouTube",
"rawData": "-",
"DOI": "10.5281/zenodo.4389243"
},
{
"id": "Cars Dataset",
"href": "https://ai.stanford.edu/~jkrause/cars/car_dataset.html",
"size_storage": "-",
"size_hours": "-",
"frames": "16185",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "camera",
"recordingPerspective": "-",
"dataType":"Real",
"mapData":"No",
"benchmark": "-",
"annotations": "car make, model, year",
"licensing": "freely available for non-commercial research and educational use",
"relatedDatasets": "BMW-10",
"publishDate":"2013",
"lastUpdate": "-",
"paperTitle": "3D Object Representations for Fine-Grained Categorization",
"relatedPaper": "https://ai.stanford.edu/~jkrause/papers/3drr13.pdf",
"location": "Sourced from internet",
"rawData": "-",
"DOI": "10.1109/ICCVW.2013.77"
},
{
"id": "CADP",
"href": "https://ankitshah009.github.io/accident_forecasting_traffic_camera",
"size_storage": "-",
"size_hours": "5.2",
"frames": "-",
"numberOfScenes": "1416",
"samplingRate": "20",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "Bird's Eye",
"dataType":"Real",
"mapData":"No",
"benchmark": "-",
"annotations": "Object detectiona and accident forecasting",
"licensing": "freely available for non-commercial use",
"relatedDatasets": "-",
"publishDate":"2018-10-04",
"lastUpdate": "-",
"paperTitle": "CADP: A Novel Dataset for CCTV Traffic Camera based Accident Analysis",
"relatedPaper": "https://ppms.cit.cmu.edu/media/project_files/CADP_IEEE_Camera_Ready_Final.pdf",
"location": "Videos sampled from YouTube",
"rawData": "-",
"DOI": "10.1109/AVSS.2018.8639160"
},
{
"id": "CARLA100",
"href": "https://github.com/felipecode/coiltraine/blob/master/docs/exploring_limitations.md",
"relatedPaper": "https://arxiv.org/pdf/1904.08980.pdf"
},
{
"id": "VITRO",
"href": "https://vitro-testing.com/test-data/dashcam-annotations/"
},
{
"id": "UDrive Dataset",
"href": "https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjqnfKIoK3uAhUOuaQKHZwcDEgQFjASegQIFBAC&url=https%3A%2F%2Ferticonetwork.com%2Fwp-content%2Fuploads%2F2017%2F12%2FUDRIVE-D41.1-UDrive-dataset-and-key-analysis-results-with-annotation-codebook.pdf&usg=AOvVaw17NgwnPrIal53hUYco9klG"
},
{
"id": "D^2 City",
"href": "https://outreach.didichuxing.com/d2city",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "11211",
"samplingRate": "25",
"lengthOfScenes": "30",
"sensors": "camera",
"sensorDetail": "",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData":"No",
"benchmark": "-",
"annotations": "2d bounding boxes and inter-frame tracking labels",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2019-04-03",
"lastUpdate": "-",
"paperTitle": "D^2-City: A Large-Scale Dashcam Video Dataset of Diverse Traffic Scenarios",
"relatedPaper": "https://arxiv.org/pdf/1904.01975v1.pdf",
"location": "5 Chinese cities",
"rawData": "-",
"DOI": "10.48550/arXiv.1904.01975"
},
{
"id": "MIT-AVT Clustered Driving Scene Dataset",
"href": "https://ieeexplore.ieee.org/abstract/document/9304677/",
"size_storage": "4000",
"size_hours": "3212",
"frames": "-",
"numberOfScenes": "1156592",
"samplingRate": "30",
"lengthOfScenes": "10",
"sensors": "camera, imu, gps",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "Seasons, Weather, Lanes, Illuminnation, Simplified Road Type, Others",
"licensing": "Not released publically",
"relatedDatasets": "-",
"publishDate":"2020-11-13",
"lastUpdate": "-",
"paperTitle": "MIT-AVT Clustered Driving Scene Dataset: Evaluating Perception Systems in Real-World Naturalistic Driving Scenarios",
"relatedPaper": "https://ieeexplore.ieee.org/abstract/document/9304677/",
"location": "Many states across the USA",
"rawData": "Yes",
"DOI": "10.1109/IV47402.2020.9304677"
},
{
"id": "DDAD",
"href": "https://github.com/AdrienGaidon-TRI/DDAD",
"size_storage": "254",
"size_hours": "-",
"frames": "21200",
"numberOfScenes": "435",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "6x cameras 2.4MP 1936x1216 10Hz, 1x Luminar-H2 Lidar sensor 360° 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "-",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License",
"relatedDatasets": "-",
"publishDate":"2020-06-19",
"lastUpdate": "-",
"paperTitle": "3D Packing for Self-Supervised Monocular Depth Estimation",
"relatedPaper": "https://arxiv.org/pdf/1905.02693.pdf",
"location": "USA (Ann Arbor, San Francisco Bay Area, Detroit, Cambridge and Massachusetts), Japan (Tokyo and Odaiba)",
"rawData": "-",
"DOI": "10.1109/CVPR42600.2020.00256"
},
{
"id": "RELLIS-3D Dataset",
"href": "https://unmannedlab.github.io/research/RELLIS-3D",
"size_storage": "58.1",
"size_hours": "-",
"frames": "13556",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "1x Nerian Karmin2 + Nerian SceneScan: 3D StereoCamera 10Hz, 1x RGB Camera: Basler acA1920-50gc camera with 16mm/F18 EDMUND Optics lens 1920x1200 10Hz, 1x Ouster OS1 LiDAR 64 Channels 10 Hz, 1x Velodyne Ultra Puck: 32 Channels 10Hz, Vectornav VN-300 Dual Antenna GNSS/INS 300Hz GPS, 100Hz IMU",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic segmentation of 2d image and 3d point clouds",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License",
"relatedDatasets": "SemanticUSL: A Dataset for LiDAR Semantic Segmentation Domain Adaptation",
"publishDate":"2020-11-26",
"lastUpdate": "2022-01-24",
"paperTitle": "RELLIS-3D Dataset: Data, Benchmarks and Analysis",
"relatedPaper": "https://arxiv.org/pdf/2011.12954.pdf",
"location": "Rellis Campus of Texas A&M University",
"rawData": "-",
"DOI": "10.1109/ICRA48506.2021.9561251"
},
{
"id": "PolySync Dataset",
"href": "http://selfracingcars.com/blog/2016/7/26/polysync"
},
{
"id": "DriveSeg (MANUAL)",
"href": "https://agelab.mit.edu/driveseg",
"size_storage": "2.98",
"size_hours": "0.03",
"frames": "5000",
"numberOfScenes": "1",
"samplingRate": "30",
"lengthOfScenes": "167",
"sensors": "camera",
"sensorDetail": "1x FDR-AX53 camera 1080P 1920x1080 30Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic segmentation",
"licensing": "freely available to academic and nonacademic entities for non-commercial purposes",
"relatedDatasets": "DriveSeg (Semi-auto)",
"publishDate":"2020-04-06",
"lastUpdate": "-",
"paperTitle": "MIT DriveSeg (Manual) Dataset for Dynamic Driving Scene Segmentation",
"relatedPaper": "https://ieee-dataport.s3.amazonaws.com/docs/25911/MIT_DriveSeg_Semiauto.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20220522%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220522T125535Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=7e2583baac0a4ad83560a6945835066bead73730d912e573c360d500a5802022",
"location": "-",
"rawData": "-",
"DOI": "10.21227/mmke-dv03"
},
{
"id": "DriveSeg (Semi-auto)",
"href": "https://agelab.mit.edu/driveseg",
"size_storage": "13.46",
"size_hours": "0.186",
"frames": "20100",
"numberOfScenes": "67",
"samplingRate": "30",
"lengthOfScenes": "10",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "pixel-wise semantic annotation",
"licensing": "-",
"relatedDatasets": "DriveSeg (MANUAL), MIT-AVT Clustered Driving Scene Dataset",
"publishDate":"2020-04-06",
"lastUpdate": "-",
"paperTitle": "MIT DriveSeg (Semi-auto) Dataset: Large-scale Semi-automated Annotation of Semantic Driving Scenes",
"relatedPaper": "https://ieee-dataport.s3.amazonaws.com/docs/25911/MIT_DriveSeg_Semiauto.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20220522%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220522T125535Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=7e2583baac0a4ad83560a6945835066bead73730d912e573c360d500a5802022",
"location": "-",
"rawData": "-",
"DOI": "10.21227/nb3n-kk46"
},
{
"id": "KUL Belgium Traffic Sign dataset",
"href": "https://people.ee.ethz.ch/~timofter/traffic_signs/"
},
{
"id": "Brain4Cars",
"href": "http://brain4cars.com/",
"size_storage": "16",
"size_hours": "-",
"frames": "2000000",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, speed logger, gps",
"sensorDetail": "-",
"recordingPerspective": "Face camera, ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "driving events (lane changes, turns, driving straight)",
"licensing": "free for educational and non-commercial purposes",
"relatedDatasets": "-",
"publishDate":"2015-12-13",
"lastUpdate": "-",
"paperTitle": "Car that Knows Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models",
"relatedPaper": "http://brain4cars.com/pdfs/iccv2015.pdf",
"location": "USA",
"rawData": "-",
"DOI": "10.1109/ICCV.2015.364"
},
{
"id": "Seasonal Variation Dataset",
"href": "http://www.cs.cmu.edu/~aayushb/localization/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, imu/gps",
"sensorDetail": "1x Point Grey Ladybug 5 panoramic camera 2448 x 2048 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "Publicly available for research community",
"relatedDatasets": "Bay Area Dataset, Illumination Changes in a day",
"publishDate":"2014-06-11",
"lastUpdate": "-",
"paperTitle": "Understanding How Camera Configuration and Environmental Conditions Affect Appearance-based Localization",
"relatedPaper": "http://www.cs.cmu.edu/~aayushb/pubs/LocalizationPaperIV2014.pdf",
"location": "California bay area and Pittsburgh, USA",
"rawData": "Yes",
"DOI": "10.1109/IVS.2014.6856605"
},
{
"id": "Bay Area Dataset",
"href": "http://www.cs.cmu.edu/~aayushb/localization/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, imu/gps",
"sensorDetail": "1x Point Grey Ladybug 5 panoramic camera 2448 x 2048 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "Available on request",
"relatedDatasets": "Seasonal Variation Dataset, Illumination Changes in a day",
"publishDate":"2014-06-11",
"lastUpdate": "-",
"paperTitle": "Understanding How Camera Configuration and Environmental Conditions Affect Appearance-based Localization",
"relatedPaper": "http://www.cs.cmu.edu/~aayushb/pubs/LocalizationPaperIV2014.pdf",
"location": "California bay area and Pittsburgh, USA",
"rawData": "Yes",
"DOI": "10.1109/IVS.2014.6856605"
},
{
"id": "Illumination Changes in a day",
"href": "http://www.cs.cmu.edu/~aayushb/localization/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, imu/gps",
"sensorDetail": "1x Point Grey Ladybug 5 panoramic camera 2448 x 2048 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "Available on request",
"relatedDatasets": "Bay Area Dataset, Seasonal Variation Dataset",
"publishDate":"2014-06-11",
"lastUpdate": "-",
"paperTitle": "Understanding How Camera Configuration and Environmental Conditions Affect Appearance-based Localization",
"relatedPaper": "http://www.cs.cmu.edu/~aayushb/pubs/LocalizationPaperIV2014.pdf",
"location": "California bay area and Pittsburgh, USA",
"rawData": "Yes",
"DOI": "10.1109/IVS.2014.6856605"
},
{
"id": "comma10k",
"href": "https://github.com/commaai/comma10k"
},
{
"id": "comma.ai",
"href": "http://research.comma.ai/",
"size_storage": "80",
"size_hours": "7.25",
"frames": "-",
"numberOfScenes": "10",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "-",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License",
"relatedDatasets": "-",
"publishDate":"2016-08-03",
"lastUpdate": "-",
"paperTitle": "Learning a Driving Simulator",
"relatedPaper": "https://arxiv.org/pdf/1608.01230",
"location": "-",
"rawData": "-",
"DOI": "10.48550/arXiv.1608.01230"
},
{
"id": "CULane Dataset",
"href": "https://xingangpan.github.io/projects/CULane.html",
"size_storage": "42.5",
"size_hours": "-",
"frames": "133235",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "traffic lanes with cubic splines",
"licensing": "freely available for non-commercial purpose",
"relatedDatasets": "-",
"publishDate":"2018-04-27",
"lastUpdate": "-",
"paperTitle": "Spatial as Deep: Spatial CNN for Traffic Scene Understanding",
"relatedPaper": "https://arxiv.org/pdf/1712.06080",
"location": "Beijing, China",
"rawData": "-",
"DOI": "10.48550/arXiv.1712.06080"
},
{
"id": "DDD20: DAVIS Driving Dataset 2020",
"href": "https://docs.google.com/document/d/1HM0CSmjO8nOpUeTvmPjopcBcVCk7KXvLUuiZFS6TWSg/pub",
"size_storage": "1300",
"size_hours": "51",
"frames": "-",
"numberOfScenes": "216",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x 346x260-pixel DAVIS346 camera 56°",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "Creative Commons Attribution-ShareAlike 4.0 International License",
"relatedDatasets": "DDD17",
"publishDate":"2017-06-05",
"lastUpdate": "2020-02-01",
"paperTitle": "DDD20 End-to-End Event Camera Driving Dataset: Fusing Frames and Events with Deep Learning for Improved Steering Prediction",
"relatedPaper": "https://arxiv.org/pdf/2005.08605.pdf",
"location": "Various states of USA, Switzerland and Germany",
"rawData": "Yes",
"DOI": "10.1109/ITSC45102.2020.9294515"
},
{
"id": "DBNet",
"href": "http://www.dbehavior.net/",
"size_storage": "1610",
"size_hours": "10",
"frames": "56800",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "lidar, camera, steering angle meter, gps",
"sensorDetail": "1x Velodyne HDL-32E laser scanner 360° 10Hz, 1x Velodyne VLP-16 laser scanner, 1x color dashboard camera 1920x1080 30Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "Driving policy prediction",
"annotations": "-",
"licensing": "Freely available for non-commercial use upon registration",
"relatedDatasets": "-",
"publishDate":"2018-06-23",
"lastUpdate": "-",
"paperTitle": "LiDAR-Video Driving Dataset: Learning Driving Policies Effectively",
"relatedPaper": "https://ieeexplore.ieee.org/document/8578713",
"location": "-",
"rawData": "Yes",
"DOI": "10.1109/CVPR.2018.00615"
},
{
"id": "DIPLECS Autonomous Driving Datasets",
"href": "https://cvssp.org/data/diplecs/",
"size_storage": "-",
"size_hours": "4",
"frames": "207364",
"numberOfScenes": "4",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, steering angle meter, eye tracker",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "Freely available for academic purposes",
"relatedDatasets": "-",
"publishDate":"2015-10-07",
"lastUpdate": "-",
"paperTitle": "How Much of Driving Is Preattentive?",
"relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7293673",
"location": "Sweden and United Kingdom",
"rawData": "-",
"DOI": "10.1109/TVT.2015.2487826"
},
{
"id": "DR(eye)VE",
"href": "https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=8",
"size_storage": "-",
"size_hours": "6.16",
"frames": "555000",
"numberOfScenes": "74",
"samplingRate": "-",
"lengthOfScenes": "300",
"sensors": "camera, eye tracker",
"sensorDetail": "1x roof mounted GARMIN VirbX camera 1080p 25Hz, 1x SMI ETG 2w Eye Tracking Glasses (ETG) 60Hz, 1x frontal camera 720p 30Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "Gaze Maps, GPS, Speed and Course",
"licensing": "Freely available for non-commercial use upon registration",
"relatedDatasets": "-",
"publishDate":"2018-06-08",
"lastUpdate": "-",
"paperTitle": "Predicting the Driver's Focus of Attention: The DR(eye)VE Project",
"relatedPaper": "https://arxiv.org/pdf/1705.03854.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/TPAMI.2018.2845370"
},
{   "id": "SemanticPOSS",
"href": "http://www.poss.pku.edu.cn/semanticposs.html"
},
{
"id": "SemanticUSL",
"href": "https://unmannedlab.github.io/semanticusl"
},
{
"id": "ELEKTRA",
"href": "http://adas.cvc.uab.es/elektra/datasets/"
},
{
"id": "GTSRB",
"href": "https://benchmark.ini.rub.de/gtsrb_news.html",
"size_storage": "-",
"size_hours": "-",
"frames": "50000",
"numberOfScenes": "-",
"samplingRate": "25",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x Prosilica GC 1380CH 1360x1024 25Hz",
"recordingPerspective": "ego-perspetive",
"dataType":"Real",
"mapData": "No",
"benchmark": "Traffic Sign Recognition",
"annotations": "2d bounding box",
"licensing": "Free to use",
"relatedDatasets": "GTSDB",
"publishDate":"2010-12-01",
"lastUpdate": "2011-01-19",
"paperTitle": "The German Traffic Sign Recognition Benchmark: A multi-class classification competition",
"relatedPaper": "https://www.ini.rub.de/upload/file/1470692848_f03494010c16c36bab9e/StallkampEtAl_GTSRB_IJCNN2011.pdf",
"location": "Germany",
"rawData": "-",
"DOI": "10.1109/IJCNN.2011.6033395"
},
{
"id": "GTSDB",
"href": "https://benchmark.ini.rub.de/gtsdb_news.html",
"size_storage": "-",
"size_hours": "-",
"frames": "900",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x Prosilica GC 1380CH camera 1360x1024",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "Traffic Sign Detection",
"annotations": "2d bounding box",
"licensing": "-",
"relatedDatasets": "GTSRB",
"publishDate":"2012-12-01",
"lastUpdate": "2013-08-09",
"paperTitle": "Detection of traffic signs in real-world images: The German traffic sign detection benchmark",
"relatedPaper": "https://ieeexplore.ieee.org/document/6706807",
"location": "Bochum, Germany",
"rawData": "-",
"DOI": "10.1109/IJCNN.2013.6706807"
},
{
"id": "HCI Challenging Stereo",
"href": "https://hci.iwr.uni-heidelberg.de/benchmarks/Challenging_Data_for_Stereo_and_Optical_Flow",
"size_storage": "-",
"size_hours": "-",
"frames": "10000",
"numberOfScenes": "11",
"samplingRate": "100",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "2 x Photon Focus MV1-D1312-160-CL-12",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "-",
"licensing": "Freely available for research purposes only",
"relatedDatasets": "-",
"publishDate":"2012-03-07",
"lastUpdate": "-",
"paperTitle": "Outdoor stereo camera system for the generation of real-world benchmark data sets",
"relatedPaper": "https://www.spiedigitallibrary.org/journals/Optical-Engineering/volume-51/issue-2/021107/Outdoor-stereo-camera-system-for-the-generation-of-real-world/10.1117/1.OE.51.2.021107.short?SSO=1",
"location": "-",
"rawData": "-",
"DOI": "10.1117/1.OE.51.2.021107"
},
{
"id": "HD1K",
"href": "http://hci-benchmark.iwr.uni-heidelberg.de/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "55",
"samplingRate": "25",
"lengthOfScenes": "-",
"sensors": "lidar, camera, gps/imu",
"sensorDetail": "1x RIEGL VMX-250-CS6 laser scanner, 1x stereo system with 2 cameras 2560x1080 69.5° 200Hz, 1x Applanix POS-LV 510 gnss & imu unit",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "stereo, optical flow, single image depth prediction, object detection, and semantic segmentation",
"annotations": "depth and optical flow",
"licensing": "Freely available for research purposes",
"relatedDatasets": "-",
"publishDate":"2018-02-01",
"lastUpdate": "2018-03-05",
"paperTitle": "The HCI Benchmark Suite: Stereo And Flow Ground Truth With Uncertainties for Urban Autonomous Driving",
"relatedPaper": "http://hci-benchmark.iwr.uni-heidelberg.de/media/publications//kondermann2016.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/CVPRW.2016.10"
},
{
"id": "LUMPI",
"href": "https://data.uni-hannover.de/ne/dataset/lumpi"
},
{
"id": "Highway Work Zones",
"href": "http://www.andrew.cmu.edu/user/jonghole/workzone/data/"
},
{
"id": "Amodal Cityscapes",
"href": "https://github.com/ifnspaml/AmodalCityscapes",
"size_storage": "-",
"size_hours": "-",
"frames": "3472",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "Dataset taken from Cityscapes and occlusions pasted artficially for amodal segmentation tasks",
"recordingPerspective": "ego-perspective",
"dataType":"-",
"mapData": "No",
"benchmark": "amodal semantic segmentation method",
"annotations": "semantic segmentation and amodal semantic segmentation",
"licensing": "Freely available code to generate the dataset",
"relatedDatasets": "-",
"publishDate":"2022-06-01",
"lastUpdate": "-",
"paperTitle": "Amodal Cityscapes: A New Dataset, its Generation, and an Amodal Semantic Segmentation Challenge Baseline",
"relatedPaper": "https://arxiv.org/pdf/2206.00527.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.48550/arXiv.2206.00527"
},
{
"id": "LISA Traffic Sign Dataset",
"href": "http://cvrr.ucsd.edu/LISA/lisa-traffic-sign-dataset.html",
"size_storage": "-",
"size_hours": "-",
"frames": "6610",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding boxes with description",
"licensing": "Academic license",
"relatedDatasets": "LISA Traffic Light Dataset",
"publishDate":"2012-10-19",
"lastUpdate": "-",
"paperTitle": "Vision-Based Traffic Sign Detection and Analysis for Intelligent Driver Assistance Systems: Perspectives and Survey",
"relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6335478&casa_token=iD5J89e1Y0MAAAAA:OKU-qJJPvSmZk1wJGlPb4G3Z7SF12nHZdr7mEV03pwgl2Q8ASZlH7T2zbo5n65e4yBniT_S5jQfn&tag=1",
"location": "USA",
"rawData": "-",
"DOI": "10.1109/TITS.2012.2209421"
},
{
"id": "LISA Traffic Light Dataset",
"href": "http://cvrr.ucsd.edu/LISA/datasets.html",
"size_storage": "5",
"size_hours": "0.75",
"frames": "43007",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x Point Grey's Bumblebee XB3 (BBX3-13S2C-60) stereo camera 1280x960 ",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding boxes with the description of state of traffic light",
"licensing": "Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",
"relatedDatasets": "LISA Traffic Sign Dataset",
"publishDate":"2016-02-03",
"lastUpdate": "-",
"paperTitle": "Vision for Looking at Traffic Lights: Issues, Survey, and Perspectives",
"relatedPaper": "https://ieeexplore.ieee.org/document/7398055",
"location": "San Diego, California, USA",
"rawData": "-",
"DOI": "10.1109/TITS.2015.2509509"
},
{
"id": "Malaga Stereo and Laser Urban",
"href": "https://www.mrpt.org/MalagaUrbanDataset",
"size_storage": "4.76",
"size_hours": "1.55",
"frames": "-",
"numberOfScenes": "15",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "1x Point Grey Research Bumblebee 2 stereo camera 1024x768 20Hz, 3x Hokuyo UTM-30LX laser scanners 270°, 2x SICK LMS-200 laser scanners, 1x xSens MTi imu 100Hz, 2x mmGPS devices from Topcon gps",
"recordingPerspective": "ego-perspective and bird's eye",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2013-10-09",
"lastUpdate": "2018-09-13",
"paperTitle": "The Málaga urban dataset: High-rate stereo and LiDAR in a realistic urban scenario",
"relatedPaper": "https://journals.sagepub.com/doi/10.1177/0278364913507326",
"location": "Málaga, Spain",
"rawData": "-",
"DOI": "10.1177/0278364913507326"
},
{
"id": "KITTI-360 PanopticBEV",
"href": "http://panoptic-bev.cs.uni-freiburg.de/",
"relatedPaper":"https://ieeexplore.ieee.org/document/9681287"
},
{
"id": "nuScenes PanopticBEV",
"href": "http://panoptic-bev.cs.uni-freiburg.de/",
"relatedPaper":"https://ieeexplore.ieee.org/document/9681287"
}, 
{
"id": "Mapillary Vistas",
"href": "https://www.mapillary.com/dataset/vistas",
"size_storage": "",
"size_hours": "-",
"frames": "25000",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "-",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "semantic image segmentation and instance-specific image segmentation",
"annotations": "instance specific object annotations",
"licensing": " Creative Commons Attribution NonCommercial Share Alike (CC BY-NC-SA)",
"relatedDatasets": "Mapillary Traffic Sign",
"publishDate":"2017-12-25",
"lastUpdate": "2021-01-18",
"paperTitle": "The Mapillary Vistas Dataset for Semantic Understanding of Street Scene",
"relatedPaper": "https://ieeexplore.ieee.org/document/8237796",
"location": "Europe, North and South America, Asia, Africa and Oceania",
"rawData": "-",
"DOI": "10.1109/ICCV.2017.534"
},
{
"id": "MTSD",
"href": "https://www.mapillary.com/dataset/trafficsign",
"size_storage": "-",
"size_hours": "-",
"frames": "105830",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Map",
"benchmark": "-",
"annotations": "2d bounding boxes and sign classification",
"licensing": "Creative Commons Attribution NonCommercial Share Alike (CC BY-NC-SA)",
"relatedDatasets": "Mapillary Vistas",
"publishDate":"2020-11-03",
"lastUpdate": "-",
"paperTitle": "The Mapillary Traffic Sign Dataset for Detection and Classification on a Global Scale",
"relatedPaper": "https://link.springer.com/chapter/10.1007/978-3-030-58592-1_5",
"location": "Europe, North and South America, Asia, Africa and Oceania",
"rawData": "-",
"DOI": "10.1007/978-3-030-58592-1_5"
},
{
"id": "TT100K",
"href": "http://cg.cs.tsinghua.edu.cn/traffic-sign/",
"size_storage": "-",
"size_hours": "-",
"frames": "100000",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "traffic-sign detection and classification",
"annotations": "2d bounding boxes and class labels",
"licensing": "Creative Commons Attribution-NonCommercial (CC-BY-NC",
"relatedDatasets": "-",
"publishDate":"2016-06-27",
"lastUpdate": "-",
"paperTitle": "Traffic-Sign Detection and Classification in the Wild",
"relatedPaper": "http://cg.cs.tsinghua.edu.cn/traffic-sign/0682.pdf",
"location": "China",
"rawData": "-",
"DOI": "10.1109/CVPR.2016.232"
},
{
"id": "NEXET",
"href": "https://blog.getnexar.com/https-medium-com-itayklein-intro-nexet-50e9b596d0e5"
},
{
"id": "Road Damage",
"href":"https://github.com/sekilab/RoadDamageDetector/",
"size_storage": "2.4",
"size_hours": "-",
"frames": "13135",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding box with class labels",
"licensing": "Creative Commons Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0)",
"relatedDatasets": "Road Damage Dataset 2018",
"publishDate":"2018-06-30",
"lastUpdate": "2020-06-02",
"paperTitle": "Road Damage Detection and Classification Using Deep Neural Networks with Smartphone Images",
"relatedPaper": "https://arxiv.org/pdf/1801.09454.pdf",
"location": "Japan",
"rawData": "-",
"DOI": "10.1111/mice.12387"
},
{
"id": "RDD2020",
"href":"https://data.mendeley.com/datasets/5ty2wb6gvg/2",
"size_storage": "1.76",
"size_hours": "-",
"frames": "26336",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "cameras",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding boxes",
"licensing": "Attribution-NonCommercial 3.0 Unported (CC BY NC 3.0)",
"relatedDatasets": "-",
"publishDate":"2021-03-18",
"lastUpdate": "2021-09-27",
"paperTitle": "RDD2020: An annotated image dataset for automatic road damage detection using deep learning",
"relatedPaper": "https://www.sciencedirect.com/science/article/pii/S2352340921004170#bib0001",
"location": "India, Japan and Czech Republic",
"rawData": "-",
"DOI": "10.17632/5ty2wb6gvg.2"
},
{
"id": "GTA5",
"href": "https://download.visinf.tu-darmstadt.de/data/from_games/",
"size_storage": "57.05",
"size_hours": "-",
"frames": "24966",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "camera 1914x1052",
"recordingPerspective": "ego-perspective",
"dataType":"Synthetic",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic segmentation of different classes",
"licensing": "Freely available for research and educational purposes",
"relatedDatasets": "-",
"publishDate":"2016-04-08",
"lastUpdate": "-",
"paperTitle": "Playing for Data: Ground Truth from Computer Games",
"relatedPaper": "https://arxiv.org/pdf/1608.02192.pdf",
"location": "Game: Grand Theft Auto 5",
"rawData": "-",
"DOI": "10.1007/978-3-319-46475-6_7"
},
{
"id": "Ground Truth Stixel",
"href": "http://www.6d-vision.com/ground-truth-stixel-dataset",
"size_storage": "3.2",
"size_hours": "-",
"frames": "78500",
"numberOfScenes": "318",
"samplingRate": "25",
"lengthOfScenes": "-",
"sensors": "camera, radar",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "stixel measurements",
"licensing": "freely available for of scholarly and technical work",
"relatedDatasets": "-",
"publishDate":"2013-06-23",
"lastUpdate": "-",
"paperTitle": "Exploiting the Power of Stereo Confidences",
"relatedPaper": "http://wwwlehre.dhbw-stuttgart.de/~sgehrig/stixelGroundTruthDataset/stixel.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/CVPR.2013.45"
},
{
"id": "Boxy",
"href": "https://boxy-dataset.com/boxy/",
"size_storage": "-",
"size_hours": "-",
"frames": "200000",
"numberOfScenes": "34",
"samplingRate": "15",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x mvBlueFOX3-2051 with a Sony IMX250 chip 2464x2056 15Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "bounding box, polygon, and real-time detections",
"annotations": "2d boxes or 3d cuboids for vehicles",
"licensing": "Freely available for non-commercial purposes",
"relatedDatasets": "-",
"publishDate":"2019-10-27",
"lastUpdate": "-",
"paperTitle": "Boxy Vehicle Detection in Large Images",
"relatedPaper": "https://boxy-dataset.com/static/boxy/boxy_preview.pdf",
"location": "San Francisco Bay Area, California, USA",
"rawData": "-",
"DOI": "10.1109/ICCVW.2019.00112"
},
{
"id": "CarlaScenes",
"href": "https://github.com/CarlaScenes/CarlaSence",
 "relatedPaper": "https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Kloukiniotis_CarlaScenes_A_Synthetic_Dataset_for_Odometry_in_Autonomous_Driving_CVPRW_2022_paper.pdf"
},
{
"id": "K-Lane",
"href": "https://github.com/kaist-avelab/k-lane",
"relatedPaper": "https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Paek_K-Lane_Lidar_Lane_Dataset_and_Benchmark_for_Urban_Roads_and_CVPRW_2022_paper.pdf" 
}, 
{
"id": "TME Motorway",
"href": "http://cmp.felk.cvut.cz/data/motorway/",
"size_storage": "-",
"size_hours": "0.45",
"frames": "30000",
"numberOfScenes": "28",
"samplingRate": "20",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "1x stereo camera 1024x768 grayscale 32° 20Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "bounding boxes around vehicles",
"licensing": "Freely available for commercial and non-commercial research",
"relatedDatasets": "-",
"publishDate":"2012-09-16",
"lastUpdate": "-",
"paperTitle": "A System for Real-time Detection and Tracking of Vehicles from a Single Car-mounted Camera",
"relatedPaper": "http://cmp.felk.cvut.cz/data/motorway/paper/itsc2012.pdf",
"location": "Northern Italy",
"rawData": "-",
"DOI": "10.1109/ITSC.2012.6338748"
},
{
"id": "TuSimple",
"href": "https://www.tusimple.com/"
},
{
"id": "ScribbleKITTI",
"href": "https://github.com/ouenal/scribblekitti",
"relatedPaper":"https://arxiv.org/abs/2203.08537"
},
{
"id": "UAH-DriveSet",
"href": "http://www.robesafe.uah.es/personal/eduardo.romera/uah-driveset/",
"size_storage": "-",
"size_hours": "8.33",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, gps",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "Freely available to academic and non-academic entities for non-commercial purposes such as academic research, teaching, scientific publications, or personal experimentation",
"relatedDatasets": "-",
"publishDate":"2016-11-01",
"lastUpdate": "-",
"paperTitle": "Need Data for Driver Behaviour Analysis? Presenting the Public UAH-DriveSet",
"relatedPaper": "http://www.robesafe.uah.es/personal/eduardo.romera/pdfs/Romera16itsc.pdf",
"location": "-",
"rawData": "Yes",
"DOI": "10.1109/ITSC.2016.7795584"
},
{
"id": "Unsupervised Llamas",
"href": "https://unsupervised-llamas.com/llamas/",
"size_storage": "-",
"size_hours": "-",
"frames": "100042",
"numberOfScenes": "14",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "binary lane marker segmentation, lane-dependent pixel-level segmentation and lane border regression",
"annotations": "dashed lane markings",
"licensing": "Freely available for non-commercial reseach purposes only",
"relatedDatasets": "-",
"publishDate":"2019-10-27",
"lastUpdate": "-",
"paperTitle": "Unsupervised Labeled Lane Markers Using Maps",
"relatedPaper": "https://unsupervised-llamas.com/static/llamas/llamas_preview.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/ICCVW.2019.00111"
},
{
"id": "Street Hazards",
"href": "https://github.com/hendrycks/anomaly-seg",
"size_storage": "2.0",
"size_hours": "-",
"frames": "7656",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"No",
"mapData": "No",
"benchmark": "Anamoly object segmentation",
"annotations": "semantic segmentation of anamolies",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2019-11-25",
"lastUpdate": "-",
"paperTitle": "Scaling Out-of-Distribution Detection for Real-World Settings",
"relatedPaper": "https://arxiv.org/pdf/1911.11132.pdf",
"location": "Carla",
"rawData": "-",
"DOI": "10.48550/arXiv.1911.11132"
},
{
"id": "Astyx HiRes 2019",
"href": "https://www.astyx.com/fileadmin/redakteur/dokumente/Astyx_Dataset_HiRes2019_specification.pdf"
},
{
"id": "A*3D",
"href": "https://github.com/I2RDL2/ASTAR-3D",
"size_storage": "-",
"size_hours": "-",
"frames": "39179",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "2x PointGrey Chameleon3 USB3 Global shutter color cameras 2048x1536 57.3° 55Hz, 1x Velodyne HDL-64ES3 3D-LiDAR 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "3d object detection",
"annotations": "3d bounding boxes",
"licensing": "Freely available for noncommercial academic research purposes only",
"relatedDatasets": "-",
"publishDate":"2019-10-04",
"lastUpdate": "-",
"paperTitle": "A 3D Dataset: Towards Autonomous Driving in Challenging Environments",
"relatedPaper": "https://arxiv.org/pdf/1909.07541.pdf",
"location": "Singapore",
"rawData": "-",
"DOI": "10.1109/ICRA40945.2020.9197385"
},
{
"id": "CamVid",
"href": "https://www.kaggle.com/carlolepelaars/camvid",
"size_storage": "-",
"size_hours": "0.16",
"frames": "701",
"numberOfScenes": "1",
"samplingRate": "30",
"lengthOfScenes": "600",
"sensors": "camera",
"sensorDetail": "1x CCD Panasonic HVX200 digital camera 960x720 30Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic segmentation",
"licensing": "Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",
"relatedDatasets": "-",
"publishDate":"2009-01-15",
"lastUpdate": "-",
"paperTitle": "Semantic object classes in video: A high-definition ground truth database",
"relatedPaper": "https://pdf.sciencedirectassets.com/271524/1-s2.0-S0167865508X00169/1-s2.0-S0167865508001220/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEGEaCXVzLWVhc3QtMSJHMEUCIQCA11sHV8h2EpCAzXyQ0V4VP%2F%2FtSTtdmwWBVRbF4T8AMwIgXu%2BDzZ9%2FZ5Matmw%2BNuJxkagSqNnDCZObjFIe3pMY2sgq2wQIif%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAEGgwwNTkwMDM1NDY4NjUiDF3yb0q0O6GdJqshbiqvBGiWqPb8EEGrHPUFfAVoMjtnfz4EEc6DVL5rwsMRYK7K5%2FddZ3n3M82ULw%2FjzUKCkbxBijZtK8o2tl%2BLKeIZ5y3Feep%2FIjIIhQ%2FjiZeni0N07G1%2FCQq6SOZivayRtp7CUjdgTXDq4zduoNEYNE1CZ4hhTLWK2RFjlPNmdOGUxCZbT2MNy27kbcwnjQw6u%2BF5Ro6L1q%2FI9bul4%2Foc0W40c0LnEQXMe5z8hTBKEFMwT5cEUf9%2FpnS%2BskWugGgsSNFiACeEJ0NETcVhuF6FLbDICqLE%2F7qw8wG0gzbDm2SljpYiw0gkW5gxczQE3Ru3sZ%2BQuUmh1zdCk4kt%2FazqaKZJqlvCnLgiQWuF%2BRNK4Hpteh%2B4i517UrWEAU8fpXuAyMVZG8JVG6xR%2FpCHWU7H1VNEA%2FbrL5zLkYp5SzX5xFzumsVeIJ37u69HuBgFFvKOn%2BmPtasGf5TudFpcA%2FE7d7C7lcES0AvfYbhyh%2FFqHCfG4s1MwfU6qJ61nZcYSMZgEFIiTlZGR80vgkYxCvIP0sNlgdOCQHArjxYHJ6yzjkPBFQ90c2snBDkfYfWcKrvWY%2FdGp4x8TBdaqrY07YEASRAOdd%2BJJUonHQDSRUJKmXibsNBM40z1qGflHPVL0wWAM3YITapAaLPxSagAqTPJ5AoOSXElsogZYoTHHXJU8KNIblOaz6XnKx32TLnw4FRkEeVBTwjOEFr5EA4srS1qbXlJsjV0aObvKBsr9Ja1UItx3CowtcCKlgY6qQFO6pFryXLKH4V7tldhJn%2BcQ35KUZOua1QBWSBkg38vxrjhQfEyvKCns7X21TxnzARlljVwzj9rL3HtZ7co4%2BT6pxwkYvj%2FKsTotHp5qtV5dYAI4w0dLNCsPz33OdLYqbBoIfr1OQ5USj9MvuO2k2e2voHKW1SmusBW5goBvuPNuWe4dHdFZAYtvtwGAs6iCSxbAYCPv6I%2BdPySt7vGydS05xTW%2FcPUZNGE&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220704T090151Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYWIZ3RZPY%2F20220704%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=f4dcb0c82201f26036b5850b0f38e19def847297bc9cc18b614ee094af344449&hash=8eef19412becfacf9bb21603db158b613ef51da58582217142ebc507be134724&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0167865508001220&tid=spdf-ec513c9a-f60a-4935-a0ec-6f936e44745a&sid=a0422e5a30875748cf6a74770c17f8e2edf3gxrqb&type=client&ua=4d50575004060303040a&rr=7256adbaee0c694c",
"location": "United Kingdom",
"rawData": "Yes",
"DOI": "10.1016/j.patrec.2008.04.005"
},
{
"id": "Daimler Urban Segmentation",
"href": "https://computervisiononline.com/dataset/1105138608"
},
{
"id": "VPGNet",
"href": "https://arxiv.org/abs/1710.06288",
"size_storage": "-",
"size_hours": "-",
"frames": "21097",
"numberOfScenes": "-",
"samplingRate": "30",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "lane and road markings, vanishing point",
"licensing": "Available for non commercial research purposes on registration",
"relatedDatasets": "-",
"publishDate":"2017-10-22",
"lastUpdate": "-",
"paperTitle": "VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition",
"relatedPaper": "https://openaccess.thecvf.com/content_ICCV_2017/papers/Lee_VPGNet_Vanishing_Point_ICCV_2017_paper.pdf",
"location": "Seoul, South Korea",
"rawData": "-",
"DOI": "10.1109/ICCV.2017.215"
},
{
"id": "Toronto 3D",
"href": "https://github.com/WeikaiTan/Toronto-3D",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps",
"sensorDetail": "1x Teledyne Optech Maverick consists of a 32-line LiDAR sensor, a Ladybug 5 panoramic camera, a GNSS system, and a SLAM system",
"recordingPerspective": "bird's eye view",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "object class labels",
"licensing": "Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)",
"relatedDatasets": "-",
"publishDate":"2020-03-22",
"lastUpdate": "2020-04-23",
"paperTitle": "Toronto-3D: A Large-scale Mobile LiDAR Dataset for Semantic Segmentation of Urban Roadways",
"relatedPaper": "https://openaccess.thecvf.com/content_CVPRW_2020/papers/w11/Tan_Toronto-3D_A_Large-Scale_Mobile_LiDAR_Dataset_for_Semantic_Segmentation_of_CVPRW_2020_paper.pdf",
"location": "Toronto, Canada",
"rawData": "-",
"DOI": "10.1109/CVPRW50498.2020.00109"
},
{
"id": "Toronto City",
"href": "http://www.cs.toronto.edu/~byang/papers/Tcity_iccv17.pdf"
},
{
"id": "Synthia",
"href": "https://synthia-dataset.net/",
"size_storage": "-",
"size_hours": "-",
"frames": "213400",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x rgb camera 960x720 100°",
"recordingPerspective": "ego-perspective",
"dataType":"Synthetic",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic segmentation",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0",
"relatedDatasets": "-",
"publishDate":"2016-06-27",
"lastUpdate": "2019-10-27",
"paperTitle": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes",
"relatedPaper": "https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.pdf",
"location": "Unity development platform",
"rawData": "-",
"DOI": "10.1109/CVPR.2016.352"
},
{
"id": "RANUS",
"href": "https://sites.google.com/site/gmchoe1/ranus",
"size_storage": "11.4",
"size_hours": "-",
"frames": "40000",
"numberOfScenes": "50",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "2x Point-grey grasshopper cameras (NIR: GS3-U3-41C6NIR-C, RGB: GS3-U3-41C6C-C) 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "pixel level semantic segmentation masks",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2018-02-02",
"lastUpdate": "-",
"paperTitle": "RANUS: RGB and NIR Urban Scene Dataset for Deep Scene Parsing",
"relatedPaper": "https://joonyoung-cv.github.io/assets/paper/18_ral_ranus.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/LRA.2018.2801390"
},
{
"id": "One Thousand and One Hours",
"href": "https://level-5.global/data/prediction/",
"size_storage": "-",
"size_hours": "1118",
"frames": "-",
"numberOfScenes": "170000",
"samplingRate": "-",
"lengthOfScenes": "25",
"sensors": "camera, lidar, radar",
"sensorDetail": "-",
"recordingPerspective": "top view",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "trajectories",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike license (CC-BY-NC-SA-4.0)",
"relatedDatasets": "Level 5 Perception Dataset",
"publishDate":"2020-06-25",
"lastUpdate": "-",
"paperTitle": "One Thousand and One Hours: Self-driving Motion Prediction Dataset",
"relatedPaper": "https://arxiv.org/pdf/2006.14480.pdf",
"location": "Palo Alto, California, USA",
"rawData": "-",
"DOI": "10.48550/arXiv.2006.14480"
},
{
"id": "LIBRE",
"href": "https://sites.google.com/g.sp.m.is.nagoya-u.ac.jp/libre-dataset/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, odometry, gps/imu",
"sensorDetail": "10 different lidars compared (VLS-128, HDL-64S2, HDL-32E, VLP-32C, VLP-16, Pandar64, Pandar40P, OS1-64, OS1-16, RS-Lidar32)",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "Lidar sensors",
"annotations": "",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2020-10-19",
"lastUpdate": "-",
"paperTitle": "LIBRE: The Multiple 3D LiDAR Dataset",
"relatedPaper": "https://arxiv.org/pdf/2003.06129.pdf",
"location": "Nagoya, Japan",
"rawData": "-",
"DOI": "10.1109/IV47402.2020.9304681"
},
{
"id": "PREVENTION",
"href": "https://prevention-dataset.uah.es/",
"relatedPaper": "https://ieeexplore.ieee.org/document/8917433"
},
{
"id": "Stanford Track Collection",
"href": "https://cs.stanford.edu/people/teichman/stc/",
"size_storage": "2",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "lidar, gps/imu",
"sensorDetail": "1x Velodyne HDL-64E S2 lidar 360° 10Hz, 1x Applanix LV-420 gps 200Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "-",
"licensing": "Freely available for non-commercial research",
"relatedDatasets": "-",
"publishDate":"2011-05-09",
"lastUpdate": "2012-06-01",
"paperTitle": "Towards 3D object recognition via classification of arbitrary object tracks",
"relatedPaper": "https://cs.stanford.edu/people/teichman/papers/icra2011.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/ICRA.2011.5979636"
},
{
"id": "LiDAR-Video Driving Dataset",
"href": "https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf",
"size_storage": "1000",
"size_hours": "-",
"frames": "10000",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, accelerometer, steering angle meter",
"sensorDetail": "1x HDL-32E Velodyne lidar 360° 10Hz, 1x VLP-16 Velodyne lidar, 1x Dashboard camera 1920x1080 30Hz ",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "driver behaviour",
"annotations": "-",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2018-06-18",
"lastUpdate": "-",
"paperTitle": "LiDAR-Video Driving Dataset: Learning Driving Policies Effectively",
"relatedPaper": "https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/CVPR.2018.00615"
},
{
"id": "WoodScape",
"href": "https://paperswithcode.com/dataset/woodscape",
"size_storage": "-",
"size_hours": "-",
"frames": "10000",
"numberOfScenes": "-",
"samplingRate": "30",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu, odometer",
"sensorDetail": "4x 1MPx RGB fisheye cameras 190°, 1x Velodyne HDL-64E lidar 20Hz, 1x NovAtel Propak6 & SPAN-IGM-A1 gnss/imu, 1x Garmin 18x GNSS Positioning with SPS, Odometry signals from the vehicle bus",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic segmentation, monocular depth estimation, 2D & 3D bounding boxes,  visual odometry, visual SLAM, motion segmentation, soiling detection and end-to-end driving (driving controls)",
"licensing": "Freely available for non-commercial research",
"relatedDatasets": "-",
"publishDate":"2019-10-27",
"lastUpdate": "2021-11-16",
"paperTitle": "WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving",
"relatedPaper": "https://arxiv.org/pdf/1905.01489.pdf",
"location": "USA, Europe, and China",
"rawData": "-",
"DOI": "10.1109/ICCV.2019.00940"
},
{
"id": "Raincouver",
"href": "https://ieeexplore.ieee.org/document/7970170"
},
{
"id": "TRoM",
"href": "http://www.tromai.icoc.me/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "712",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, gps",
"sensorDetail": "1x PointGray color camera 1280x960",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "Road marking detection",
"annotations": "Pixel level road markings",
"licensing": "Freely available for academic use",
"relatedDatasets": "-",
"publishDate":"2017-10-16",
"lastUpdate": "-",
"paperTitle": "Benchmark for road marking detection: Dataset specification and performance baseline",
"relatedPaper": "https://ieeexplore.ieee.org/document/8317749",
"location": "Beijing, China",
"rawData": "-",
"DOI": "10.1109/ITSC.2017.8317749"
},
{
"id": "Caltech Lanes",
"href": "http://www.mohamedaly.info/datasets/caltech-lanes",
"size_storage": "0.55",
"size_hours": "-",
"frames": "1225",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d splines for lane markings",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2008-06-04",
"lastUpdate": "-",
"paperTitle": "Real time Detection of Lane Markers in Urban Streets",
"relatedPaper": "https://ieeexplore.ieee.org/abstract/document/4621152?casa_token=V3xXln8DOeUAAAAA:h4ALqmxoJhgv3D2Szr9llQIc0UwNNkGNXzHR2486xb478J9dm_Pthf2ay4Zdl3-uPtl-BMi762BM",
"location": "Pasadena, California, USA",
"rawData": "-",
"DOI": "10.1109/IVS.2008.4621152"
},
{
"id": "Complex Urban Dataset",
"href": "https://irap.kaist.ac.kr/dataset/"
},
{
"id": "CCSAD",
"href": "https://www.researchgate.net/publication/277476726_Towards_Ubiquitous_Autonomous_Driving_The_CCSAD_Dataset"
},
{
"id": "Street Learn",
"href": "https://paperswithcode.com/dataset/streetlearn"
},
{
"id": "Multi Vehicle Stereo Event Camera",
"href": "https://daniilidis-group.github.io/mvsec/"
},
{
"id": "AMUSE",
"href": "http://www.cvl.isy.liu.se/research/datasets/amuse/"
},
{
"id": "Cheddar Gorge Dataset",
"href": "https://www.researchgate.net/publication/228428941_The_Cheddar_Gorge_Data_Set"
},
{
"id": "The Annotated Laser Dataset",
"href": "https://journals.sagepub.com/doi/pdf/10.1177/0278364910389840"
},
{
"id": "DDD 17",
"href": "https://www.paperswithcode.com/dataset/ddd17"
},
{
"id": "BLVD",
"href": "https://paperswithcode.com/dataset/blvd"
},
{
"id": "FLIR Thermal Dataset",
"href": "https://www.flir.com/oem/adas/adas-dataset-form/"
},
{
"id": "Multispectral Object Detection",
"href": "https://deepai.org/publication/multispectral-object-detection-with-deep-learning"
},
{
"id": "CityPersons",
"href": "https://arxiv.org/abs/1702.05693"
},
{
"id": "Tsinghua Daimler Cyclist Detection",
"href": "http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Tsinghua-Daimler_Cyclist_Detec/tsinghua-daimler_cyclist_detec.html"
},
{
"id": "TDU Brussels Pedestrian",
"href": "https://www.researchgate.net/figure/Results-on-the-TUD-Brussels-Pedestrian-Dataset_fig13_321232691"
},
{
"id": "ETH Pedestrian",
"href": "https://paperswithcode.com/dataset/eth"
},
{
"id": "RoadSaW",
"href": "https://www.viscoda.com/index.php/downloads/roadsaw-dataset",
"relatedPaper": "https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Cordes_RoadSaW_A_Large-Scale_Dataset_for_Camera-Based_Road_Surface_and_Wetness_CVPRW_2022_paper.pdf"
},
{
"id": "Daimler Pedestrian",
"href": "http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/daimler_pedestrian_benchmark_d.html"
},
{
"id": "Small Obstacle",
"href": "https://small-obstacle-dataset.github.io/"
},
{
"id":"GROUNDED Localizing Ground Penetrating Radar (LGPR) Evaluation Dataset",
"href": "https://lgprdata.com/"
},
{
"id":"highD",
"href": "https://www.highd-dataset.com/"
},
{
"id":"inD",
"href": "https://www.ind-dataset.com/"
},
{
"id":"rounD",
"href": "https://www.round-dataset.com/"
},
{
"id":"TAF-BW",
"href": "https://github.com/fzi-forschungszentrum-informatik/test-area-autonomous-driving-dataset"
},
{
"id":"Road Scene Graph",
"href": "https://github.com/TianYafu/road-status-graph-dataset"
},
{
"id":"R3 Driving Dataset",
"href": "https://github.com/rllab-snu/R3-Driving-Dataset",
"relatedPaper": "https://arxiv.org/pdf/2109.07995.pdf"

},
{
"id": "EISATS",
"href": "https://ccv.wordpress.fos.auckland.ac.nz/eisats/",
"size_storage": "",
"size_hours": "",
"frames": "",
"numberOfScenes": "",
"samplingRate": "",
"lengthOfScenes": "",
"sensors": "",
"sensorDetail": "",
"benchmark": "",
"annotations": "",
"licensing": "",
"relatedDatasets": "",
"publishDate":" ",
"lastUpdate": "",
"relatedPaper": ""
},
{
"id": "Ford CAMPUS",
"href": "https://www.researchgate.net/publication/220122924_Ford_Campus_vision_and_lidar_data_set",
"size_storage": "",
"size_hours": "",
"frames": "",
"numberOfScenes": "",
"samplingRate": "",
"lengthOfScenes": "",
"sensors": "",
"sensorDetail": "",
"benchmark": "",
"annotations": "",
"licensing": "",
"relatedDatasets": "",
"publishDate":" ",
"lastUpdate": "",
"relatedPaper": ""
},
{
"id": "Argoverse Stereo",
"href": "https://www.argoverse.org/data.html#stereo-link",
"size_storage": "14.2",
"size_hours": "",
"frames": "",
"numberOfScenes": "",
"samplingRate": "",
"lengthOfScenes": "",
"sensors": "",
"sensorDetail": "",
"benchmark": "stereo",
"annotations": "",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",
"relatedDatasets": "Argoverse 3D Tracking",
"publishDate": "2021-04-01",
"lastUpdate": "",
"relatedPaper": ""
},
{
"id": "uniD",
"href": "https://www.unid-dataset.com/",
"size_storage": "",
"size_hours": "",
"frames": "",
"numberOfScenes": "",
"samplingRate": "",
"lengthOfScenes": "",
"sensors": "",
"sensorDetail": "",
"benchmark": "",
"annotations": "",
"licensing": "",
"relatedDatasets": "",
"publishDate":" ",
"lastUpdate": "",
"relatedPaper": ""
},

{
"id": "exiD",
"href": "https://www.exid-dataset.com/",
"size_storage": "",
"size_hours": "",
"frames": "",
"numberOfScenes": "",
"samplingRate": "",
"lengthOfScenes": "",
"sensors": "",
"sensorDetail": "",
"benchmark": "",
"annotations": "",
"licensing": "",
"relatedDatasets": "",
"publishDate":" ",
"lastUpdate": "",
"relatedPaper": ""
},

{
"id": "The USyd Campus Dataset",
"href": "http://its.acfr.usyd.edu.au/datasets/usyd-campus-dataset/",
"size_storage": "",
"size_hours": "40",
"frames": "",
"numberOfScenes": "",
"samplingRate": "",
"lengthOfScenes": "",
"sensors": "6 cameras, lidar, gps, imu, wheel encoders",
"sensorDetail": "GMSL cameras 30, 60 Hz, VLP-16 10 Hz",
"benchmark": "",
"annotations": "Semantic Segmentation",
"licensing": "",
"relatedDatasets": "",
"publishDate": "2020-06-05",
"lastUpdate": "",
"relatedPaper": "https://ieeexplore.ieee.org/document/9109704",
"DOI": "10.1109/MITS.2020.2990183"
},
{
"id": "SemKITTI-DVPS",
"href": "https://github.com/joe-siyuan-qiao/ViP-DeepLab",
"relatedDatasets": "SemanticKITTI",
"relatedPaper": "https://arxiv.org/pdf/2012.05258",
"location": "Karlsruhe, Germany",
"rawData": "Yes"
},
{
"id": "Cityscapes-DVPS",
"href": "https://github.com/joe-siyuan-qiao/ViP-DeepLab",
"relatedDatasets": "Cityscapes",
"relatedPaper": "https://arxiv.org/pdf/2012.05258",
"rawData": "Yes"
},
{
"id": "IDDA",
"href": "https://idda-dataset.github.io/home/",
"relatedPaper": "https://arxiv.org/pdf/2004.08298.pdf",
"benchmark": "semantic segmentation",
"rawData": "Yes"
},
{
"id": "UTBM EU LTD",
"href": "https://epan-utbm.github.io/utbm_robocar_dataset/",
"relatedPaper": "https://arxiv.org/pdf/1909.03330.pdf",
"rawData": "Yes",
"publishDate": "2020-08-01"
},
{
"id": "MOTSynth",
"href": "https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=42",
"relatedPaper": "https://arxiv.org/pdf/2108.09518.pdf"
},
{
  "id": "Argoverse 2",
  "href": "https://www.argoverse.org/av2.html",
  "relatedPaper": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/4734ba6f3de83d861c3176a6273cac6d-Paper-round2.pdf"
},
{
  "id": "CODA",
  "href": "https://coda-dataset.github.io/",
  "relatedPaper": "https://arxiv.org/pdf/2203.07724.pdf",
  "benchmark": "anomaly detection"
},
{
  "id": "Synthetic Discrepancy Datasets",
  "href": "https://github.com/cvlab-epfl/detecting-the-unexpected",
  "relatedPaper": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lis_Detecting_the_Unexpected_via_Image_Resynthesis_ICCV_2019_paper.pdf"
},
{
  "id": "SODA10M",
  "href": "https://soda-2d.github.io/",
  "relatedPaper": "https://arxiv.org/pdf/2106.11118.pdf"
},
{
  "id": "A9",
  "href": "https://innovation-mobility.com/en/a9-dataset/",
  "relatedPaper": "https://arxiv.org/pdf/2204.06527.pdf"
},
{
  "id": "Rope3D",
  "href": "https://thudair.baai.ac.cn/rope",
  "relatedPaper": "https://arxiv.org/pdf/2203.13608.pdf"
},
{
  "id": "AugKITTI",
  "relatedPaper": "https://arxiv.org/pdf/2203.00214.pdf"
},
{
  "id": "MAVD Multimodal Audio-Visual Detection",
  "href": "http://multimodal-distill.cs.uni-freiburg.de/",
  "relatedPaper": "https://arxiv.org/abs/2103.01353"
},
{
  "id": "KITTI-360-APS",
  "href": "http://amodal-panoptic.cs.uni-freiburg.de/",
  "relatedPaper": "https://arxiv.org/pdf/2202.11542.pdf"
},
{
  "id": "BDD100K-APS",
  "href": "http://amodal-panoptic.cs.uni-freiburg.de/",
  "relatedPaper": "https://arxiv.org/pdf/2202.11542.pdf"
},
{
  "id": "Boreas",
  "href": "https://www.boreas.utias.utoronto.ca/#/",
  "relatedPaper": "https://arxiv.org/pdf/2203.10168.pdf"
},
{
  "id": "DGL-MOTS",
  "href": "https://goodproj13.github.io/DGL-MOTS/",
  "relatedPaper": "https://arxiv.org/pdf/2110.07790.pdf"
},
{
  "id": "OpenMPD",
  "href": "http://openmpd.com/",
  "relatedPaper": "https://ieeexplore.ieee.org/document/9682587"
},
{
  "id": "On Salience-Sensitive Sign Classification in Autonomous Vehicle Path Planning",
  "relatedPaper": "https://arxiv.org/pdf/2112.00942.pdf"
},
{
  "id": "The Autonomous Platform Inertial Dataset",
  "href": "https://github.com/ansfl/Navigation-Data-Project/",
  "relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9684368"
},
{
  "id": "PIE",
  "href": "https://data.nvision2.eecs.yorku.ca/PIE_dataset/"
},
{
"id": "SeeingThroughFog",
"href": "https://github.com/princeton-computational-imaging/SeeingThroughFog",
"size_storage": "-",
"size_hours": "-",
"frames": "1400000",
"numberOfScenes": "-",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera, radar, lidar, imu, weather sensor",
"sensorDetail": "2x stereo cameras 1920x1024 30Hz, 1 gated camera 1280x720 120Hz, 1 FMCW radar 15Hz, 2x Velodyne lidars 10Hz, 1 FIR camera 640x480 30Hz, 1 Airmar WX150 weather sensor (temperature, wind speed and humidity)",
"recordingPerspective": "Ego-perspective",
"dataType":"real",
"mapData":"No",
"benchmark": "-",
"annotations": "weather of scenes in frames",
"licensing": "Freely available for research and teaching purposes",
"relatedDatasets": "Gated2Gated, Gated2Depth, PointCloudDeNoising",
"publishDate":"2019-02-24",
"lastUpdate": "-",
"paperTitle": "Seeing Through FogWithout Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather",
"relatedPaper": "https://www.cs.princeton.edu/~fheide/AdverseWeatherFusion/figures/AdverseWeatherFusion.pdf",
"location": "Germany, Sweden, Denmark and Finland",
"rawData": "-",
"DOI": "10.1109/CVPR42600.2020.01170"
},
{
"id": "Gated2Gated",
"href": "https://github.com/princeton-computational-imaging/Gated2Gated#gated2gated--self-supervised-depth-estimation-from-gated-images",
"size_storage": "-",
"size_hours": "-",
"frames": "130000",
"numberOfScenes": "1835",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera, radar, lidar, imu, weather sensor",
"sensorDetail": "2x stereo cameras 1920x1024 30Hz, 1 gated camera 1280x720 120Hz, 1 FMCW radar 15Hz, 2x Velodyne lidars 10Hz, 1 FIR camera 640x480 30Hz, 1 Airmar WX150 weather sensor (temperature, wind speed and humidity)",
"recordingPerspective": "Ego-perspective",
"dataType":"real",
"mapData":"No",
"benchmark": "-",
"annotations": "-",
"licensing": "Freely available for research and teaching purposes",
"relatedDatasets": "SeeingThroughFog, PointCloudDeNoising, Gated2Depth",
"publishDate":"2021-12-04",
"lastUpdate": "-",
"paperTitle": "Gated2Gated: Self-Supervised Depth Estimation from Gated Images",
"relatedPaper": "https://arxiv.org/pdf/2112.02416.pdf",
"location": "Germany, Sweden, Denmark and Finland",
"rawData": "-",
"DOI": "10.48550/arXiv.2112.02416"
},
{
"id": "Gated2Depth",
"href": "https://github.com/gruberto/Gated2Depth",
"size_storage": "-",
"size_hours": "-",
"frames": "17686",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "1x Aptina AR0230 stereo camera 1920x1080 30Hz, 1x Velodyne HDL64-S3 lidar, 1x gated camera 10bit images 1280x720 120Hz",
"recordingPerspective": "Ego-perspective",
"dataType":"Real",
"mapData":"No",
"benchmark": "-",
"annotations": "-",
"licensing": "Freely available for research and teaching purposes",
"relatedDatasets": "SeeingThroughFog, PointCloudDeNoising, Gated2Gated",
"publishDate":"2020-02-13",
"lastUpdate": "2020-04-12",
"paperTitle": "Gated2Depth: Real-Time Dense Lidar From Gated Images",
"relatedPaper": "https://arxiv.org/pdf/1902.04997.pdf",
"location": "Germany, Denmark and Sweden",
"rawData": "-",
"DOI": "10.1109/ICCV.2019.00159"
},
{
"id": "RUGD: Robot Unstructured Ground Driving",
"href": "http://rugd.vision/",
"size_storage": "5.4",
"size_hours": "-",
"frames": "37000",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "1x Prosilica GT2750C camera 1376x1110, 1x Velodyne HDL-32 LiDAR, 1x Garmin GPS receiver, 1x Microstrain GX3-25 IMU",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "semantic segmentation",
"annotations": "",
"licensing": "Freely available for research purposes",
"relatedDatasets": "-",
"publishDate":"2019-11-08",
"lastUpdate": "-",
"paperTitle": "A RUGD Dataset for Autonomous Navigation and Visual Perception in Unstructured Outdoor Environments",
"relatedPaper": "http://rugd.vision/pdfs/RUGD_IROS2019.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/IROS40897.2019.8968283"
}
]
