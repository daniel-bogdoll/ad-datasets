[{
  "id": "KITTI",
  "href": "http://www.cvlibs.net/datasets/kitti/",
  "size_hours": "6",
  "size_storage": "180",
  "frames": "-",
  "numberOfScenes": "50",
  "samplingRate": "10",
  "lengthOfScenes": "-",
  "sensors": "camera, lidar, gps/imu",
  "sensorDetail": "2 greyscale cameras 1.4 MP, 2 color cameras 1.4 MP, 1 lidar 64 beams 360Â° 10Hz, 1 inertial and GPS navigation system",
  "benchmark": "stereo, optical flow, visual odometry, slam, 3d object detection, 3d object tracking",
  "annotations": "3d bounding boxes",
  "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0",
  "relatedDatasets": "Semantic KITTI, KITTI-360",
  "publishDate": "2012-03-01",
  "lastUpdate": "2021-02-01",
  "paperTitle": "Vision meets Robotics: The KITTI Dataset",
  "relatedPaper": "http://www.cvlibs.net/publications/Geiger2013IJRR.pdf",
  "location": "Karlsruhe, Germany",
  "rawData": "Yes",
  "DOI": "10.1177/0278364913491297"
},
  {
  "id":"Cityscapes",
  "href":"https://www.cityscapes-dataset.com/",
  "relatedPaper":"https://arxiv.org/abs/1604.01685",
  "paperTitle":"The Cityscapes Dataset for Semantic Urban Scene Understanding",
  "DOI": "10.1109/CVPR.2016.350"
},
{
  "id":"CARLA Real Traffic Scenarios",
  "href":"https://github.com/deepsense-ai/carla-real-traffic-scenarios",
  "relatedPaper":"https://arxiv.org/abs/2012.11329",
  "paperTitle":"CARLA Real Traffic Scenarios -- novel training ground and benchmark for autonomous driving"
},
{
  "id":"Cooperative Localization using CARLA-SUMO-ARTERY Simulators",
  "href":"https://ieee-dataport.org/documents/cooperative-localization-using-carla-sumo-artery-simulators"
},
{
  "id":"Realistic Vehicle Trajectories and Driving Parameters from CARLA Autonomous Driving Simulator",
  "href":"https://ieee-dataport.org/documents/realistic-vehicle-trajectories-and-driving-parameters-carla-autonomous-driving-simulator"
},
{
  "id":"V2I-CARLA",
  "href":"https://github.com/Yx1322441675/V2I-CARLA",
  "relatedPaper":"https://ieeexplore.ieee.org/document/9733359",
  "paperTitle":"V2I-CARLA: A Novel Dataset and a Method for Vehicle Reidentification-Based V2I Environment"
},
{
  "id":"MICC-SRI",
  "href":"https://www.micc.unifi.it/resources/datasets/semantic-road-inpainting/",
  "relatedPaper":"https://arxiv.org/abs/1805.11746",
  "paperTitle":"Semantic Road Layout Understanding by Generative Adversarial Inpainting"
},
{
  "id":"KITTI-CARLA",
  "href":"https://npm3d.fr/kitti-carla",
  "relatedPaper":"https://arxiv.org/abs/2109.00892",
  "paperTitle":"KITTI-CARLA: a KITTI-like dataset generated by CARLA Simulator"
},
{
  "id":"Romanian (European Union) Dataset of License Plates",
  "href":"https://github.com/RobertLucian/license-plate-dataset"
},
{
  "id":"THI Synthetic Automotive Dataset",
  "href":"https://www.thi.de/forschung/carissma/c-isafe/thi-synthetic-automotive-dataset/"
},
{
  "id":"THI License Plate Dataset",
  "href":"https://www.thi.de/forschung/carissma/c-isafe/thi-license-plate-dataset/",
  "relatedPaper":"https://ieeexplore.ieee.org/document/9294240",
  "paperTitle":"European Union Dataset and Annotation Tool for Real Time Automatic License Plate Detection and Blurring"
},
{
  "id":"carla-training-data",
  "href":"https://github.com/enginBozkurt/carla-training-data"
},
{
  "id":"Carla-Object-Detection-Dataset",
  "href":"https://github.com/DanielHfnr/Carla-Object-Detection-Dataset"
},
{
  "id":"Lane Detection for Carla Driving Simulator",
  "href":"https://www.kaggle.com/datasets/thomasfermi/lane-detection-for-carla-driving-simulator"
},
{
  "id":"[CARLA] Densely Annotated Driving Dataset",
  "href":"https://www.kaggle.com/datasets/albertozorzetto/carla-densely-annotated-driving-dataset"
},
{
  "id":"ARD-16",
  "href":"https://github.com/dslrproject/dslr/tree/master/Data",
  "relatedPaper":"https://arxiv.org/abs/2105.12774",
  "paperTitle":"DSLR: Dynamic to Static LiDAR Scan Reconstruction Using Adversarially Trained Autoencoder"
},
{
  "id":"CARLA-64",
  "href":"https://github.com/dslrproject/dslr/tree/master/Data",
  "relatedPaper":"https://arxiv.org/abs/2105.12774",
  "paperTitle":"DSLR: Dynamic to Static LiDAR Scan Reconstruction Using Adversarially Trained Autoencoder"
},
{
  "id":"CARLA-GEAR Semantic Segmentation",
  "href":"https://carlagear.retis.santannapisa.it/",
  "relatedPaper":"https://arxiv.org/abs/2206.04365",
  "paperTitle":"CARLA-GeAR: a Dataset Generator for a Systematic Evaluation of Adversarial Robustness of Vision Models"
},
{
  "id":"CARLA-GEAR 2D Object Detection",
  "href":"https://carlagear.retis.santannapisa.it/",
  "relatedPaper":"https://arxiv.org/abs/2206.04365",
  "paperTitle":"CARLA-GeAR: a Dataset Generator for a Systematic Evaluation of Adversarial Robustness of Vision Models"
},
{
  "id":"CARLA-GEAR Monocular Depth Estimation",
  "href":"https://carlagear.retis.santannapisa.it/",
  "relatedPaper":"https://arxiv.org/abs/2206.04365",
  "paperTitle":"CARLA-GeAR: a Dataset Generator for a Systematic Evaluation of Adversarial Robustness of Vision Models"
},
{
  "id":"CARLA-GEAR Stereo 3D Object Detection",
  "href":"https://carlagear.retis.santannapisa.it/",
  "relatedPaper":"https://arxiv.org/abs/2206.04365",
  "paperTitle":"CARLA-GeAR: a Dataset Generator for a Systematic Evaluation of Adversarial Robustness of Vision Models"
},
{
  "id":"Crosswalk Change Dataset",
  "href":"https://www.kaggle.com/datasets/buvision/crosswalkchange",
  "relatedPaper":"https://www.ri.cmu.edu/publications/towards-hd-map-updates-with-crosswalk-change-detection-from-vehicle-mounted-cameras/",
  "paperTitle":"Towards HD Map Updates with Crosswalk Change Detection from Vehicle-Mounted Cameras"
},
{
  "id":"Synthetic Fire Hydrants",
  "href":"https://www.kaggle.com/datasets/xinhez/synthetic-fire-hydrants",
  "relatedPaper":"https://ieeexplore.ieee.org/document/9564932",
  "paperTitle":"CARLA Simulated Data for Rare Road Object Detection"
},
{
  "id":"Synthetic Crosswalks",
  "href":"https://www.kaggle.com/datasets/buvision/synthetic-crosswalks",
  "relatedPaper":"https://ieeexplore.ieee.org/document/9564932",
  "paperTitle":"CARLA Simulated Data for Rare Road Object Detection"
},
{
  "id":"Ground-Challenge",
  "href":"https://github.com/sjtuyinjie/Ground-Challenge",
  "relatedPaper":"https://arxiv.org/abs/2307.03890",
  "paperTitle":"Ground-Challenge: A Multi-sensor SLAM Dataset Focusing on Corner Cases for Ground Robots"
},
{
  "id":"V2X-Seq",
  "href":"https://github.com/AIR-THU/DAIR-V2X-Seq",
  "relatedPaper":"https://arxiv.org/abs/2305.05938",
  "paperTitle":"V2X-Seq: A Large-Scale Sequential Dataset for Vehicle-Infrastructure Cooperative Perception and Forecasting"
},
{
  "id":"commaSteeringControl",
  "href":"https://github.com/commaai/comma-steering-control"
},
{
  "id":"commavq",
  "href":"https://github.com/commaai/commavq"
},
{
  "id":"MAVAD",
  "href":"https://gitlab.au.dk/maleci/audiovisualanomalydetection",
  "relatedPaper":"https://arxiv.org/abs/2305.15084",
  "paperTitle":"Audio-Visual Dataset and Method for Anomaly Detection in Traffic Videos"
},
{
  "id":"OpenLane-V2",
  "href":"https://github.com/OpenDriveLab/OpenLane-V2",
  "relatedPaper":"https://arxiv.org/abs/2304.10440",
  "paperTitle":"OpenLane-V2: A Topology Reasoning Benchmark for Scene Understanding in Autonomous Driving"
},
{
  "id":"Paris-CARLA-3D",
  "href":"https://npm3d.fr/paris-carla-3d",
  "relatedPaper":"https://arxiv.org/abs/2111.11348",
  "paperTitle":"Paris-CARLA-3D: A Real and Synthetic Outdoor Point Cloud Dataset for Challenging Tasks in 3D Mapping"
},
{
  "id":"UrbanLaneGraph",
  "href":"http://urbanlanegraph.cs.uni-freiburg.de/",
  "relatedPaper":"https://arxiv.org/abs/2302.06175",
  "paperTitle":"Learning and Aggregating Lane Graphs for Urban Automated Driving"
},
{
  "id":"PeSOTIF",
  "href":"https://github.com/SOTIF-AVLab/PeSOTIF",
  "relatedPaper":"https://arxiv.org/abs/2211.03402",
  "paperTitle":"PeSOTIF: a Challenging Visual Dataset for Perception SOTIF Problems in Long-tail Traffic Scenarios"
},
{
  "id":"IMPTC",
  "paperTitle":"The IMPTC Dataset: An Infrastructural Multi-Person Trajectory and Context Dataset"
},
{
  "id":"LUCOOP",
  "href":"https://data.uni-hannover.de/dataset/lucoop-leibniz-university-cooperative-perception-and-urban-navigation-dataset",
  "paperTitle":"LUCOOP: Leibniz University Cooperative Perception and Urban Navigation Dataset"
},
{
  "id":"DSIOD",
  "href":"https://github.com/wnklmx/DSIOD",
  "relatedPaper":"https://arxiv.org/abs/2110.02892",
  "paperTitle":"Probabilistic Metamodels for an Efficient Characterization of Complex Driving Scenarios"
},
{
  "id":"Zenseact Open",
  "href":"https://github.com/zenseact/zod",
  "relatedPaper":"https://arxiv.org/abs/2305.02008",
  "paperTitle":"Zenseact Open Dataset: A large-scale and diverse multimodal dataset for autonomous driving"
},
{
  "id":"UPCT",
  "href":"https://figshare.com/s/4b9a25a958c3ec578362",
  "relatedPaper":"https://www.mdpi.com/1424-8220/23/4/2009",
  "paperTitle":"Autonomous Vehicle Dataset with Real Multi-Driver Scenes and Biometric Data"
},
{
  "id":"Indian Vehicle Dataset",
  "href":"https://www.kaggle.com/datasets/dataclusterlabs/indian-vehicle-dataset"
},
{
  "id":"V2V4Real",
  "href":"https://github.com/ucla-mobility/V2V4Real",
  "relatedPaper":"https://arxiv.org/abs/2303.07601",
  "paperTitle":"V2V4Real: A Real-world Large-scale Dataset for Vehicle-to-Vehicle Cooperative Perception"
},
{
  "id":"Surat Trajectory Data",
  "href":"https://www.researchgate.net/publication/351108614_Extended_trajectory_data_from_Indian_Traffic_at_three_flow_conditions",
  "paperTitle":"Extended trajectory data from Indian Traffic at three flow conditions"
},
{
  "id":"3DHD CityScenes",
  "href":"https://www.hi-drive.eu/downloads/#data",
  "relatedPaper":"https://ieeexplore.ieee.org/document/9921866",
  "paperTitle":"3DHD CityScenes: High-Definition Maps in High-Density Point Clouds"
},
{
  "id":"LiDAR-CS",
  "href":"https://github.com/LiDAR-Perception/LiDAR-CS",
  "relatedPaper":"https://arxiv.org/abs/2301.12515",
  "paperTitle":"LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection"
},
{
  "id":"UofTPed50",
  "href":"https://www.autodrive.utoronto.ca/uoftped50",
  "relatedPaper":"https://arxiv.org/abs/1905.08758",
  "paperTitle":"aUToTrack: A Lightweight Object Detection and Tracking System for the SAE AutoDrive Challenge"
}, 
{
  "id":"Apollo-SouthBay",
  "href":"https://developer.apollo.auto/southbay.html",
  "relatedPaper":"https://ieeexplore.ieee.org/abstract/document/8954371/",
  "paperTitle":"L3-net: Towards learning based lidar localization for autonomous driving"
},
{
  "id":"Apollo-DaoxiangLake",
  "href":"https://developer.apollo.auto/daoxianglake.html",
  "relatedPaper":"https://arxiv.org/abs/2003.03026",
  "paperTitle":"DA4AD: End-to-end Deep Attention-based Visual Localization for Autonomous Driving"
},
{
  "id":"Fallen Person detection with Driving scenes (FPD-set)",
  "href":"https://github.com/suhyeonlee/FPD",
  "relatedPaper":"https://www.sciencedirect.com/science/article/abs/pii/S0957417422022606",
  "paperTitle":"Fallen person detection for autonomous driving"
},
{
  "id":"Car Crash (CCD)",
  "href":"https://github.com/Cogito2012/CarCrashDataset",
  "relatedPaper":"https://arxiv.org/abs/2209.02438",
  "paperTitle":"Threat Detection In Self-Driving Vehicles Using Computer Vision"
},
{
  "id":"Multiple Uncertainties for Autonomous Driving (MUAD)",
  "href":"https://muad-dataset.github.io/",
  "relatedPaper":"https://arxiv.org/abs/2203.01437",
  "paperTitle":"MUAD: Multiple Uncertainties for Autonomous Driving, a benchmark for multiple uncertainty types and tasks"
},
{
  "id":"Pascal-WD",
  "href":"https://github.com/pb-brainiac/semseg_od",
  "relatedPaper":"https://arxiv.org/abs/1908.01098",
  "paperTitle":"Simultaneous Semantic Segmentation and Outlier Detection in Presence of Domain Shift"
},
{
  "id":"Vistas-NP",
  "href":"https://github.com/matejgrcic/Vistas-NP",
  "relatedPaper":"https://arxiv.org/abs/2011.11094",
  "paperTitle":"Dense open-set recognition with synthetic outliers generated by Real NVP"
},
{
  "id":"PP4AV",
  "href":"https://github.com/khaclinh/pp4av",
  "relatedPaper":"https://openaccess.thecvf.com/content/WACV2023/papers/Trinh_PP4AV_A_Benchmarking_Dataset_for_Privacy-Preserving_Autonomous_Driving_WACV_2023_paper.pdf",
  "paperTitle":"PP4AV: A benchmarking Dataset for Privacy-preserving Autonomous Driving"
},
{
  "id": "aiMotive",
  "href": "https://github.com/aimotive/aimotive_dataset",
  "size_storage": "85",
  "size_hours": "-",
  "frames": "26583",
  "numberOfScenes": "176",
  "samplingRate": "-",
  "lengthOfScenes": "15",
  "sensors": "camera, lidar, radar, gnss",
  "sensorDetail": "2x fisheye cameras 1920x1080 30-60Hz, 2x pinhole cameras 2896x1876 30-40Hz, 1x LiDAR 360Â° 10Hz, 2x Radar 18Hz, 1x GNSS+INS 100Hz",
  "recordingPerspective": "ego-perspective",
  "dataType":"Real",
  "mapData": "No",
  "benchmark": "-",
  "annotations": "3d bounding boxes",
  "licensing": "Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",
  "relatedDatasets": "-",
  "publishDate":"2022-11-17",
  "lastUpdate": "-",
  "paperTitle": "aiMotive Dataset: A Multimodal Dataset for Robust Autonomous Driving with Long-Range Perception",
  "relatedPaper": "https://arxiv.org/pdf/2211.09445.pdf",
  "location": "USA (California), Austria and Hungary",
  "rawData": "-",
  "DOI": "10.48550/arXiv.2211.09445"
},
{
  "id": "DANGER-vKITTI2",
  "href": "https://github.com/jayhsu0627/DANGER",
  "paperTitle": "A Framework for Generating Dangerous Scenes for Testing Robustness",
  "relatedPaper": "https://openreview.net/pdf?id=ZjN2AuXgu1"
},
{
  "id": "DANGER-vKITTI",
  "href": "https://github.com/jayhsu0627/DANGER",
  "paperTitle": "A Framework for Generating Dangerous Scenes for Testing Robustness",
  "relatedPaper": "https://openreview.net/pdf?id=ZjN2AuXgu1"
},
{
  "id": "I see you",
  "href": "https://github.com/hvzzzz/Vehicle_Trajectory_Dataset",
  "paperTitle": "I see you: A Vehicle-Pedestrian Interaction Dataset from Traffic Surveillance Cameras",
  "relatedPaper": "https://arxiv.org/abs/2211.09342"
},
{
"id": "nuScenes",
"href": "https://www.nuscenes.org/",
"size_hours": "15",
"size_storage": "-",
"frames": "1400000",
"numberOfScenes": "1000",
"samplingRate": "-",
"lengthOfScenes": "20",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "1x lidar 32 channels 360Â° 20Hz, 5x long range radar 13Hz, 6x camera 1600x1200 12Hz, 1x gps/imu 1000Hz",
"benchmark": "3d object detection, tracking, trajectory (prediction), lidar segmentation, panoptic segmentation & tracking",
"annotations": "semantic category, attributes, 3d bounding boxes ",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",
"relatedDatasets": "nuImages",
"publishDate": "2019-03-01",
"lastUpdate": "2020-12-01",
"paperTitle": "nuScenes: A multimodal dataset for autonomous driving",
"relatedPaper": "https://arxiv.org/pdf/1903.11027.pdf",
"location": "Boston, USA and Singapore",
"rawData": "Yes",
"DOI": "10.1109/cvpr42600.2020.01164"
},
{
"id": "Oxford Robot Car", 
"href": "https://robotcar-dataset.robots.ox.ac.uk/",
"size_hours": "210",
"size_storage": "23150",
"frames": "-",
"numberOfScenes": "100",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, ins/gps",
"sensorDetail": "1x camera Bumblebee XB3 1280x960x3 16Hz, 3x camera Grasshopper2 1024x1024 12Hz, 2x lidar SICK LMS-151 270Â° 50Hz, 1x lidar SICK LD-MRS 90Â° 4 plane 12.5Hz, 1x NovAtel SPAN-CPT ALIGN 50Hz GPS+INS",
"benchmark": "-",
"annotations": "-",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International",
"relatedDatasets": "Oxford Radar Robot Car",
"publishDate": "2016-11-01",
"lastUpdate": "2020-02-01",
"paperTitle": "1 Year, 1000km: The Oxford RobotCar Dataset",
"relatedPaper": "https://journals.sagepub.com/doi/10.1177/0278364916679498",
"location": "Oxford, UK",
"rawData": "Yes",
"DOI": "10.1177/0278364916679498"
},
{
"id": "Waymo Open Perception", 
"href": "https://waymo.com/open/data/perception/",
"size_hours":  "10.83", 
"size_storage": "-",
"frames": "390000",
"numberOfScenes": "1950",
"samplingRate": "10",
"lengthOfScenes": "20",
"sensors": "camera, lidar",
"sensorDetail": "5x cameras (front and sides) 1920x1280 & 1920x1040, 1x mid-range lidar, 4x short-range lidars",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "-",
"benchmark": "2d detection, 3d detection, 2d tracking, 3d tracking",
"annotations": "3d bounding boxes (lidar), 2d bounding boxes (camera)",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "Waymo Open Motion",
"publishDate": "2019-08-01",
"lastUpdate": "2020-03-01",
"paperTitle": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
"relatedPaper": "https://arxiv.org/pdf/1912.04838.pdf",
"location": "San Francisco, Mountain View, Los Angeles, Detroit, Seattle and Phoenix, USA",
"rawData": "Yes",
"DOI": "10.1109/CVPR42600.2020.00252"
},
{
"id": "Argoverse Motion Forecasting", 
"href": "https://www.argoverse.org/",
"size_storage": "4.81",
"size_hours": "320",
"frames": "16227850",
"numberOfScenes": "324557",
"samplingRate": "10",
"lengthOfScenes": "5",
"sensors": "camera, lidar, gps",
"sensorDetail": "2x lidar 32 beam 40Â° 10Hz, 7x ring cameras 1920x1200 combined 360Â° 30Hz, 2x front-view facing stereo cameras 0.2986m baseline 2056x2464 5Hz",
"benchmark": "forecasting",
"annotations": "semantic vector map, rasterized map, trajectories",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",
"relatedDatasets": "Argoverse 3D Tracking",
"publishDate": "2019-06-01",
"lastUpdate": "-",
"paperTitle": "Argoverse: 3D Tracking and Forecasting with Rich Maps",
"relatedPaper": "https://arxiv.org/pdf/1911.02620.pdf",
"location": "Miami and Pittsburgh, USA",
"rawData": "No",
"DOI": "10.1109/CVPR.2019.00895"
},
{
"id": "Argoverse 3D Tracking", 
"href": "https://www.argoverse.org/",
"size_storage": "254.4",
"size_hours": "1",
"frames": "44000",
"numberOfScenes": "113",
"samplingRate": "30",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps",
"sensorDetail": "2x lidar 40Â° 10Hz, 7x ring cameras 1920x1200 combined 360Â° 30Hz, 2x front-view facing stereo cameras 2056x2464 5Hz",
"benchmark": "tracking",
"annotations": "semantic vector map, rasterized map, 3d bounding boxes",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",
"relatedDatasets": "Argoverse Motion Forecasting",
"publishDate": "2019-06-01",
"lastUpdate": "-",
"paperTitle": "Argoverse: 3D Tracking and Forecasting with Rich Maps",
"relatedPaper": "https://arxiv.org/pdf/1911.02620.pdf",
"location": "Miami and Pittsburgh, USA",
"rawData": "Yes",
"DOI": "10.1109/CVPR.2019.00895"
},
{
"id": "Semantic KITTI", 
"href": "http://www.semantic-kitti.org/",
"size_storage": "-",
"size_hours": "-",
"frames": "43552", 
"numberOfScenes": "21",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "lidar",
"sensorDetail": "Velodyne HDL-64E from sequences of the odometry 'benchmark' of the KITTI Vision Benchmark with 360Â° view",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "semantic segmentation, panoptic segmentation, 4D panoptic segmentation, moving object segmentation, semantic scene completion",
"annotations": "semantic segmentation",
"licensing": "Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) ",
"relatedDatasets": "KITTI",
"publishDate": "2019-07-01",
"lastUpdate": "2021-02-01",
"paperTitle": "SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences",
"relatedPaper": "https://arxiv.org/abs/1904.01416.pdf",
"location": "Karlsruhe, Germany",
"rawData": "No",
"DOI": "10.1109/ICCV.2019.00939"
},
{
"id": "ApolloScape", 
"href": "http://apolloscape.auto/",
"size_hours": "100", 
"size_storage": "-",
"frames": "143906",
"numberOfScenes": "-",
"samplingRate": "30",
"lengthOfScenes": "-",
"sensors": "camera, lidar, imu/gnss",
"sensorDetail": "2x VUX-1HA laser scanners 360Â°, 1x VMX-CS6 camera system, 1x measuring head with gnss/imu, 2x high frontal cameras 3384 Ã2710",
"benchmark": "2d image parsing, 3d car instance understanding, landmark segmentation, self-localization, trajectory prediction, 3d detection, 3d tracking, stereo",
"annotations": "high density 3d point cloud map, per-pixel, per-frame semantic image label, lane mark label semantic instance segmentation, geo-tagged",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "-",
"publishDate": "2018-03-01",
"lastUpdate": "2020-09-01",
"paperTitle": "The ApolloScape Open Dataset for Autonomous Driving and its Application",
"relatedPaper": "https://arxiv.org/pdf/1803.06184.pdf",
"location": "Beijing, Shanghai and Shenzhen, China",
"rawData": "Yes"
},
{
"id": "BDD100k", 
"href": "https://www.bdd100k.com/",
"size_storage": "1800",
"size_hours": "1111",
"frames": "120000000",
"numberOfScenes": "100000",
"samplingRate": "30",
"lengthOfScenes": "40",
"sensors": "camera, gps/imu",
"sensorDetail": "crowd-sourced therefore no fixed setup, camera (720p) and gps/imu",
"benchmark": "object detection, instance segmentation, multiple object tracking, segmentation tracking, semantic segmentation, lane marking, drivable area, image tagging, imitation learning, domain adaption",
"annotations": "bounding boxes, instance segmentation, semantic segmentation, box tracking, semantic tracking, drivable area",
"licensing": "Individual License (https://doc.bdd100k.com/license.html)",
"relatedDatasets": "-",
"publishDate": "2020-04-01",
"lastUpdate": "-",
"paperTitle": "BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning",
"relatedPaper": "https://arxiv.org/pdf/1805.04687.pdf",
"location": "New York, Berkeley, San Francisco and Bay Area, USA",
"rawData": "Yes"
},
{
"id": "WildDash", 
"href": "https://wilddash.cc/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "156",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "various sources, e.g. YouTube",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "semantic segmentation, instance segmentation, panoptic segmentation",
"annotations": "semantic segmentation, instance segmentation",
"licensing": "CC-BY-NC 4.0 ",
"relatedDatasets": "-",
"publishDate": "2018-02-01",
"lastUpdate": "2020-06-01",
"paperTitle": "WildDash - Creating Hazard-Aware Benchmarks",
"relatedPaper": "https://openaccess.thecvf.com/content_ECCV_2018/papers/Oliver_Zendel_WildDash_-_Creating_ECCV_2018_paper.pdf",
"location": "All over the world",
"rawData": "Yes",
"DOI": "10.1007/978-3-030-01231-1_25"
},
{
"id": "WildDash 2", 
"href": "https://wilddash.cc/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "5032",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "Publically available dash cam videos",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "semantic, panoptic and instance segmentation",
"annotations": "semantic, panoptic and instance segmentation",
"licensing": "Available upon registration",
"relatedDatasets": "WildDash",
"publishDate":"2022-06-18",
"lastUpdate": "-",
"paperTitle": "Unifying Panoptic Segmentation for Autonomous Driving",
"relatedPaper": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zendel_Unifying_Panoptic_Segmentation_for_Autonomous_Driving_CVPR_2022_paper.pdf",
"location": "All over the world",
"rawData": "Yes",
"DOI": "10.1109/CVPR52688.2022.02066"
},
{
"id": "Lyft Level5 Prediction",
"href": "https://level-5.global/data/prediction/",
"size_hours": "1118",
"size_storage": "-",
"frames": "42500000",
"numberOfScenes": "170000",
"samplingRate": "10",
"lengthOfScenes": "25",
"sensors": "camera, lidar, radar",
"sensorDetail": "7 cameras with 360Â° view, 3 lidars with 40-64 channels at 10Hz, 5 radars",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "semantic map \"annotations\", trajectories",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 (CC-BY-NC-SA-4.0)",
"relatedDatasets": "Lyft Level5 Perception",
"publishDate": "2020-06-01",
"lastUpdate": "-",
"paperTitle": "One Thousand and One Hours: Self-driving Motion Prediction Dataset",
"relatedPaper": "https://arxiv.org/abs/2006.14480",
"location": "Palo Alto, USA",
"rawData": "No",
"DOI": "10.48550/arXiv.2006.14480"
},
{
"id": "Cityscapes 3D",
"href": "https://www.cityscapes-dataset.com/",
"size_hours": "-",
"size_storage": "63.141",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "17",
"lengthOfScenes": "1.8",
"sensors": "camera, gps, thermometer",
"sensorDetail": "stereo cameras 22 cm baseline 17Hz, odometry from in-vehicle \"sensors\" & outs\"id\"e temperature & GPS tracks",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "pixel-level semantic labeling, instance-level semantic labeling, panoptic semantic sabeling 3d vehicle detection",
"annotations": "dense semantic segmentation, instance segmentation for vehicles & people, 3d bounding boxes",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "-",
"publishDate": "2016-02-01",
"lastUpdate": "2020-10-01",
"paperTitle": "Cityscapes 3D: Dataset and Benchmark for 9 DoF Vehicle Detection",
"relatedPaper": "https://arxiv.org/pdf/2006.07864.pdf",
"location": "50 cities in Germany and neighboring countries",
"rawData": "Yes",
"DOI": "10.48550/arXiv.2006.07864"
},
{
"id": "Lyft Level5 Perception",
"href": "https://level-5.global/data/perception/",
"size_hours": "2.5",
"size_storage": "-",
"frames": "-",
"numberOfScenes": "366",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "3d bounding boxes, rasterised road geometry",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 (CC-BY-NC-SA-4.0)",
"relatedDatasets": "Lyft Level5 Prediction",
"publishDate": "2019-07-01",
"lastUpdate": "-",
"paperTitle": "One Thousand and One Hours: Self-driving Motion Prediction Dataset",
"relatedPaper": "https://arxiv.org/abs/2006.14480",
"location": "Palo Alto, USA",
"rawData": "Yes",
"DOI": "10.48550/arXiv.2006.14480"
},
{
"id": "nuImages",
"href": "https://www.nuscenes.org/nuimages",
"size_hours": "150",
"size_storage": "-",
"frames": "1200000",
"numberOfScenes": "93000",
"samplingRate": "2",
"lengthOfScenes": "-",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "1x lidar 32 channels 360Â° 20Hz, 5x long range radar 13Hz, 6x camera 1600x1200 12Hz, 1x gps/imu 1000Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "-",
"benchmark": "-",
"annotations": "instance masks, 2d bounding boxes, semantic segmentation masks, attribute annotations",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",
"relatedDatasets": "nuScenes",
"publishDate": "2020-07-01",
"lastUpdate": "-",
"paperTitle": "nuScenes: A multimodal dataset for autonomous driving",
"relatedPaper": "https://arxiv.org/pdf/1903.11027.pdf",
"location": "Boston, USA and Singapore",
"rawData": "Yes",
"DOI": "10.1109/cvpr42600.2020.01164"
},
{
"id": "PandaSet",
"href": "https://pandaset.org/",
"size_hours": "0.23",
"size_storage": "-",
"frames": "48000",
"numberOfScenes": "103",
"samplingRate": "-",
"lengthOfScenes": "8",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "5x wide angle cameras 1920x1080 10Hz, 1x long focus camera 1920x1080 10Hz, 1x mechanical spinning LiDAR 64 channels 360Â° 10Hz, 1x forward-facing LiDAR 150 channels 60Â° 10Hz1x mechanical spinning LiDAR, 1x forward-facing LiDAR, 6x cameras, on-board GPS/IMU",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "3d bounding boxes, attributes, point cloud segmentation ",
"licensing": "Creative Commons Attribution 4.0 International Public (CC BY 4.0)",
"relatedDatasets": "-",
"publishDate": "2020-04-01",
"lastUpdate": "-",
"paperTitle": "PandaSet: Advanced Sensor Suite Dataset for Autonomous Driving",
"relatedPaper": "https://arxiv.org/abs/2112.12610",
"location": "San Francisco and El Camina Real, USA",
"rawData": "Yes",
"DOI": "-"
},
{
"id": "Waymo Open Motion",
"href": "https://waymo.com/open/data/motion/",
"size_hours":  "574",
"size_storage": "-",
"frames": "20670800",
"numberOfScenes": "103354",
"samplingRate": "10",
"lengthOfScenes": "20",
"sensors": "camera, lidar",
"sensorDetail": "5x cameras, 5x lidar",
"recordingPerspective": "-",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "motion prediction, interaction prediction",
"annotations": "3d bounding boxes, 3d hd map information",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "Waymo Open Perception",
"publishDate": "2021-03-01",
"lastUpdate": "2021-09-01",
"paperTitle": "Large Scale Interactive Motion Forecasting for Autonomous Driving : The Waymo Open Motion Dataset",
"relatedPaper": "https://arxiv.org/pdf/2104.10133.pdf",
"location": "San Francisco, Mountain View, Los Angeles, Detroit, Seattle and Phoenix, USA",
"rawData": "No",
"DOI": "10.1109/ICCV48922.2021.00957"
},
{
"id": "openDD",
"href": "https://l3pilot.eu/data/opendd",
"size_storage": "-",
"size_hours": "62.7",
"frames": "6771600",
"numberOfScenes": "501",
"samplingRate": "30",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "DJI Phantom 4 3840Ã2160 camera drone",
"recordingPerspective": "Bird's Eye",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "trajectory predictions",
"annotations": "2d bounding boxes, trajectories",
"licensing": "Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) ",
"relatedDatasets": "-",
"publishDate": "2020-09-01",
"lastUpdate": "-",
"paperTitle": "openDD: A Large-Scale Roundabout Drone Dataset",
"relatedPaper": "https://arxiv.org/pdf/2007.08463.pdf",
"location": "Wolfsburg and Ingolstadt, Germany",
"rawData": "Yes",
"DOI": "10.1109/ITSC45102.2020.9294301"
},
{
"id": "RoadAnomaly21",
"href": "https://segmentmeifyoucan.com/datasets",
"size_storage": "0.05",
"size_hours": "-",
"frames": "100",
"numberOfScenes": "100",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "images from web resources 2048x1024 & 1280x720",
"recordingPerspective": "ego-perspective",
"dataType":"Real and Synthetic",
"mapData": "No",
"benchmark": "anomaly detection",
"annotations": "semantic segmentation",
"licensing": "various, see \"https://github.com/SegmentMeIfYouCan/road-anomaly-\"benchmark\"/blob/master/doc/RoadAnomaly/credits.txt\" for detail",
"relatedDatasets": "RoadObstacle21",
"publishDate": "2021-04-01",
"lastUpdate": "-",
"paperTitle": "SegmentMeIfYouCan: A Benchmark for Anomaly Segmentation",
"relatedPaper": "https://arxiv.org/pdf/2104.14812.pdf",
"location": "-",
"rawData": "Yes",
"DOI": "10.48550/arXiv.2104.14812"
},
{
"id": "Comma2k19",
"href": "https://github.com/commaai/comma2k19",
"size_storage": "100",
"size_hours": "33.65",
"frames": "-",
"numberOfScenes": "2019",
"samplingRate": "-",
"lengthOfScenes": "60",
"sensors": "camera, radar, gnss/imu",
"sensorDetail": "two different car types, 1x road-facing camera Sony IMX2984 20Hz, 1x gnss u-blox M8 chip5 10Hz, gyro and accelerometer data LSM6DS3 100Hz, magnetometer data AK09911 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "MIT",
"relatedDatasets": "-",
"publishDate": "2018-12-01",
"lastUpdate": "-",
"paperTitle": "A Commute in Data: The comma2k19 Dataset",
"relatedPaper": "https://arxiv.org/abs/1812.05752",
"location": "California's 280 highway, USA",
"rawData": "Yes",
"DOI": "10.48550/arXiv.1812.05752"
},
{
"id":"Comma2k19 LD",
"href":"https://github.com/ASGuard-UCI/ld-metric",
"relatedPaper":"https://arxiv.org/abs/2203.16851",
"paperTitle":"Towards Driving-Oriented Metric for Lane Detection Models"
},
{
"id": "KITTI-360",
"href": "http://www.cvlibs.net/datasets/kitti-360/",
"size_storage": "-",
"size_hours": "-",
"frames": "400000",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "2x 180Â° fisheye camera, 1x 90Â° perspective stereo camera, 1x Velodyne HDL-64E & SICK LMS 200 laser scanning unit in pushbroom configuration",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic instance segmentation",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0",
"relatedDatasets": "KITTI",
"publishDate": "2015-11-01",
"lastUpdate": "2021-04-01",
"paperTitle": "Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer",
"relatedPaper": "https://arxiv.org/abs/1511.03240",
"location": "Karlsruhe, Germany",
"rawData": "Yes",
"DOI": "10.1109/CVPR.2016.401"
},
{
"id": "Fishyscapes",
"href": "https://fishyscapes.com/",
"size_storage":"-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "based on the validation set of Cityscapes overlayed with anomalous objects and the original LostAndFound with extended pixel-wise annotations",
"recordingPerspective": "ego-perspective",
"dataType":"Real and Synthetic",
"mapData": "No",
"benchmark": "anomaly detection, semantic segmentation",
"annotations": "semantic segmentation",
"licensing": "-",
"relatedDatasets": "Cityscapes, LostAndFound",
"publishDate": "2019-09-01",
"lastUpdate": "-",
"paperTitle": "The Fishyscapes Benchmark: Measuring Blind Spots in Semantic Segmentation",
"relatedPaper": "https://arxiv.org/pdf/1904.03215.pdf",
"location": "-",
"rawData": "No",
"DOI": "10.48550/arXiv.1904.03215"
},
{
"id": "LostAndFound",
"href": "http://www.6d-vision.com/lostandfounddataset",
"size_storage": "-",
"size_hours": "-",
"frames": "21040",
"numberOfScenes": "112",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "stereo camera setup baseline 21cm 2048x1024",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "-",
"benchmark": "anomaly detection",
"annotations": "semantic segmentation",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "-",
"publishDate": "2016-09-01",
"lastUpdate": "-",
"paperTitle": "Lost and Found: Detecting Small Road Hazards for Self-Driving Vehicles",
"relatedPaper": "https://arxiv.org/pdf/1609.04653.pdf",
"location": "-",
"rawData": "Yes",
"DOI": "10.1109/IROS.2016.7759186"
},
{
"id": "KAIST Multi-Spectral Day/Night",
"href": "http://multispectral.kaist.ac.kr",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "25",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu, thermal camera",
"sensorDetail": "2x PointGrey Flea3 RGB camera 1280x960, 1x FLIR A655Sc thermal camera 640x480 50Hz, 1x Velodyne HDL-32E 3D LiDAR 360Â° 32 beams 10Hz, 1x OXTS RT2002 gps/ins 100Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "object detection, vision sensor enhancement, depth estimation, multi-spectral colorization",
"annotations": "dense depth map, bounding boxes",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0",
"relatedDatasets": "-",
"publishDate": "2017-12",
"lastUpdate": "-",
"paperTitle": "KAIST Multi-Spectral Day/Night Data Set for Autonomous and Assisted Driving",
"relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8293689",
"location": "-",
"rawData": "Yes",
"DOI": "10.1109/TITS.2018.2791533"
},
{
"id": "A2D2",
"href": "https://www.a2d2.audi/a2d2/en.html",
"size_storage": "2300",
"size_hours": "-",
"frames": "433833",
"numberOfScenes": "3",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "5x lidar 16 channels 360Â° 10Hz, 1x front centre camera 1920x1208 30Hz, 5x surround cameras1920x1208 30Hz, vehicle bus data",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "semantic segmentation, point cloud segmentation, instance segmentation, 3d bounding boxes",
"licensing": "Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0)",
"relatedDatasets": "-",
"publishDate": "2020-04-01",
"lastUpdate": "-",
"paperTitle": "A2D2: Audi Autonomous Driving Dataset",
"relatedPaper": "https://arxiv.org/pdf/2004.06320.pdf",
"location": "Three cities in the south of Germany",
"rawData": "Yes",
"DOI": "10.48550/arXiv.2004.06320"
},
{
"id": "Caltech Pedestrian",
"href": "http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/",
"size_storage": "-",
"size_hours": "10",
"frames": "1000000",
"numberOfScenes": "137",
"samplingRate": "30",
"lengthOfScenes": "60",
"sensors": "camera",
"sensorDetail": "1x camera 640x480 30Hz",
"benchmark": "pedestrian detection",
"annotations": "bounding boxes",
"licensing": "-",
"relatedDatasets": "-",
"publishDate": "2010-03-01",
"lastUpdate": "2019-01-01",
"paperTitle": "Pedestrian Detection: A Benchmark",
"relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5206631",
"location": "Loa Angeles, USA",
"rawData": "Yes",
"DOI": "10.1109/CVPR.2009.5206631"
},
{
"id": "Udacity",
"href": "https://github.com/udacity/self-driving-car/",
"size_storage": "223",
"size_hours": "10",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "monocular color camera 1920x1200, velodyne 32 lidar, gps/imu",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding boxes",
"licensing": "MIT",
"relatedDatasets": "-",
"publishDate": "2016-09-01",
"lastUpdate": "-",
"paperTitle": "-",
"relatedPaper": "-",
"location": "-",
"rawData": "True",
"DOI": "-"
},
{
"id": "Ford Autonomous Vehicle",
"href": "https://avdata.ford.com/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "4x HDL-32E Lidars, 4x Flea3 GigE Point Grey Cameras in stereo pairs (front & back) 80Â° 15Hz,2x Flea3 GigE Point Grey Cameras (sides) 80Â° 15Hz, 1x Flea3 GigE Point Grey Camera 40Â° 7Hz, 1x Applanix POS LV gps/imu",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "3d point cloud maps, ground reflectivity map",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International",
"relatedDatasets": "-",
"publishDate": "2020-03-01",
"lastUpdate": "-",
"paperTitle": "",
"relatedPaper": "https://arxiv.org/abs/2003.07969",
"location": "Michigan, USA",
"rawData": "True",
"DOI": "10.1177/0278364920961451"
},
{
"id": "INTERACTION",
"href": "https://interaction-dataset.com/",
"size_storage": "-",
"size_hours": "16.5",
"frames": "594588",
"numberOfScenes": "-",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "drones & traffic cameras 3840x2160 30Hz downscaled to 10Hz",
"recordingPerspective": "Birdâs Eye",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "motion prediction",
"annotations": "2d bounding boxes, semantic map, motion/trajectories",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "-",
"publishDate": "2019-09-01",
"lastUpdate": "-",
"paperTitle": "INTERACTION Dataset: An INTERnational, Adversarial and Cooperative moTION Dataset in Interactive Driving Scenarios with Semantic Maps",
"relatedPaper": "https://arxiv.org/pdf/1910.03088.pdf",
"location": "USA, China, Germany and Bulgaria",
"rawData": "Yes",
"DOI": "-"
},
{
"id": "MCity Data Collection",
"href": "https://arxiv.org/pdf/1912.06258.pdf",
"size_storage": "11000",
"size_hours": "50",
"frames": "-",
"numberOfScenes": "255",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "3x Velodyne Ultra Puck VLP-32C lidar 10Hz, 2x forward-facing cameras 30Â° 1080P 30Hz,1x backward-facing camera 90Â° 1080P 30Hz, 1x cabin pose camera 1280Ã1080 30Hz, 1x cabin head/eyeball camera 640P 30Hz, 1x Ibeo four beam LUX sensor 25Hz, 1x Delphi ESR 2.5 Radar 90Â° 20Hz,1x NovAtel FlexPak6 with IMU-IGM-S1 and 4G cellular for RTK GPS single antenna 1Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "semantic segmentation of objects, traffic lights, traffic signs, lanes",
"licensing": "-",
"relatedDatasets": "-",
"publishDate": "2019-12-01",
"lastUpdate": "-",
"paperTitle": "Mcity Data Collection for Automated Vehicles Study",
"relatedPaper": "https://arxiv.org/pdf/1912.06258.pdf",
"location": "Ann Arbor, USA",
"rawData": "Yes",
"DOI": "10.48550/arXiv.1912.06258"
},
{
"id": "Oxford Radar Robot Car",
"href": "https://oxford-robotics-institute.github.io/radar-robotcar-dataset/",
"size_storage": "4700",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "32",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "1 x Navtech CTS350-X Millimetre-Wave FMCW radar 4 Hz, 2 x Velodyne HDL-32E LIDAR 360Â°32 planes 20 Hz, 1 x Point Grey Bumblebee XB3 trinocular stereo camera 1280Ã960Ã3 16 Hz 66Â°3 x Point Grey Grasshopper2 1024Ã1024 11.1 Hz 180Â°, 2 x SICK LMS-151 2D LIDAR 270Â° 50Hz, 1 x NovAtel SPAN-CPT ALIGN inertial and GPS navigation system 6 axis 50Hz,",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "ground truth data",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International",
"relatedDatasets": "Oxford Robot Car",
"publishDate": "2020-02-01",
"lastUpdate": "-",
"paperTitle": "The Oxford Radar RobotCar Dataset: A Radar Extension to the Oxford RobotCar Dataset",
"relatedPaper": "https://arxiv.org/pdf/1909.01300.pdf",
"location": "Oxford",
"rawData": "Yes",
"DOI": "10.1109/ICRA40945.2020.9196884"
},
{
"id": "NightOwls",
"href": "https://www.nightowls-dataset.org/",
"size_storage": "-",
"size_hours": "5.17",
"frames": "279000",
"numberOfScenes": "40",
"samplingRate": "15",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x industry standard 1024x640 camera",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "pedestrian detection, object detection",
"annotations": "bounding boxes, attributes, temporal tracking annotations",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "-",
"publishDate": "2018-12-01",
"lastUpdate": "-",
"paperTitle": "NightOwls: A Pedestrians at Night Dataset",
"relatedPaper": "https://www.robots.ox.ac.uk/~vgg/publications/2018/Neumann18b/neumann18b.pdf",
"location": "Several cities across Europe",
"rawData": "Yes",
"DOI": "10.1007/978-3-030-20887-5_43"
},
{
"id": "H3D",
"href": "https://usa.honda-ri.com/H3D",
"size_storage": "-",
"size_hours": "0.77",
"frames": "27721",
"numberOfScenes": "160",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "3x color PointGrey Grasshopper3 video cameras 1920x1200 90Â°/80Â° 30Hz, 1x Velodyne HDL-64E LiDAR 64 beams 360Â° 10Hz, 1x GeneSys Eletronik GmbH Automotive Dynamic Motion Analyzer 100Hz",
"recordingPerspective": "Ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "3d bounding boxes",
"licensing": "-",
"relatedDatasets": "-",
"publishDate": "2019-03-01",
"lastUpdate": "-",
"paperTitle": "The H3D Dataset for Full-Surround 3D Multi-Object Detection and Tracking in Crowded Urban Scenes",
"relatedPaper": "https://arxiv.org/pdf/1903.01568.pdf",
"location": "San Francisco Bay Area, USA",
"rawData": "Yes",
"DOI": "10.1109/ICRA.2019.8793925"
},
{
"id": "4Seasons",
"href": "https://www.4seasons-dataset.com/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "30",
"samplingRate": "30",
"lengthOfScenes": "-",
"sensors": "camera, imu/rtk-gnss",
"sensorDetail": "2x cameras stereo baseline 30cm 800x400 (after cropping)",
"recordingPerspective": "ego-perspetive",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "globally consistent reference poses",
"annotations": "-",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",
"relatedDatasets": "-",
"publishDate": "2020-10-01",
"lastUpdate": "-",
"paperTitle": "4Seasons: A Cross-Season Dataset for Multi-Weather SLAM in Autonomous Driving",
"relatedPaper": "https://arxiv.org/pdf/2009.06364.pdf",
"location": "-",
"rawData": "Yes",
"DOI": "10.48550/arXiv.2009.06364"
},
{
"id": "RadarScenes",
"href": "https://radar-scenes.com/",
"size_storage": "-",
"size_hours": "4",
"frames": "-",
"numberOfScenes": "158",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, radar, odometry",
"sensorDetail": "4x 77 GHz series production automotive 60Â° radar sensor, 1x documentary camera",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "point-wise",
"licensing": "Creative Commons Attribution Non Commercial Share Alike 4.0 International",
"relatedDatasets": "-",
"publishDate":"2021-03-01",
"lastUpdate": "-",
"paperTitle": "RadarScenes: A Real-World Radar Point Cloud Data Set for Automotive Applications",
"relatedPaper": "https://arxiv.org/pdf/2104.02493.pdf",
"location": "-",
"rawData": "Yes",
"DOI": "10.23919/FUSION49465.2021.9627037"
},
{
"id": "India Driving",
"href": "https://idd.insaan.iiit.ac.in/",
"size_storage": "-",
"size_hours": "-",
"frames": "10004",
"numberOfScenes": "182",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1080p & 720p stereo image",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "Pixel-Level Semantic Segmentation Task, Instance-Level Semantic Segmentation Task",
"annotations": "semantic segmentation",
"licensing": "-",
"relatedDatasets": "-",
"publishDate": "2018-11-01",
"lastUpdate": "-",
"paperTitle": "IDD: A Dataset for Exploring Problems of Autonomous Navigation in Unconstrained Environments",
"relatedPaper": "https://arxiv.org/abs/1811.10200",
"location": "Bangalore and Hyderabad, India",
"rawData": "Yes",
"DOI": "10.1109/WACV.2019.00190"
},
{
"id": "Synscapes",
"href": "https://7dlabs.com/synscapes-overview",
"size_storage": "-",
"size_hours": "-",
"frames": "25000",
"numberOfScenes": "25000",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "RGB images in PNG format 1440x720 & upscaled version 2048x1024",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding boxes, 3d bounding boxes, occlusion, truncation, semantic segmentation,instance segmentation, depth segmentation, scene metadata",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "-",
"publishDate": "2018-10-01",
"lastUpdate": "-",
"paperTitle": "Synscapes: A Photorealistic Synthetic Dataset for Street Scene Parsing",
"relatedPaper": "https://arxiv.org/abs/1810.08705",
"location": "-",
"rawData": "-",
"DOI": "10.48550/arXiv.1810.08705"
},
{
"id": "RADIATE",
"href": "http://pro.hw.ac.uk/radiate/",
"size_storage": "-",
"size_hours": "5",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "1x ZED stereo camera 672x376 15Hz, 1x Velodyne HDL-32e LiDAR 32 channel 360Â° 10Hz, 1x Navtech CTS350-X radar 360Â°, 1x Advanced Navigation Spatial Dual GPS/IMU",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "",
"benchmark": "-",
"annotations": "2d bounding boxes",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International",
"relatedDatasets": "-",
"publishDate": "2020-10-01",
"lastUpdate": "-",
"paperTitle": "RADIATE: A Radar Dataset for Automotive Perception in Bad Weather",
"relatedPaper": "https://arxiv.org/pdf/2010.09076.pdf",
"location": "-",
"rawData": "Yes",
"DOI": "10.1109/ICRA48506.2021.9562089"
},
{
"id": "Bosch Small Traffic Lights Dataset",
"href": "https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset",
"size_storage": "-",
"size_hours": "-",
"frames": "13427",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "raw 12bit HDR images with a red-clear-clear-blue filter 1280x720 & reconstructed 8-bit RGB color images 1280x720",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "bounding boxes, state",
"licensing": "freely available for non-commercial purposes",
"relatedDatasets": "-",
"publishDate": "2017-05-01",
"lastUpdate": "-",
"paperTitle": "A deep learning approach to traffic lights: Detection, tracking, and classification",
"relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7989163",
"location": "-",
"rawData": "Yes",
"DOI": "10.1109/ICRA.2017.7989163"
},
{
"id": "PepScenes",
"href": "https://github.com/huawei-noah/PePScenes",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "Images taken from nuScenes and annotations added",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "-",
"benchmark": "-",
"annotations": "2d/3d bounding boxes and behavioral annotation",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2020-12-14",
"lastUpdate": "-",
"paperTitle": "PePScenes: A Novel Dataset and Baseline for Pedestrian Action Prediction in 3D",
"relatedPaper": "https://arxiv.org/pdf/2012.07773.pdf",
"location": "Boston, USA and Singapore",
"rawData": "Yes",
"DOI": "-"
},
{
"id": "WZ-traffic",
"href": "https://github.com/Fangyu0505/traffic-scene-recognition",
"size_storage": "-",
"size_hours": "-",
"frames": "6035",
"numberOfScenes": "6035",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "Photos privately captured and part of it sourced online and labelled",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "location category",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2019-06-19",
"lastUpdate": "-",
"paperTitle": "Deep multiple classifier fusion for traffic scene recognition",
"relatedPaper": "https://link.springer.com/article/10.1007/s41066-019-00182-6",
"location": "-",
"rawData": "-",
"DOI": "10.1007/s41066-019-00182-6"
},
{
"id": "Nighttime Driving",
"href": "http://people.ee.ethz.ch/~daid/NightDriving/",
"size_storage": "-",
"size_hours": "-",
"frames": "35000",
"numberOfScenes": "5",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "GoPro Hero 5 camera",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "pixellevel semantic annotations",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2018-11-04",
"lastUpdate": "-",
"paperTitle": "Dark Model Adaptation: Semantic Image Segmentation from Daytime to Nighttime",
"relatedPaper": "https://arxiv.org/abs/1810.02575",
"location": "Switzerland",
"rawData": "-",
"DOI": "10.1109/ITSC.2018.8569387"
},
{
"id": "Cooperative Driving (CODD)",
"href": "https://github.com/eduardohenriquearnold/CODD",
"size_hours": "-",
"size_storage": "16.7",
"frames": "13500",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "lidar",
"sensorDetail": "-",
"recordingPerspective": "Top Shot",
"dataType":"Real",
"benchmark": "-",
"annotations": "3d bounding boxes",
"licensing": "Creative Commons Attribution Share Alike 4.0 International",
"relatedDatasets": "-",
"publishDate": "2021-11-23",
"lastUpdate": "-",
"paperTitle": "Fast and Robust Registration of Partially Overlapping Point Clouds",
"relatedPaper": "https://arxiv.org/pdf/2112.09922.pdf",
"location": "CARLA environment",
"rawData": "-",
"DOI": "10.1109/LRA.2021.3137888"
},
{
"id": "AIODrive",
"href": "http://www.aiodrive.org/overview.html",
"size_hours": "-",
"size_storage": "3041.71",
"frames": "100000",
"numberOfScenes": "100",
"samplingRate": "-",
"lengthOfScenes": "100",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "5x color cameras 1920x720 10Hz, 5x depth cameras 1920x720 10Hz, 3x lidar 64/800/1200 channels 360Â° 10Hz, 1x SPAD-LiDAR, 4x radar 360Â° 10Hz, 1x gps/imu 10Hz",
"recordingPerspective": "Birdâs Eye",
"dataType":"Real",
"benchmark": "3d object detection, trajectory forecasting",
"annotations": "2d/3d bounding boxes, object category and attributes, 2d-3d semantic, instance and panoptic segmentation",
"licensing": "freely available for both commercial and non-commercial purposes",
"relatedDatasets": "-",
"publishDate": "2021-04-06",
"lastUpdate": "-",
"paperTitle": "All-In-One Drive: A Comprehensive Perception Dataset with High-Density Long-Range Point Clouds",
"relatedPaper": "https://www.xinshuoweng.com/papers/AIODrive/arXiv.pdf",
"location": "CARLA environment",
"rawData": "-",
"DOI": "10.13140/RG.2.2.21621.81122"
},
{
"id": "ROAD",
"href": "https://github.com/gurkirt/road-dataset",
"size_hours": "2.83",
"size_storage": "-",
"frames": "122000",
"numberOfScenes": "22",
"samplingRate": "12",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"benchmark": "agent detection, action detection and road event detection",
"annotations": "2d/3d bounding boxes, action label and location labels",
"licensing": "Creative Commons Attribution Share Alike 4.0 International",
"relatedDatasets": "Oxford Robot Car Dataset (OxRD)",
"publishDate": "-",
"lastUpdate": "-",
"paperTitle": "ROAD: The ROad event Awareness Dataset for Autonomous Driving",
"relatedPaper": "https://arxiv.org/abs/2102.11585",
"location": "Oxford, UK",
"rawData": "-",
"DOI": "10.1109/TPAMI.2022.3150906"
},
{
"id": "ONCE",
"href": "https://once-for-auto-driving.github.io/index.html",
"size_storage": "-",
"size_hours": "144",
"frames": "1000000",
"numberOfScenes": "581",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "7x color cameras 1920x1020 10Hz, 1x 40-beam lidar 360Â° 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"benchmark": "3d object detection",
"annotations": "2d/3d bounding boxes",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (CC BY-NC-SA 4.0)",
"relatedDatasets": "ONCE-3DLanes",
"publishDate":"2021-05-18",
"lastUpdate": "2021-08-05",
"paperTitle": "One Million Scenes for Autonomous Driving: ONCE Dataset",
"relatedPaper": "https://arxiv.org/pdf/2106.11037.pdf",
"location": "China",
"rawData": "-",
"DOI": "10.48550/arXiv.2106.11037"
},
{
"id": "DriveU Traffic Light",
"href": "https://www.uni-ulm.de/en/in/driveu/projects/driveu-traffic-light-dataset/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "15",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x stereo camera 60Â°, 1x stereo camera 130Â°",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData":"Yes",
"benchmark": "-",
"annotations": "2d bounding boxes",
"licensing": "freely available for non-commercial research and teaching activities",
"relatedDatasets": "-",
"publishDate":"2018-11-27",
"lastUpdate": "2021-04",
"paperTitle": "The DriveU Traffic Light Dataset: Introduction and Comparison with Existing Datasets",
"relatedPaper": "https://ieeexplore.ieee.org/document/8460737",
"location": "10 cities across Germany",
"rawData": "-",
"DOI": "10.1109/ICRA.2018.8460737"
},
{
"id": "Bosch TL",
"href": "https://github.com/asimonov/Bosch-TL-Dataset",
"size_storage": "-",
"size_hours": "-",
"frames": "13427",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "camera 1280x720",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData":"No",
"benchmark": "-",
"annotations": "2d bounding boxes",
"licensing": "freely available for non-commercial use",
"relatedDatasets": "-",
"publishDate":"2017-07-24",
"lastUpdate": "-",
"paperTitle": "A deep learning approach to traffic lights: Detection, tracking, and classification",
"relatedPaper": "https://ieeexplore.ieee.org/document/7989163",
"location": "El Camino Real in the San Francisco Bay Area, California",
"rawData": "-",
"DOI": "10.1109/ICRA.2017.7989163"
},
{
"id": "nuPlan",
"href": "https://arxiv.org/abs/2106.11810",
"size_storage": "-",
"size_hours": "1500",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "8x cameras 2000x1200 10Hz, 5x lidar 20Hz, 1x imu 100Hz, 1x gnss 20Hz",
"recordingPerspective": "-",
"dataType":"Real",
"mapData":"No",
"benchmark": "autonomous vehicle planning",
"annotations": "2d/3d bounding boxes",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (CC BY-NC-SA 4.0)",
"relatedDatasets": "-",
"publishDate":"2021-12-10",
"lastUpdate": "-",
"paperTitle": "nuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles",
"relatedPaper": "https://arxiv.org/abs/2106.11810",
"location": "Boston, Pittsburgh, Las Vegas and Singapore",
"rawData": "-",
"DOI": "10.48550/arXiv.2106.11810"
},
{
"id": "JAAD",
"href": "https://paperswithcode.com/dataset/jaad",
"size_storage": "3.1",
"size_hours": "-",
"frames": "82032",
"numberOfScenes": "346",
"samplingRate": "30",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x camera 1920x1080 110Â°, 1x camera 1920x1080 170Â°, 1x camera 1280x720 100Â°",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData":"No",
"benchmark": "-",
"annotations": "2d bounding boxes, behavioral tags, contextual tags",
"licensing": "MIT License",
"relatedDatasets": "PIE Dataset",
"publishDate":"2016-09-15",
"lastUpdate": "-",
"paperTitle": "https://ieeexplore.ieee.org/document/8265243",
"relatedPaper": "https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w3/Rasouli_Are_They_Going_ICCV_2017_paper.pdf",
"location": "North America and Europe",
"rawData": "-",
"DOI": "10.1109/ICCVW.2017.33"
},
{
"id": "RoadObstacle21",
"href": "https://segmentmeifyoucan.com/datasets",
"size_storage": "0.2",
"size_hours": "-",
"frames": "327",
"numberOfScenes": "327",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "images from web resources 1920x1080",
"recordingPerspective": "ego-perspective",
"dataType":"Real and Synthetic",
"mapData": "No",
"benchmark": "anomaly detection",
"annotations": "semantic segmentation",
"licensing": "various, see \"https://github.com/SegmentMeIfYouCan/road-anomaly-\"benchmark\"/blob/master/doc/RoadAnomaly/credits.txt\" for detail",
"relatedDatasets": "RoadAnomaly21",
"publishDate":"2021-04-01",
"lastUpdate": "-",
"paperTitle": "SegmentMeIfYouCan: A Benchmark for Anomaly Segmentation",
"relatedPaper": "https://arxiv.org/pdf/2104.14812.pdf",
"location": "-",
"rawData": "Yes",
"DOI": "10.48550/arXiv.2104.14812"
},
{
"id": "BoxCars116k",
"href": "https://github.com/JakubSochor/BoxCars",
"size_storage": "-",
"size_hours": "-",
"frames": "116000",
"numberOfScenes": "116000",
"samplingRate": "12.5",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "images obtained from 137 different surveillance cameras",
"recordingPerspective": "Bird's Eye",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d/3d bounding boxes",
"licensing": "Freely available for research purposes",
"relatedDatasets": "BoxCars21k",
"publishDate":"2018-03-07",
"lastUpdate": "-",
"paperTitle": "BoxCars: Improving Fine-Grained Recognition of Vehicles Using 3-D Bounding Boxes in Traffic Surveillance",
"relatedPaper": "https://arxiv.org/abs/1703.00686",
"location": "Brno, Czech Republic",
"rawData": "-",
"DOI": "10.1109/TITS.2018.2799228"
},
{
"id": "Beyond PASCAL",
"href": "https://yuxng.github.io/Xiang_WACV_03242014.pdf",
"size_storage": "8.5",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"recordingPerspective": "-",
"dataType":"Real",
"mapData":"No",
"sensorDetail": "-",
"benchmark": "anomaly detection, 3D object detection and pose estimation",
"annotations": "label landmarks of the CAD model on the 2D image",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2014-03-26",
"lastUpdate": "-",
"paperTitle": "Beyond PASCAL: A Benchmark for 3D Object Detection in the Wild",
"relatedPaper": "https://ieeexplore.ieee.org/document/6836101",
"location": "-",
"rawData": "-",
"DOI": "10.1109/WACV.2014.6836101"
},
{
"id": "EuroCity Persons",
"href": "https://arxiv.org/abs/1805.07193",
"size_storage": "-",
"size_hours": "-",
"frames": "47300",
"numberOfScenes": "-",
"samplingRate": "20",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x camera 1920x1080 20Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData":"Yes",
"benchmark": "object detection",
"annotations": "2d bounding boxes",
"licensing": "freely available for research use by eligiible persons",
"relatedDatasets": "ECP2.5D - Person Localization in Traffic Scenes",
"publishDate":"2019-03-31",
"lastUpdate": "2020-11-11",
"paperTitle": "The EuroCity Persons Dataset: A Novel Benchmark for Object Detection",
"relatedPaper": "https://arxiv.org/pdf/1805.07193.pdf",
"location": "12 countries and 31 cities across Europe",
"rawData": "-",
"DOI": "10.1109/TPAMI.2019.2897684"
},
{
"id": "CADC",
"href": "http://cadcd.uwaterloo.ca/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "75",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "cameras, lidar, gps",
"sensorDetail": "8x camera Ximea MQ013CG-E2 1280x1024 10Hz,  1x lidar Veldyne VLP-32C 360Â° 10Hz, 1x NovAtel OEM638 Triple-Frequency GPS, 1x Sensonor STIM300 MEMS 100Hz IMU, 2x Xsens 200Hz IMU",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData":"Yes",
"benchmark": "-",
"annotations": "2d/3d bounding boxes",
"licensing": "Creative Commons Attribution-NonCommercial 4.0 International Public License (CC BY-NC 4.0)",
"relatedDatasets": "-",
"publishDate":"2020-01-27",
"lastUpdate": "-",
"paperTitle": "Canadian Adverse Driving Conditions Dataset",
"relatedPaper": "https://arxiv.org/pdf/2001.10117.pdf",
"location": "Waterloo region in Ontorio, Canada",
"rawData": "-",
"DOI": "10.1177/0278364920979368"
},
{
"id": "CARRADA",
"href": "https://github.com/valeoai/carrada_dataset",
"size_storage": "288",
"size_hours": "-",
"frames": "12666",
"numberOfScenes": "30",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, radar",
"sensorDetail": "1x camera 1238x1024,  1x radar 180Â°",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData":"No",
"benchmark": "-",
"annotations": "range-angle Doppler annotations",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (CC BY-NC-SA 4.0)",
"relatedDatasets": "-",
"publishDate":"2020-05-04",
"lastUpdate": "2021-07",
"paperTitle": "CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations",
"relatedPaper": "https://arxiv.org/pdf/2005.01456.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/ICPR48806.2021.9413181"
},
{
"id": "Astyx",
"href": "https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/astyx_dataset.html",
"size_storage": "-",
"size_hours": "-",
"frames": "500",
"numberOfScenes": "500",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, radar",
"sensorDetail": "1x Point Grey Blackfly camera 2048x618 30Hz, 1x Velodyne VLP-16 10Hz, 1x Astyx 6455 HiRes 110Â° 13Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "3d bounding boxes",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2019-10-02",
"lastUpdate": "-",
"paperTitle": "Automotive Radar Dataset for Deep Learning Based 3D Object Detection",
"relatedPaper": "https://ieeexplore.ieee.org/document/8904734",
"location": "-",
"rawData": "-",
"DOI": "-"
},
 {
   "id": "TJ4DRadSet",
   "href": "https://github.com/TJRadarLab/TJ4DRadSet",
   "relatedPaper":"https://arxiv.org/abs/2204.13483",
   "paperTitle":"TJ4DRadSet: A 4D Radar Dataset for Autonomous Driving"
 },
{
"id": "PointCloudDeNoising",
"href": "https://github.com/rheinzler/PointCloudDeNoising",
"size_storage": "-",
"size_hours": "-",
"frames": "175941",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "lidar",
"sensorDetail": "1 Velodyne VLP32c lidar sensor",
"recordingPerspective": "Ego-perspective",
"dataType":"Real",
"mapData":"No",
"benchmark": "-",
"annotations": "pointwise annotations",
"licensing": "Freely available for research and teaching purposes",
"relatedDatasets": "Gated2Depth, Gated2Gated, SeeingThroughFog",
"publishDate":"2019-12-09",
"lastUpdate": "-",
"paperTitle": "CNN-based Lidar Point Cloud De-Noising in Adverse Weather",
"relatedPaper": "https://arxiv.org/abs/1912.03874",
"location": "CEREMA's climatic chamber",
"rawData": "-",
"DOI": "10.1109/LRA.2020.2972865"
},
{
"id": "Talk2Car",
"href": "https://talk2car.github.io/",
"size_storage": "300",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "850",
"samplingRate": "-",
"lengthOfScenes": "20",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "1x lidar 32 channels 360Â° 20Hz, 5x long range radar 13Hz, 6x camera 1600x1200 12Hz, 1x gps/imu 1000Hz",
"recordingPerspective": "Ego-perspective",
"dataType":"Real",
"mapData":"No",
"benchmark": "find bounding box for objects based on user commands in natural language",
"annotations": "bounding box and command in natural language related to the bounding box",
"licensing": "MIT license",
"relatedDatasets": "-",
"publishDate":"2020-03-18",
"lastUpdate": "2021-10-06",
"paperTitle": "Talk2Car: Taking Control of Your Self-Driving Car",
"relatedPaper": "https://arxiv.org/pdf/1909.10838.pdf",
"location": "Boston and Singapore",
"rawData": "-",
"DOI": "10.18653/v1/D19-1215"
},
{
"id": "A Parametric Top-View Representation of Complex Road Scenes",
"href": "https://www.nec-labs.com/~mas/BEV/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "Semantic and BEV annotations added to images taken from KITTI and NuScenes",
"recordingPerspective": "ego-perspective",
"dataType":"Real and Synthetic",
"mapData": "-",
"benchmark": "-",
"annotations": "Semantic and BEV annotations",
"licensing": "Free to use",
"relatedDatasets": "-",
"publishDate":"2019-06-15",
"lastUpdate": "-",
"paperTitle": "A Parametric Top-View Representation of Complex Road Scenes",
"relatedPaper": "https://arxiv.org/pdf/1812.06152.pdf",
"location": "Karlsruhe, Germany (KITTI); Boston, USA and Singapore (nuScenes)",
"rawData": "-",
"DOI": "10.1109/CVPR.2019.01057"
},
{
"id": "DRIV100",
"href": "https://zenodo.org/record/4389243#.YnvlruhBxD8",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "100",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData":"No",
"benchmark": "Domain adaptation techniques on in-the-wild road-scene videos collected from the Internet",
"annotations": "pixel level semantic segmentation",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License",
"relatedDatasets": "-",
"publishDate":"2021-01-30",
"lastUpdate": "-",
"paperTitle": "DRIV100: In-The-Wild Multi-Domain Dataset and Evaluation for Real-World Domain Adaptation of Semantic Segmentation",
"relatedPaper": "https://arxiv.org/pdf/2102.00150.pdf",
"location": "Random videos from YouTube",
"rawData": "-",
"DOI": "10.5281/zenodo.4389243"
},
{
"id": "Cars",
"href": "https://ai.stanford.edu/~jkrause/cars/car_dataset.html",
"size_storage": "-",
"size_hours": "-",
"frames": "16185",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "camera",
"recordingPerspective": "-",
"dataType":"Real",
"mapData":"No",
"benchmark": "-",
"annotations": "car make, model, year",
"licensing": "freely available for non-commercial research and educational use",
"relatedDatasets": "BMW-10",
"publishDate":"2013",
"lastUpdate": "-",
"paperTitle": "3D Object Representations for Fine-Grained Categorization",
"relatedPaper": "https://ieeexplore.ieee.org/document/6755945",
"location": "Sourced from internet",
"rawData": "-",
"DOI": "10.1109/ICCVW.2013.77"
},
{
"id": "CADP",
"href": "https://ankitshah009.github.io/accident_forecasting_traffic_camera",
"size_storage": "-",
"size_hours": "5.2",
"frames": "-",
"numberOfScenes": "1416",
"samplingRate": "20",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "Bird's Eye",
"dataType":"Real",
"mapData":"No",
"benchmark": "-",
"annotations": "Object detectiona and accident forecasting",
"licensing": "freely available for non-commercial use",
"relatedDatasets": "-",
"publishDate":"2018-10-04",
"lastUpdate": "-",
"paperTitle": "CADP: A Novel Dataset for CCTV Traffic Camera based Accident Analysis",
"relatedPaper": "https://arxiv.org/abs/1809.05782",
"location": "Videos sampled from YouTube",
"rawData": "-",
"DOI": "10.1109/AVSS.2018.8639160"
},
{
"id": "NoCrash",
"href": "https://github.com/felipecode/coiltraine/blob/master/docs/exploring_limitations.md",
"size_storage": "-",
"size_hours": "100",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Synthetic",
"mapData": "No",
"benchmark": "autonomous driving and crashes",
"annotations": "-",
"licensing": "Free to use",
"relatedDatasets": "NoCrash",
"publishDate":"2019-10-27",
"lastUpdate": "-",
"paperTitle": "Exploring the Limitations of Behavior Cloning for Autonomous Driving",
"relatedPaper": "https://arxiv.org/pdf/1904.08980.pdf",
"location": "CARLA environment",
"rawData": "-",
"DOI": "10.1109/ICCV.2019.00942"
},
{
"id": "VITRO",
"href": "https://vitro-testing.com/test-data/dashcam-annotations/"
},
{
"id": "UDrive",
"href": "https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjqnfKIoK3uAhUOuaQKHZwcDEgQFjASegQIFBAC&url=https%3A%2F%2Ferticonetwork.com%2Fwp-content%2Fuploads%2F2017%2F12%2FUDRIVE-D41.1-UDrive-dataset-and-key-analysis-results-with-annotation-codebook.pdf&usg=AOvVaw17NgwnPrIal53hUYco9klG"
},
{
"id": "D^2 City",
"href": "https://outreach.didichuxing.com/d2city",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "11211",
"samplingRate": "25",
"lengthOfScenes": "30",
"sensors": "camera",
"sensorDetail": "",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData":"No",
"benchmark": "-",
"annotations": "2d bounding boxes and inter-frame tracking labels",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2019-04-03",
"lastUpdate": "-",
"paperTitle": "D^2-City: A Large-Scale Dashcam Video Dataset of Diverse Traffic Scenarios",
"relatedPaper": "https://arxiv.org/abs/1904.01975",
"location": "5 Chinese cities",
"rawData": "-",
"DOI": "10.48550/arXiv.1904.01975"
},
{
"id": "MIT-AVT Clustered Driving Scene",
"href": "https://ieeexplore.ieee.org/abstract/document/9304677/",
"size_storage": "4000",
"size_hours": "3212",
"frames": "-",
"numberOfScenes": "1156592",
"samplingRate": "30",
"lengthOfScenes": "10",
"sensors": "camera, imu, gps",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "Seasons, Weather, Lanes, Illuminnation, Simplified Road Type, Others",
"licensing": "Not released publically",
"relatedDatasets": "-",
"publishDate":"2020-11-13",
"lastUpdate": "-",
"paperTitle": "MIT-AVT Clustered Driving Scene Dataset: Evaluating Perception Systems in Real-World Naturalistic Driving Scenarios",
"relatedPaper": "https://ieeexplore.ieee.org/abstract/document/9304677/",
"location": "Many states across the USA",
"rawData": "Yes",
"DOI": "10.1109/IV47402.2020.9304677"
},
{
  "id":"SceNDD",
  "relatedPaper":"https://arxiv.org/abs/2212.12436",
  "paperTitle":"SceNDD: A Scenario-based Naturalistic Driving Dataset"
},
{
"id": "DDAD",
"href": "https://github.com/AdrienGaidon-TRI/DDAD",
"size_storage": "254",
"size_hours": "-",
"frames": "21200",
"numberOfScenes": "435",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "6x cameras 2.4MP 1936x1216 10Hz, 1x Luminar-H2 Lidar sensor 360Â° 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "-",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License",
"relatedDatasets": "-",
"publishDate":"2020-06-19",
"lastUpdate": "-",
"paperTitle": "3D Packing for Self-Supervised Monocular Depth Estimation",
"relatedPaper": "https://arxiv.org/pdf/1905.02693.pdf",
"location": "USA (Ann Arbor, San Francisco Bay Area, Detroit, Cambridge and Massachusetts), Japan (Tokyo and Odaiba)",
"rawData": "-",
"DOI": "10.1109/CVPR42600.2020.00256"
},
{
"id": "RELLIS-3D",
"href": "https://unmannedlab.github.io/research/RELLIS-3D",
"size_storage": "58.1",
"size_hours": "-",
"frames": "13556",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "1x Nerian Karmin2 + Nerian SceneScan: 3D StereoCamera 10Hz, 1x RGB Camera: Basler acA1920-50gc camera with 16mm/F18 EDMUND Optics lens 1920x1200 10Hz, 1x Ouster OS1 LiDAR 64 Channels 10 Hz, 1x Velodyne Ultra Puck: 32 Channels 10Hz, Vectornav VN-300 Dual Antenna GNSS/INS 300Hz GPS, 100Hz IMU",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic segmentation of 2d image and 3d point clouds",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License",
"relatedDatasets": "SemanticUSL: A Dataset for LiDAR Semantic Segmentation Domain Adaptation",
"publishDate":"2020-11-26",
"lastUpdate": "2022-01-24",
"paperTitle": "RELLIS-3D Dataset: Data, Benchmarks and Analysis",
"relatedPaper": "https://arxiv.org/pdf/2011.12954.pdf",
"location": "Rellis Campus of Texas A&M University",
"rawData": "-",
"DOI": "10.1109/ICRA48506.2021.9561251"
},
{
"id": "PolySync",
"href": "http://selfracingcars.com/blog/2016/7/26/polysync"
},
{
"id": "DriveSeg (MANUAL)",
"href": "https://agelab.mit.edu/driveseg",
"size_storage": "2.98",
"size_hours": "0.03",
"frames": "5000",
"numberOfScenes": "1",
"samplingRate": "30",
"lengthOfScenes": "167",
"sensors": "camera",
"sensorDetail": "1x FDR-AX53 camera 1080P 1920x1080 30Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic segmentation",
"licensing": "freely available to academic and nonacademic entities for non-commercial purposes",
"relatedDatasets": "DriveSeg (Semi-auto)",
"publishDate":"2020-04-06",
"lastUpdate": "-",
"paperTitle": "Value of Temporal Dynamics Information in Driving Scene Segmentation",
"relatedPaper": "https://arxiv.org/abs/1904.00758",
"location": "-",
"rawData": "-",
"DOI": "10.21227/mmke-dv03"
},
{
"id": "DriveSeg (Semi-auto)",
"href": "https://agelab.mit.edu/driveseg",
"size_storage": "13.46",
"size_hours": "0.186",
"frames": "20100",
"numberOfScenes": "67",
"samplingRate": "30",
"lengthOfScenes": "10",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "pixel-wise semantic annotation",
"licensing": "-",
"relatedDatasets": "DriveSeg (MANUAL), MIT-AVT Clustered Driving Scene Dataset",
"publishDate":"2020-04-06",
"lastUpdate": "-",
"paperTitle": "Value of Temporal Dynamics Information in Driving Scene Segmentation",
"relatedPaper": "https://arxiv.org/abs/1904.00758",
"location": "-",
"rawData": "-",
"DOI": "10.21227/nb3n-kk46"
},
{
"id": "KUL Belgium Traffic Sign",
"href": "https://people.ee.ethz.ch/~timofter/traffic_signs/",
"size_storage": "50",
"size_hours": "-",
"frames": " 145000",
"numberOfScenes": " 145000",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "8x cameras 1628x1236",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding box with labels",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2009-12-07",
"lastUpdate": "-",
"paperTitle": "Multi-view traffic sign detection, recognition, and 3D localisation",
"relatedPaper": "https://ieeexplore.ieee.org/document/5403121",
"location": "Belgium",
"rawData": "Yes",
"DOI": "10.1109/WACV.2009.5403121"
},
{
 "id": "GLARE",
  "href":"https://github.com/NicholasCG/GLARE_Dataset",
  "relatedPaper":"https://arxiv.org/pdf/2209.08716.pdf",
  "paperTitle":"GLARE: A Dataset for Traffic Sign Detection in Sun Glare",
  "rawData": "Yes"
},
{
"id": "Brain4Cars",
"href": "http://brain4cars.com/",
"size_storage": "16",
"size_hours": "-",
"frames": "2000000",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, speed logger, gps",
"sensorDetail": "-",
"recordingPerspective": "Face camera, ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "driving events (lane changes, turns, driving straight)",
"licensing": "free for educational and non-commercial purposes",
"relatedDatasets": "-",
"publishDate":"2015-12-13",
"lastUpdate": "-",
"paperTitle": "Car that Knows Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models",
"relatedPaper": "https://arxiv.org/abs/1504.02789",
"location": "USA",
"rawData": "-",
"DOI": "10.1109/ICCV.2015.364"
},
{
"id": "Seasonal Variation",
"href": "http://www.cs.cmu.edu/~aayushb/localization/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, imu/gps",
"sensorDetail": "1x Point Grey Ladybug 5 panoramic camera 2448 x 2048 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "Publicly available for research community",
"relatedDatasets": "Bay Area Dataset, Illumination Changes in a day",
"publishDate":"2014-06-11",
"lastUpdate": "-",
"paperTitle": "Understanding How Camera Configuration and Environmental Conditions Affect Appearance-based Localization",
"relatedPaper": "https://ieeexplore.ieee.org/document/6856605",
"location": "California bay area and Pittsburgh, USA",
"rawData": "Yes",
"DOI": "10.1109/IVS.2014.6856605"
},
{
"id": "Bay Area",
"href": "http://www.cs.cmu.edu/~aayushb/localization/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, imu/gps",
"sensorDetail": "1x Point Grey Ladybug 5 panoramic camera 2448 x 2048 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "Available on request",
"relatedDatasets": "Seasonal Variation Dataset, Illumination Changes in a day",
"publishDate":"2014-06-11",
"lastUpdate": "-",
"paperTitle": "Understanding How Camera Configuration and Environmental Conditions Affect Appearance-based Localization",
"relatedPaper": "https://ieeexplore.ieee.org/document/6856605",
"location": "California bay area and Pittsburgh, USA",
"rawData": "Yes",
"DOI": "10.1109/IVS.2014.6856605"
},
{
"id": "Illumination Changes in a day",
"href": "http://www.cs.cmu.edu/~aayushb/localization/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, imu/gps",
"sensorDetail": "1x Point Grey Ladybug 5 panoramic camera 2448 x 2048 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "Available on request",
"relatedDatasets": "Bay Area Dataset, Seasonal Variation Dataset",
"publishDate":"2014-06-11",
"lastUpdate": "-",
"paperTitle": "Understanding How Camera Configuration and Environmental Conditions Affect Appearance-based Localization",
"relatedPaper": "https://ieeexplore.ieee.org/document/6856605",
"location": "California bay area and Pittsburgh, USA",
"rawData": "Yes",
"DOI": "10.1109/IVS.2014.6856605"
},
{
"id": "comma10k",
"href": "https://github.com/commaai/comma10k",
"size_storage": "",
"size_hours": "",
"frames": "10000",
"numberOfScenes": "10000",
"samplingRate": "",
"lengthOfScenes": "",
"sensors": "",
"sensorDetail": "",
"recordingPerspective": "",
"dataType":"",
"mapData": "",
"benchmark": "",
"annotations": "",
"licensing": "MIT License",
"relatedDatasets": "",
"publishDate":"",
"lastUpdate": "",
"paperTitle": "",
"relatedPaper": "",
"location": "",
"rawData": "",
"DOI": ""
},
{
"id": "comma.ai",
"href": "http://research.comma.ai/",
"size_storage": "80",
"size_hours": "7.25",
"frames": "-",
"numberOfScenes": "10",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "-",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License",
"relatedDatasets": "-",
"publishDate":"2016-08-03",
"lastUpdate": "-",
"paperTitle": "Learning a Driving Simulator",
"relatedPaper": "https://arxiv.org/pdf/1608.01230",
"location": "-",
"rawData": "-",
"DOI": "10.48550/arXiv.1608.01230"
},
{
"id": "CULane",
"href": "https://xingangpan.github.io/projects/CULane.html",
"size_storage": "42.5",
"size_hours": "-",
"frames": "133235",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "traffic lanes with cubic splines",
"licensing": "freely available for non-commercial purpose",
"relatedDatasets": "-",
"publishDate":"2018-04-27",
"lastUpdate": "-",
"paperTitle": "Spatial as Deep: Spatial CNN for Traffic Scene Understanding",
"relatedPaper": "https://arxiv.org/abs/1712.06080",
"location": "Beijing, China",
"rawData": "-",
"DOI": "10.1609/aaai.v32i1.12301"
},
{
"id": "DDD20: DAVIS Driving 2020",
"href": "https://docs.google.com/document/d/1HM0CSmjO8nOpUeTvmPjopcBcVCk7KXvLUuiZFS6TWSg/pub",
"size_storage": "1300",
"size_hours": "51",
"frames": "-",
"numberOfScenes": "216",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x 346x260-pixel DAVIS346 camera 56Â°",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "Creative Commons Attribution-ShareAlike 4.0 International License",
"relatedDatasets": "DDD17",
"publishDate":"2017-06-05",
"lastUpdate": "2020-02-01",
"paperTitle": "DDD20 End-to-End Event Camera Driving Dataset: Fusing Frames and Events with Deep Learning for Improved Steering Prediction",
"relatedPaper": "https://arxiv.org/pdf/2005.08605.pdf",
"location": "Various states of USA, Switzerland and Germany",
"rawData": "Yes",
"DOI": "10.1109/ITSC45102.2020.9294515"
},
{
"id": "DBNet",
"href": "http://www.dbehavior.net/",
"size_storage": "1610",
"size_hours": "10",
"frames": "56800",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "lidar, camera, steering angle meter, gps",
"sensorDetail": "1x Velodyne HDL-32E laser scanner 360Â° 10Hz, 1x Velodyne VLP-16 laser scanner, 1x color dashboard camera 1920x1080 30Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "Driving policy prediction",
"annotations": "-",
"licensing": "Freely available for non-commercial use upon registration",
"relatedDatasets": "-",
"publishDate":"2018-06-23",
"lastUpdate": "-",
"paperTitle": "LiDAR-Video Driving Dataset: Learning Driving Policies Effectively",
"relatedPaper": "https://ieeexplore.ieee.org/document/8578713",
"location": "-",
"rawData": "Yes",
"DOI": "10.1109/CVPR.2018.00615"
},
{
"id": "DIPLECS Autonomous Driving",
"href": "https://cvssp.org/data/diplecs/",
"size_storage": "-",
"size_hours": "4",
"frames": "207364",
"numberOfScenes": "4",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, steering angle meter, eye tracker",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "Freely available for academic purposes",
"relatedDatasets": "-",
"publishDate":"2015-10-07",
"lastUpdate": "-",
"paperTitle": "How Much of Driving Is Preattentive?",
"relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7293673",
"location": "Sweden and United Kingdom",
"rawData": "-",
"DOI": "10.1109/TVT.2015.2487826"
},
{
"id": "DR(eye)VE",
"href": "https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=8",
"size_storage": "-",
"size_hours": "6.16",
"frames": "555000",
"numberOfScenes": "74",
"samplingRate": "-",
"lengthOfScenes": "300",
"sensors": "camera, eye tracker",
"sensorDetail": "1x roof mounted GARMIN VirbX camera 1080p 25Hz, 1x SMI ETG 2w Eye Tracking Glasses (ETG) 60Hz, 1x frontal camera 720p 30Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "Gaze Maps, GPS, Speed and Course",
"licensing": "Freely available for non-commercial use upon registration",
"relatedDatasets": "-",
"publishDate":"2018-06-08",
"lastUpdate": "-",
"paperTitle": "Predicting the Driver's Focus of Attention: The DR(eye)VE Project",
"relatedPaper": "https://arxiv.org/pdf/1705.03854.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/TPAMI.2018.2845370"
},
{   
"id": "SemanticPOSS",
"href": "http://www.poss.pku.edu.cn/semanticposs.html",
"size_storage": "2.2",
"size_hours": "-",
"frames": "2988",
"numberOfScenes": "2988",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "1x forward-facing color camera, 4x wide-angle mono cameras 360Â°, 1x Pandora LiDAR 360Â°, 1x XW-GI7660 GPS/IMU",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "2d/3d bounding boxes",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License",
"relatedDatasets": "-",
"publishDate":"2020-10-19",
"lastUpdate": "-",
"paperTitle": "SemanticPOSS: A Point Cloud Dataset with Large Quantity of Dynamic Instances",
"relatedPaper": "https://arxiv.org/abs/2002.09147",
"location": "Peking University, China",
"rawData": "Yes",
"DOI": "10.1109/IV47402.2020.9304596"
},
{
"id": "SemanticUSL",
"href": "https://unmannedlab.github.io/semanticusl",
"size_storage": "27",
"size_hours": "-",
"frames": "17778",
"numberOfScenes": "17778",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "lidar",
"sensorDetail": "1x  Clearpath Warthog robotics with an Ouster OS1-64 LiDAR",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "semantic segmentation",
"licensing": "Free to use",
"relatedDatasets": "RELLIS-3D",
"publishDate":"2021-05-30",
"lastUpdate": "-",
"paperTitle": "LiDARNet: A Boundary-Aware Domain Adaptation Model for Point Cloud Semantic Segmentation",
"relatedPaper": "https://arxiv.org/pdf/2003.01174.pdf",
"location": " Texas A&M University, USA",
"rawData": "-",
"DOI": "10.1109/ICRA48506.2021.9561255"
},
{
"id": "ELEKTRA",
"href": "http://adas.cvc.uab.es/elektra/datasets/"
},
{
"id": "GTSRB",
"href": "https://benchmark.ini.rub.de/gtsrb_news.html",
"size_storage": "-",
"size_hours": "-",
"frames": "50000",
"numberOfScenes": "-",
"samplingRate": "25",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x Prosilica GC 1380CH 1360x1024 25Hz",
"recordingPerspective": "ego-perspetive",
"dataType":"Real",
"mapData": "No",
"benchmark": "Traffic Sign Recognition",
"annotations": "2d bounding box",
"licensing": "Free to use",
"relatedDatasets": "GTSDB",
"publishDate":"2010-12-01",
"lastUpdate": "2011-01-19",
"paperTitle": "The German Traffic Sign Recognition Benchmark: A multi-class classification competition",
"relatedPaper": "https://www.ini.rub.de/upload/file/1470692848_f03494010c16c36bab9e/StallkampEtAl_GTSRB_IJCNN2011.pdf",
"location": "Germany",
"rawData": "-",
"DOI": "10.1109/IJCNN.2011.6033395"
},
{
"id": "GTSDB",
"href": "https://benchmark.ini.rub.de/gtsdb_news.html",
"size_storage": "-",
"size_hours": "-",
"frames": "900",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x Prosilica GC 1380CH camera 1360x1024",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "Traffic Sign Detection",
"annotations": "2d bounding box",
"licensing": "-",
"relatedDatasets": "GTSRB",
"publishDate":"2012-12-01",
"lastUpdate": "2013-08-09",
"paperTitle": "Detection of traffic signs in real-world images: The German traffic sign detection benchmark",
"relatedPaper": "https://ieeexplore.ieee.org/document/6706807",
"location": "Bochum, Germany",
"rawData": "-",
"DOI": "10.1109/IJCNN.2013.6706807"
},
{
"id": "HCI Challenging Stereo",
"href": "https://hci.iwr.uni-heidelberg.de/benchmarks/Challenging_Data_for_Stereo_and_Optical_Flow",
"size_storage": "-",
"size_hours": "-",
"frames": "10000",
"numberOfScenes": "11",
"samplingRate": "100",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "2 x Photon Focus MV1-D1312-160-CL-12",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "-",
"licensing": "Freely available for research purposes only",
"relatedDatasets": "-",
"publishDate":"2012-03-07",
"lastUpdate": "-",
"paperTitle": "Outdoor stereo camera system for the generation of real-world benchmark data sets",
"relatedPaper": "https://www.spiedigitallibrary.org/journals/Optical-Engineering/volume-51/issue-2/021107/Outdoor-stereo-camera-system-for-the-generation-of-real-world/10.1117/1.OE.51.2.021107.short?SSO=1",
"location": "-",
"rawData": "-",
"DOI": "10.1117/1.OE.51.2.021107"
},
{
"id": "HD1K",
"href": "http://hci-benchmark.iwr.uni-heidelberg.de/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "55",
"samplingRate": "25",
"lengthOfScenes": "-",
"sensors": "lidar, camera, gps/imu",
"sensorDetail": "1x RIEGL VMX-250-CS6 laser scanner, 1x stereo system with 2 cameras 2560x1080 69.5Â° 200Hz, 1x Applanix POS-LV 510 gnss & imu unit",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "stereo, optical flow, single image depth prediction, object detection, and semantic segmentation",
"annotations": "depth and optical flow",
"licensing": "Freely available for research purposes",
"relatedDatasets": "-",
"publishDate":"2018-02-01",
"lastUpdate": "2018-03-05",
"paperTitle": "The HCI Benchmark Suite: Stereo And Flow Ground Truth With Uncertainties for Urban Autonomous Driving",
"relatedPaper": "http://hci-benchmark.iwr.uni-heidelberg.de/media/publications//kondermann2016.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/CVPRW.2016.10"
},
{
"id": "LUMPI",
"href": "https://data.uni-hannover.de/ne/dataset/lumpi",
"size_storage": "-",
"size_hours": "2.45",
"frames": "200000",
"numberOfScenes": "7",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "1x Raspberry Pi Camera Module v2 1640x1232 62.2Â° 30Hz, 1x Dream Chip Technologies ATOM One 1920x1080 89Â° 50Hz, 1x Xiaomi Yi Action Camera 1920x1080 155Â° 30Hz, 1x Velodyne VLP-16 360Â° 10Hz, 1x Velodyne HDL64-S3 360Â° 10Hz, 1x Hesai Pandar64 360Â° 10Hz, 1x Hesai PandarQT 360Â° 10Hz",
"recordingPerspective": "bird's eye view",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "2d/3d bounding boxes with class labels",
"licensing": "Creative Commons Attribution-NonCommercial 3.0",
"relatedDatasets": "-",
"publishDate":"2022-06-05",
"lastUpdate": "-",
"paperTitle": "LUMPI: The Leibniz University Multi-Perspective Intersection Dataset",
"relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9827157",
"location": "Hanover, Germany",
"rawData": "-",
"DOI": "10.1109/iv51971.2022.9827157"
},
{
"id": "Highway Work Zones",
"href": "http://www.andrew.cmu.edu/user/jonghole/workzone/data/"
},
{
"id": "Amodal Cityscapes",
"href": "https://github.com/ifnspaml/AmodalCityscapes",
"size_storage": "-",
"size_hours": "-",
"frames": "3472",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "Dataset taken from Cityscapes and occlusions pasted artficially for amodal segmentation tasks",
"recordingPerspective": "ego-perspective",
"dataType":"-",
"mapData": "No",
"benchmark": "amodal semantic segmentation method",
"annotations": "semantic segmentation and amodal semantic segmentation",
"licensing": "Freely available code to generate the dataset",
"relatedDatasets": "-",
"publishDate":"2022-06-01",
"lastUpdate": "-",
"paperTitle": "Amodal Cityscapes: A New Dataset, its Generation, and an Amodal Semantic Segmentation Challenge Baseline",
"relatedPaper": "https://arxiv.org/pdf/2206.00527.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.48550/arXiv.2206.00527"
},
{
"id": "LISA Traffic Sign",
"href": "http://cvrr.ucsd.edu/LISA/lisa-traffic-sign-dataset.html",
"size_storage": "-",
"size_hours": "-",
"frames": "6610",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding boxes with description",
"licensing": "Academic license",
"relatedDatasets": "LISA Traffic Light Dataset",
"publishDate":"2012-10-19",
"lastUpdate": "-",
"paperTitle": "Vision-Based Traffic Sign Detection and Analysis for Intelligent Driver Assistance Systems: Perspectives and Survey",
"relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6335478&casa_token=iD5J89e1Y0MAAAAA:OKU-qJJPvSmZk1wJGlPb4G3Z7SF12nHZdr7mEV03pwgl2Q8ASZlH7T2zbo5n65e4yBniT_S5jQfn&tag=1",
"location": "USA",
"rawData": "-",
"DOI": "10.1109/TITS.2012.2209421"
},
{
"id": "LISA Traffic Light",
"href": "http://cvrr.ucsd.edu/LISA/datasets.html",
"size_storage": "5",
"size_hours": "0.75",
"frames": "43007",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x Point Grey's Bumblebee XB3 (BBX3-13S2C-60) stereo camera 1280x960 ",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding boxes with the description of state of traffic light",
"licensing": "Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",
"relatedDatasets": "LISA Traffic Sign Dataset",
"publishDate":"2016-02-03",
"lastUpdate": "-",
"paperTitle": "Vision for Looking at Traffic Lights: Issues, Survey, and Perspectives",
"relatedPaper": "https://ieeexplore.ieee.org/document/7398055",
"location": "San Diego, California, USA",
"rawData": "-",
"DOI": "10.1109/TITS.2015.2509509"
},
{
"id": "Malaga Stereo and Laser Urban",
"href": "https://www.mrpt.org/MalagaUrbanDataset",
"size_storage": "4.76",
"size_hours": "1.55",
"frames": "-",
"numberOfScenes": "15",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "1x Point Grey Research Bumblebee 2 stereo camera 1024x768 20Hz, 3x Hokuyo UTM-30LX laser scanners 270Â°, 2x SICK LMS-200 laser scanners, 1x xSens MTi imu 100Hz, 2x mmGPS devices from Topcon gps",
"recordingPerspective": "ego-perspective and bird's eye",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2013-10-09",
"lastUpdate": "2018-09-13",
"paperTitle": "The MÃ¡laga urban dataset: High-rate stereo and LiDAR in a realistic urban scenario",
"relatedPaper": "https://journals.sagepub.com/doi/10.1177/0278364913507326",
"location": "MÃ¡laga, Spain",
"rawData": "-",
"DOI": "10.1177/0278364913507326"
},
{
"id": "KITTI-360 PanopticBEV",
"href": "http://panoptic-bev.cs.uni-freiburg.de/",
"size_storage": "3.9",
"size_hours": "-",
"frames": "64071",
"numberOfScenes": "9",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "Monocular images and lidar pointclouds taken from KITTI dataset",
"recordingPerspective": "ego-perspective and bird's eye",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "panoptic segmentation",
"licensing": "Freely available for non-commercial purposes",
"relatedDatasets": "nuScenes PanopticBEV",
"publishDate":"2022-01-13",
"lastUpdate": "-",
"paperTitle": "Bird's-Eye-View Panoptic Segmentation Using Monocular Frontal View Images",
"relatedPaper":"https://arxiv.org/abs/2108.03227",
"location": "Karlsruhe, Germany",
"rawData": "-",
"DOI": "10.1109/LRA.2022.3142418"
},
{
"id": "nuScenes PanopticBEV",
"href": "http://panoptic-bev.cs.uni-freiburg.de/",
"size_storage": "2.1",
"size_hours": "-",
"frames": "34149",
"numberOfScenes": "850",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "Monocular images and lidar pointclouds taken from nuScenes dataset",
"recordingPerspective": "ego-perspective and bird's eye",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "panoptic segmentation",
"licensing": "Freely available for non-commercial purposes",
"relatedDatasets": "KITTI-360 PanopticBEV",
"publishDate":"2022-01-13",
"lastUpdate": "-",
"paperTitle": "Bird's-Eye-View Panoptic Segmentation Using Monocular Frontal View Images",
"relatedPaper":"https://arxiv.org/abs/2108.03227",
"location": "Boston, USA and Singapore",
"rawData": "-",
"DOI": "10.1109/LRA.2022.3142418"
},
{
"id": "Waymo Block-NeRF",
"href": "https://waymo.com/research/block-nerf/",
"relatedPaper":"https://arxiv.org/abs/2202.05263",
"paperTitle": "Block-NeRF: Scalable Large Scene Neural View Synthesis"
}, 
{
"id": "Mapillary Vistas",
"href": "https://www.mapillary.com/dataset/vistas",
"size_storage": "",
"size_hours": "-",
"frames": "25000",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "-",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "semantic image segmentation and instance-specific image segmentation",
"annotations": "instance specific object annotations",
"licensing": " Creative Commons Attribution NonCommercial Share Alike (CC BY-NC-SA)",
"relatedDatasets": "Mapillary Traffic Sign",
"publishDate":"2017-12-25",
"lastUpdate": "2021-01-18",
"paperTitle": "The Mapillary Vistas Dataset for Semantic Understanding of Street Scene",
"relatedPaper": "https://ieeexplore.ieee.org/document/8237796",
"location": "Europe, North and South America, Asia, Africa and Oceania",
"rawData": "-",
"DOI": "10.1109/ICCV.2017.534"
},
{
"id": "MTSD",
"href": "https://www.mapillary.com/dataset/trafficsign",
"size_storage": "-",
"size_hours": "-",
"frames": "105830",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Map",
"benchmark": "-",
"annotations": "2d bounding boxes and sign classification",
"licensing": "Creative Commons Attribution NonCommercial Share Alike (CC BY-NC-SA)",
"relatedDatasets": "Mapillary Vistas",
"publishDate":"2020-11-03",
"lastUpdate": "-",
"paperTitle": "The Mapillary Traffic Sign Dataset for Detection and Classification on a Global Scale",
"relatedPaper": "https://arxiv.org/abs/1909.04422",
"location": "Europe, North and South America, Asia, Africa and Oceania",
"rawData": "-",
"DOI": "10.1007/978-3-030-58592-1_5"
},
{
"id": "TT100K",
"href": "http://cg.cs.tsinghua.edu.cn/traffic-sign/",
"size_storage": "-",
"size_hours": "-",
"frames": "100000",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "traffic-sign detection and classification",
"annotations": "2d bounding boxes and class labels",
"licensing": "Creative Commons Attribution-NonCommercial (CC-BY-NC",
"relatedDatasets": "-",
"publishDate":"2016-06-27",
"lastUpdate": "-",
"paperTitle": "Traffic-Sign Detection and Classification in the Wild",
"relatedPaper": "http://cg.cs.tsinghua.edu.cn/traffic-sign/0682.pdf",
"location": "China",
"rawData": "-",
"DOI": "10.1109/CVPR.2016.232"
},
{
"id": "NEXET",
"href": "https://www.kaggle.com/datasets/solesensei/nexet-original",
"size_storage": "10.71",
"size_hours": "-",
"frames": "91190",
"numberOfScenes": "91190",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "Images recorded through an App",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding box",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2017-08-17",
"lastUpdate": "-",
"paperTitle": "NEXET â The Largest and Most Diverse Road Dataset in the World",
"relatedPaper": "https://blog.getnexar.com/https-medium-com-itayklein-intro-nexet-50e9b596d0e5",
"location": "77 countries",
"rawData": "-",
"DOI": "-"
},
{
"id": "Road Damage",
"href":"https://github.com/sekilab/RoadDamageDetector/",
"size_storage": "2.4",
"size_hours": "-",
"frames": "13135",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding box with class labels",
"licensing": "Creative Commons Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0)",
"relatedDatasets": "Road Damage Dataset 2018",
"publishDate":"2018-06-30",
"lastUpdate": "2020-06-02",
"paperTitle": "Road Damage Detection and Classification Using Deep Neural Networks with Smartphone Images",
"relatedPaper": "https://arxiv.org/pdf/1801.09454.pdf",
"location": "Japan",
"rawData": "-",
"DOI": "10.1111/mice.12387"
},
{
"id": "RDD2020",
"href":"https://data.mendeley.com/datasets/5ty2wb6gvg/2",
"size_storage": "1.76",
"size_hours": "-",
"frames": "26336",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding boxes",
"licensing": "Attribution-NonCommercial 3.0 Unported (CC BY NC 3.0)",
"relatedDatasets": "-",
"publishDate":"2021-03-18",
"lastUpdate": "2021-09-27",
"paperTitle": "RDD2020: An annotated image dataset for automatic road damage detection using deep learning",
"relatedPaper": "https://www.sciencedirect.com/science/article/pii/S2352340921004170#bib0001",
"location": "India, Japan and Czech Republic",
"rawData": "-",
"DOI": "j.dib.2021.107133"
},
{
"id": "GTA5",
"href": "https://download.visinf.tu-darmstadt.de/data/from_games/",
"size_storage": "57.05",
"size_hours": "-",
"frames": "24966",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "camera 1914x1052",
"recordingPerspective": "ego-perspective",
"dataType":"Synthetic",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic segmentation of different classes",
"licensing": "Freely available for research and educational purposes",
"relatedDatasets": "-",
"publishDate":"2016-04-08",
"lastUpdate": "-",
"paperTitle": "Playing for Data: Ground Truth from Computer Games",
"relatedPaper": "https://arxiv.org/pdf/1608.02192.pdf",
"location": "Game: Grand Theft Auto 5",
"rawData": "-",
"DOI": "10.1007/978-3-319-46475-6_7"
},
{
"id": "Ground Truth Stixel",
"href": "http://www.6d-vision.com/ground-truth-stixel-dataset",
"size_storage": "3.2",
"size_hours": "-",
"frames": "78500",
"numberOfScenes": "318",
"samplingRate": "25",
"lengthOfScenes": "-",
"sensors": "camera, radar",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "stixel measurements",
"licensing": "freely available for of scholarly and technical work",
"relatedDatasets": "-",
"publishDate":"2013-06-23",
"lastUpdate": "-",
"paperTitle": "Exploiting the Power of Stereo Confidences",
"relatedPaper": "http://wwwlehre.dhbw-stuttgart.de/~sgehrig/stixelGroundTruthDataset/stixel.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/CVPR.2013.45"
},
{
"id": "Boxy",
"href": "https://boxy-dataset.com/boxy/",
"size_storage": "-",
"size_hours": "-",
"frames": "200000",
"numberOfScenes": "34",
"samplingRate": "15",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x mvBlueFOX3-2051 with a Sony IMX250 chip 2464x2056 15Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "bounding box, polygon, and real-time detections",
"annotations": "2d boxes or 3d cuboids for vehicles",
"licensing": "Freely available for non-commercial purposes",
"relatedDatasets": "-",
"publishDate":"2019-10-27",
"lastUpdate": "-",
"paperTitle": "Boxy Vehicle Detection in Large Images",
"relatedPaper": "https://boxy-dataset.com/static/boxy/boxy_preview.pdf",
"location": "San Francisco Bay Area, California, USA",
"rawData": "-",
"DOI": "10.1109/ICCVW.2019.00112"
},
{
"id": "CarlaScenes",
"href": "https://github.com/CarlaScenes/CarlaSence",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "30",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "1x RGB camera 1280x960 30Hz, 1x Semantic Segmentation camera 1280x960 30Hz, 1x Depth camera 1280x960 30Hz, 1x Velodyne 16/64 lidar 30Hz, 1x Velodyne 16/64 Semantic lidar 30Hz, 1x IMU, 1x GNSS",
"recordingPerspective": "ego-perspective",
"dataType":"Synthetic",
"mapData": "Yes",
"benchmark": "odometry methods in autonomous driving",
"annotations": "semantic annotation at the instance level for both image and lidar data",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2022-06-19",
"lastUpdate": "-",
"paperTitle": "CarlaScenes: A synthetic dataset for odometry in autonomous driving",
"relatedPaper": "https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Kloukiniotis_CarlaScenes_A_Synthetic_Dataset_for_Odometry_in_Autonomous_Driving_CVPRW_2022_paper.pdf",
"location": "CARLA",
"rawData": "-",
"DOI": "10.1109/CVPRW56347.2022.00498"
},
{
"id": "K-Lane",
"href": "https://github.com/kaist-avelab/k-lane",
"size_storage": "43",
"size_hours": "-",
"frames": "15382",
"numberOfScenes": "15382",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "1x camera 1920x1200, 1x Ouster OS2-64 Lidar sensor",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "lane markings and segmntation",
"licensing": "Apache-2.0",
"relatedDatasets": "-",
"publishDate":"2022-04-18",
"lastUpdate": "-",
"paperTitle": "K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways",
"relatedPaper": "https://arxiv.org/abs/2110.11048" ,
"location": "-",
"rawData": "Yes",
"DOI": "10.1109/CVPRW56347.2022.00491"
},
{
"id": "Ithaca365",
"href": "https://ithaca365.mae.cornell.edu/",
"size_storage": "-",
"size_hours": "-",
"frames": "682217",
"numberOfScenes": "682217",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "4x Sekonix SF3325-100 cameras 1928x1208 30Hz, 2x Velodyne Puck (VLP-16) LiDAR 10Hz, 1x Ouster OS2-128 long range LiDAR 10Hz, 1x Novatel PwrPak7D dual GNSS-502 antenna, 1x Epson G320N IMU",
"recordingPerspective": "ego-perspetive",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "Amodal road labelling, Amodal instance segmentation",
"licensing": "Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",
"relatedDatasets": "-",
"publishDate":"2022-08-01",
"lastUpdate": "-",
"paperTitle": "Ithaca365: Dataset and Driving Perception under Repeated and Challenging Weather Conditions",
"relatedPaper": "https://arxiv.org/abs/2208.01166",
"location": "-",
"rawData": "Yes",
"DOI": "10.48550/arXiv.2208.01166"
}, 
{
"id": "TME Motorway",
"href": "http://cmp.felk.cvut.cz/data/motorway/",
"size_storage": "-",
"size_hours": "0.45",
"frames": "30000",
"numberOfScenes": "28",
"samplingRate": "20",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "1x stereo camera 1024x768 grayscale 32Â° 20Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "bounding boxes around vehicles",
"licensing": "Freely available for commercial and non-commercial research",
"relatedDatasets": "-",
"publishDate":"2012-09-16",
"lastUpdate": "-",
"paperTitle": "A System for Real-time Detection and Tracking of Vehicles from a Single Car-mounted Camera",
"relatedPaper": "http://cmp.felk.cvut.cz/data/motorway/paper/itsc2012.pdf",
"location": "Northern Italy",
"rawData": "-",
"DOI": "10.1109/ITSC.2012.6338748"
},
{
"id": "TuSimple",
"href": "https://www.tusimple.com/"
},
{
"id": "ScribbleKITTI",
"href": "https://github.com/ouenal/scribblekitti",
"size_storage": "0.12",
"size_hours": "-",
"frames": "19130",
"numberOfScenes": "10",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "lidar",
"sensorDetail": "Dataset taken from SemanticKITTI which is based on KITTI and then lidar data is annotated",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d/3d semantic segmentation",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2022-03-01",
"lastUpdate": "-",
"paperTitle": "Scribble-Supervised LiDAR Semantic Segmentation",
"relatedPaper":"https://arxiv.org/abs/2203.08537",
"location": "Karlsruhe, Germany",
"rawData": "-",
"DOI": "10.48550/arXiv.2203.08537"
},
{
"id": "UAH-DriveSet",
"href": "http://www.robesafe.uah.es/personal/eduardo.romera/uah-driveset/",
"size_storage": "-",
"size_hours": "8.33",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, gps",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "Freely available to academic and non-academic entities for non-commercial purposes such as academic research, teaching, scientific publications, or personal experimentation",
"relatedDatasets": "-",
"publishDate":"2016-11-01",
"lastUpdate": "-",
"paperTitle": "Need Data for Driver Behaviour Analysis? Presenting the Public UAH-DriveSet",
"relatedPaper": "http://www.robesafe.uah.es/personal/eduardo.romera/pdfs/Romera16itsc.pdf",
"location": "-",
"rawData": "Yes",
"DOI": "10.1109/ITSC.2016.7795584"
},
{
"id": "Unsupervised Llamas",
"href": "https://unsupervised-llamas.com/llamas/",
"size_storage": "-",
"size_hours": "-",
"frames": "100042",
"numberOfScenes": "14",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "binary lane marker segmentation, lane-dependent pixel-level segmentation and lane border regression",
"annotations": "dashed lane markings",
"licensing": "Freely available for non-commercial reseach purposes only",
"relatedDatasets": "-",
"publishDate":"2019-10-27",
"lastUpdate": "-",
"paperTitle": "Unsupervised Labeled Lane Markers Using Maps",
"relatedPaper": "https://unsupervised-llamas.com/static/llamas/llamas_preview.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/ICCVW.2019.00111"
},
{
"id": "Street Hazards",
"href": "https://github.com/hendrycks/anomaly-seg",
"size_storage": "2.0",
"size_hours": "-",
"frames": "7656",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"No",
"mapData": "No",
"benchmark": "Anamoly object segmentation",
"annotations": "semantic segmentation of anamolies",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2019-11-25",
"lastUpdate": "-",
"paperTitle": "Scaling Out-of-Distribution Detection for Real-World Settings",
"relatedPaper": "https://arxiv.org/pdf/1911.11132.pdf",
"location": "Carla",
"rawData": "-",
"DOI": "10.48550/arXiv.1911.11132"
},
{
"id": "Astyx HiRes 2019",
"href": "https://www.astyx.com/fileadmin/redakteur/dokumente/Astyx_Dataset_HiRes2019_specification.pdf"
},
{
"id": "A*3D",
"href": "https://github.com/I2RDL2/ASTAR-3D",
"size_storage": "-",
"size_hours": "-",
"frames": "39179",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "2x PointGrey Chameleon3 USB3 Global shutter color cameras 2048x1536 57.3Â° 55Hz, 1x Velodyne HDL-64ES3 3D-LiDAR 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "3d object detection",
"annotations": "3d bounding boxes",
"licensing": "Freely available for noncommercial academic research purposes only",
"relatedDatasets": "-",
"publishDate":"2019-10-04",
"lastUpdate": "-",
"paperTitle": "A 3D Dataset: Towards Autonomous Driving in Challenging Environments",
"relatedPaper": "https://arxiv.org/pdf/1909.07541.pdf",
"location": "Singapore",
"rawData": "-",
"DOI": "10.1109/ICRA40945.2020.9197385"
},
{
"id": "CamVid",
"href": "https://www.kaggle.com/carlolepelaars/camvid",
"size_storage": "-",
"size_hours": "0.16",
"frames": "701",
"numberOfScenes": "1",
"samplingRate": "30",
"lengthOfScenes": "600",
"sensors": "camera",
"sensorDetail": "1x CCD Panasonic HVX200 digital camera 960x720 30Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic segmentation",
"licensing": "Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",
"relatedDatasets": "-",
"publishDate":"2009-01-15",
"lastUpdate": "-",
"paperTitle": "Semantic object classes in video: A high-definition ground truth database",
"relatedPaper": "https://pdf.sciencedirectassets.com/271524/1-s2.0-S0167865508X00169/1-s2.0-S0167865508001220/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEGEaCXVzLWVhc3QtMSJHMEUCIQCA11sHV8h2EpCAzXyQ0V4VP%2F%2FtSTtdmwWBVRbF4T8AMwIgXu%2BDzZ9%2FZ5Matmw%2BNuJxkagSqNnDCZObjFIe3pMY2sgq2wQIif%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAEGgwwNTkwMDM1NDY4NjUiDF3yb0q0O6GdJqshbiqvBGiWqPb8EEGrHPUFfAVoMjtnfz4EEc6DVL5rwsMRYK7K5%2FddZ3n3M82ULw%2FjzUKCkbxBijZtK8o2tl%2BLKeIZ5y3Feep%2FIjIIhQ%2FjiZeni0N07G1%2FCQq6SOZivayRtp7CUjdgTXDq4zduoNEYNE1CZ4hhTLWK2RFjlPNmdOGUxCZbT2MNy27kbcwnjQw6u%2BF5Ro6L1q%2FI9bul4%2Foc0W40c0LnEQXMe5z8hTBKEFMwT5cEUf9%2FpnS%2BskWugGgsSNFiACeEJ0NETcVhuF6FLbDICqLE%2F7qw8wG0gzbDm2SljpYiw0gkW5gxczQE3Ru3sZ%2BQuUmh1zdCk4kt%2FazqaKZJqlvCnLgiQWuF%2BRNK4Hpteh%2B4i517UrWEAU8fpXuAyMVZG8JVG6xR%2FpCHWU7H1VNEA%2FbrL5zLkYp5SzX5xFzumsVeIJ37u69HuBgFFvKOn%2BmPtasGf5TudFpcA%2FE7d7C7lcES0AvfYbhyh%2FFqHCfG4s1MwfU6qJ61nZcYSMZgEFIiTlZGR80vgkYxCvIP0sNlgdOCQHArjxYHJ6yzjkPBFQ90c2snBDkfYfWcKrvWY%2FdGp4x8TBdaqrY07YEASRAOdd%2BJJUonHQDSRUJKmXibsNBM40z1qGflHPVL0wWAM3YITapAaLPxSagAqTPJ5AoOSXElsogZYoTHHXJU8KNIblOaz6XnKx32TLnw4FRkEeVBTwjOEFr5EA4srS1qbXlJsjV0aObvKBsr9Ja1UItx3CowtcCKlgY6qQFO6pFryXLKH4V7tldhJn%2BcQ35KUZOua1QBWSBkg38vxrjhQfEyvKCns7X21TxnzARlljVwzj9rL3HtZ7co4%2BT6pxwkYvj%2FKsTotHp5qtV5dYAI4w0dLNCsPz33OdLYqbBoIfr1OQ5USj9MvuO2k2e2voHKW1SmusBW5goBvuPNuWe4dHdFZAYtvtwGAs6iCSxbAYCPv6I%2BdPySt7vGydS05xTW%2FcPUZNGE&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220704T090151Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYWIZ3RZPY%2F20220704%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=f4dcb0c82201f26036b5850b0f38e19def847297bc9cc18b614ee094af344449&hash=8eef19412becfacf9bb21603db158b613ef51da58582217142ebc507be134724&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0167865508001220&tid=spdf-ec513c9a-f60a-4935-a0ec-6f936e44745a&sid=a0422e5a30875748cf6a74770c17f8e2edf3gxrqb&type=client&ua=4d50575004060303040a&rr=7256adbaee0c694c",
"location": "United Kingdom",
"rawData": "Yes",
"DOI": "10.1016/j.patrec.2008.04.005"
},
{
"id": "Daimler Urban Segmentation",
"href": "https://computervisiononline.com/dataset/1105138608"
},
{
"id": "VPGNet",
"href": "https://arxiv.org/abs/1710.06288",
"size_storage": "-",
"size_hours": "-",
"frames": "21097",
"numberOfScenes": "-",
"samplingRate": "30",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "lane and road markings, vanishing point",
"licensing": "Available for non commercial research purposes on registration",
"relatedDatasets": "-",
"publishDate":"2017-10-22",
"lastUpdate": "-",
"paperTitle": "VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition",
"relatedPaper": "https://arxiv.org/abs/1710.06288",
"location": "Seoul, South Korea",
"rawData": "-",
"DOI": "10.1109/ICCV.2017.215"
},
{
"id": "Toronto 3D",
"href": "https://github.com/WeikaiTan/Toronto-3D",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps",
"sensorDetail": "1x Teledyne Optech Maverick consists of a 32-line LiDAR sensor, a Ladybug 5 panoramic camera, a GNSS system, and a SLAM system",
"recordingPerspective": "bird's eye view",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "object class labels",
"licensing": "Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)",
"relatedDatasets": "-",
"publishDate":"2020-03-22",
"lastUpdate": "2020-04-23",
"paperTitle": "Toronto-3D: A Large-scale Mobile LiDAR Dataset for Semantic Segmentation of Urban Roadways",
"relatedPaper": "https://arxiv.org/abs/2003.08284",
"location": "Toronto, Canada",
"rawData": "-",
"DOI": "10.1109/CVPRW50498.2020.00109"
},
{
"id": "Toronto City",
"href": "http://www.cs.toronto.edu/~byang/papers/Tcity_iccv17.pdf",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective and bird's eye",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "semantic segmentation of certain classes",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2017-10-22",
"lastUpdate": "-",
"paperTitle": "TorontoCity: Seeing the World with a Million Eyes",
"relatedPaper": "https://arxiv.org/abs/1612.00423",
"location": "Toronto, Canada",
"rawData": "-",
"DOI": "10.1109/ICCV.2017.327"
},
{
"id": "CrashD",
"href": "https://crashd-cars.github.io/",
"size_storage": "19",
"size_hours": "-",
"frames": "15340",
"numberOfScenes": "15340",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "64-beam LiDAR in simulator sensors",
"recordingPerspective": "ego-perspective",
"dataType":"Synthetic",
"mapData": "No",
"benchmark": "-",
"annotations": "Class labels for damage types",
"licensing": "Creative Commons Attribution-ShareAlike 4.0 International License",
"relatedDatasets": "-",
"publishDate":"2022-06-18",
"lastUpdate": "-",
"paperTitle": "3D-VField: Adversarial Augmentation of Point Clouds for Domain Generalization in 3D Object Detection",
"relatedPaper":"https://arxiv.org/abs/2112.04764",
"location": "BeamNG simulator",
"rawData": "-",
"DOI": "10.1109/CVPR52688.2022.01678"
},
{
"id": "SHIFT",
"href": "https://www.vis.xyz/shift/",
"size_storage": "10000",
"size_hours": "-",
"frames": "2500000",
"numberOfScenes": "4850",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "1x Multi-View RGB 5 cameras set: 1280x800 90Â° 10Hz, 1x stereo RGB camera set 1280x800 90Â° 10Hz, 1x optical flow sensor, 1x depth camera 1280x800 90Â° 10Hz, 1x 128-channel LiDAR 36Â° 10Hz, 1x gnss sensor, 1x imu sensor",
"recordingPerspective": "ego-perspective",
"dataType":"Synthetic",
"mapData": "No",
"benchmark": "-",
"annotations": "2d/3d bounding boxes, optical flow, depth maps, instance segmentation and semantic segmentation",
"licensing": "Creative Commons Attribution-ShareAlike 4.0 International License",
"relatedDatasets": "",
"publishDate":"2022-06-10",
"lastUpdate": "2022-09-16",
"paperTitle": "SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation",
"relatedPaper":"https://arxiv.org/abs/2206.08367",
"location": "CARLA",
"rawData": "-",
"DOI": "10.48550/arXiv.2206.08367"
},
{
"id": "Synthia",
"href": "https://synthia-dataset.net/",
"size_storage": "-",
"size_hours": "-",
"frames": "213400",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x rgb camera 960x720 100Â°",
"recordingPerspective": "ego-perspective",
"dataType":"Synthetic",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic segmentation",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0",
"relatedDatasets": "-",
"publishDate":"2016-06-27",
"lastUpdate": "2019-10-27",
"paperTitle": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes",
"relatedPaper": "https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.pdf",
"location": "Unity development platform",
"rawData": "-",
"DOI": "10.1109/CVPR.2016.352"
},
{
"id": "RANUS",
"href": "https://sites.google.com/site/gmchoe1/ranus",
"size_storage": "11.4",
"size_hours": "-",
"frames": "40000",
"numberOfScenes": "50",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "2x Point-grey grasshopper cameras (NIR: GS3-U3-41C6NIR-C, RGB: GS3-U3-41C6C-C) 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "pixel level semantic segmentation masks",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2018-02-02",
"lastUpdate": "-",
"paperTitle": "RANUS: RGB and NIR Urban Scene Dataset for Deep Scene Parsing",
"relatedPaper": "https://joonyoung-cv.github.io/assets/paper/18_ral_ranus.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/LRA.2018.2801390"
},
{
"id": "One Thousand and One Hours",
"href": "https://level-5.global/data/prediction/",
"size_storage": "-",
"size_hours": "1118",
"frames": "-",
"numberOfScenes": "170000",
"samplingRate": "-",
"lengthOfScenes": "25",
"sensors": "camera, lidar, radar",
"sensorDetail": "-",
"recordingPerspective": "top view",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "trajectories",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike license (CC-BY-NC-SA-4.0)",
"relatedDatasets": "Level 5 Perception Dataset",
"publishDate":"2020-06-25",
"lastUpdate": "-",
"paperTitle": "One Thousand and One Hours: Self-driving Motion Prediction Dataset",
"relatedPaper": "https://arxiv.org/pdf/2006.14480.pdf",
"location": "Palo Alto, California, USA",
"rawData": "-",
"DOI": "10.48550/arXiv.2006.14480"
},
{
"id": "LIBRE",
"href": "https://sites.google.com/g.sp.m.is.nagoya-u.ac.jp/libre-dataset/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, odometry, gps/imu",
"sensorDetail": "10 different lidars compared (VLS-128, HDL-64S2, HDL-32E, VLP-32C, VLP-16, Pandar64, Pandar40P, OS1-64, OS1-16, RS-Lidar32)",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "Lidar sensors",
"annotations": "-",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2020-10-19",
"lastUpdate": "-",
"paperTitle": "LIBRE: The Multiple 3D LiDAR Dataset",
"relatedPaper": "https://arxiv.org/pdf/2003.06129.pdf",
"location": "Nagoya, Japan",
"rawData": "-",
"DOI": "10.1109/IV47402.2020.9304681"
},
{
"id": "PREVENTION",
"href": "https://prevention-dataset.uah.es/",
"size_storage": "-",
"size_hours": "6",
"frames": "-",
"numberOfScenes": "11",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "2x Grasshopper3 cameras 1920x1200 163Hz, 1x Velodyne HDL-32E 10Hz, 1x Continental ARS308 long-range radar 56Â° 15Hz, 2x  Continental SRR208 blind-corner radars 150Â° 33Hz, 1x Trimble Net R9 Geospatial gnss 20Hz, 1x MPU6050 imu",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "bounding box, trajectory information, lane markings, object classification",
"licensing": "Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",
"relatedDatasets": "-",
"publishDate":"2019-10-27",
"lastUpdate": "-",
"paperTitle": "The PREVENTION dataset: a novel benchmark for PREdiction of VEhicles iNTentIONs",
"relatedPaper": "https://ieeexplore.ieee.org/document/8917433",
"location": "Madrid, Spain",
"rawData": "Yes",
"DOI": "10.1109/ITSC.2019.8917433"
},
{
"id": "Stanford Track Collection",
"href": "https://cs.stanford.edu/people/teichman/stc/",
"size_storage": "2",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "lidar, gps/imu",
"sensorDetail": "1x Velodyne HDL-64E S2 lidar 360Â° 10Hz, 1x Applanix LV-420 gps 200Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "-",
"licensing": "Freely available for non-commercial research",
"relatedDatasets": "-",
"publishDate":"2011-05-09",
"lastUpdate": "2012-06-01",
"paperTitle": "Towards 3D object recognition via classification of arbitrary object tracks",
"relatedPaper": "https://cs.stanford.edu/people/teichman/papers/icra2011.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/ICRA.2011.5979636"
},
{
"id": "LiDAR-Video Driving",
"href": "https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf",
"size_storage": "1000",
"size_hours": "-",
"frames": "10000",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, accelerometer, steering angle meter",
"sensorDetail": "1x HDL-32E Velodyne lidar 360Â° 10Hz, 1x VLP-16 Velodyne lidar, 1x Dashboard camera 1920x1080 30Hz ",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "driver behaviour",
"annotations": "-",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2018-06-18",
"lastUpdate": "-",
"paperTitle": "LiDAR-Video Driving Dataset: Learning Driving Policies Effectively",
"relatedPaper": "https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/CVPR.2018.00615"
},
{
"id": "WoodScape",
"href": "https://paperswithcode.com/dataset/woodscape",
"size_storage": "-",
"size_hours": "-",
"frames": "10000",
"numberOfScenes": "-",
"samplingRate": "30",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu, odometer",
"sensorDetail": "4x 1MPx RGB fisheye cameras 190Â°, 1x Velodyne HDL-64E lidar 20Hz, 1x NovAtel Propak6 & SPAN-IGM-A1 gnss/imu, 1x Garmin 18x GNSS Positioning with SPS, Odometry signals from the vehicle bus",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic segmentation, monocular depth estimation, 2D & 3D bounding boxes,  visual odometry, visual SLAM, motion segmentation, soiling detection and end-to-end driving (driving controls)",
"licensing": "Freely available for non-commercial research",
"relatedDatasets": "-",
"publishDate":"2019-10-27",
"lastUpdate": "2021-11-16",
"paperTitle": "WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving",
"relatedPaper": "https://arxiv.org/pdf/1905.01489.pdf",
"location": "USA, Europe, and China",
"rawData": "-",
"DOI": "10.1109/ICCV.2019.00940"
},
{
"id": "Raincouver",
"href": "https://ieeexplore.ieee.org/document/7970170",
"size_storage": "-",
"size_hours": "0.5",
"frames": "-",
"numberOfScenes": "10",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x windshield mounted camera 720p 30Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic segmentation",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2017-07-06",
"lastUpdate": "-",
"paperTitle": "The Raincouver Scene Parsing Benchmark for Self-Driving in Adverse Weather and at Night",
"relatedPaper": "https://ieeexplore.ieee.org/document/7970170",
"location": "Vancouver, Canada",
"rawData": "-",
"DOI": "10.1109/LRA.2017.2723926"
},
{
"id": "TRoM",
"href": "http://www.tromai.icoc.me/",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "712",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, gps",
"sensorDetail": "1x PointGray color camera 1280x960",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "Road marking detection",
"annotations": "Pixel level road markings",
"licensing": "Freely available for academic use",
"relatedDatasets": "-",
"publishDate":"2017-10-16",
"lastUpdate": "-",
"paperTitle": "Benchmark for road marking detection: Dataset specification and performance baseline",
"relatedPaper": "https://ieeexplore.ieee.org/document/8317749",
"location": "Beijing, China",
"rawData": "-",
"DOI": "10.1109/ITSC.2017.8317749"
},
{
"id": "Caltech Lanes",
"href": "http://www.mohamedaly.info/datasets/caltech-lanes",
"size_storage": "0.55",
"size_hours": "-",
"frames": "1225",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d splines for lane markings",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2008-06-04",
"lastUpdate": "-",
"paperTitle": "Real time Detection of Lane Markers in Urban Streets",
"relatedPaper": "https://arxiv.org/abs/1411.7113",
"location": "Pasadena, California, USA",
"rawData": "-",
"DOI": "10.1109/IVS.2008.4621152"
},
{
"id": "Complex Urban",
"href": "https://sites.google.com/view/complex-urban-dataset",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "41",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu, altimeter",
"sensorDetail": "2x FLIR FL3-U3-20E4C-C Global shutter color camera 1280x560 10Hz, 2x Velodyne VLP-16 16 CH LiDAR 360Â° 10Hz, 2x SICK LMS-511 1 CH LiDAR 190Â° 100Hz, 1x U-Blox EVK-7P Consumer-level GPS 10Hz, 1x SOKKIA GRX 2 VRS-RTK GPS 1Hz, 1x Xsens MTi-300 Consumer-level AHRS imu 200Hz, 1x  myPressure Altimeter sensor 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 License",
"relatedDatasets": "",
"publishDate":"2017-09-13",
"lastUpdate": "2019-06-17",
"paperTitle": "Complex urban dataset with multi-level sensors from highly diverse urban environments",
"relatedPaper": "https://journals.sagepub.com/doi/pdf/10.1177/0278364919843996",
"location": "South Korea",
"rawData": "Yes",
"DOI": "10.1177/0278364919843996"
},
{
"id": "CCSAD",
"href": "https://www.researchgate.net/publication/277476726_Towards_Ubiquitous_Autonomous_Driving_The_CCSAD_Dataset",
"size_storage": "500",
"size_hours": "1.2",
"frames": "96000",
"numberOfScenes": "42",
"samplingRate": "20",
"lengthOfScenes": "-",
"sensors": "camera, imu/gps",
"sensorDetail": "2x Basler Scout scA1300-32fm Firewire greyscale cameras 1.2MP 33Hz, 1x XSens MTi miniature MEMS-based Attitude and Heading Reference System, 1x Android GPS",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "-",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2015-08-25",
"lastUpdate": "-",
"paperTitle": "Towards Ubiquitous Autonomous Driving: The CCSAD Dataset",
"relatedPaper": "https://link.springer.com/content/pdf/10.1007/978-3-319-23192-1_49.pdf",
"location": "Mexico",
"rawData": "Yes",
"DOI": "10.1007/978-3-319-23192-1_49"
},
{
"id": "Provident Vehicle Detection at Night (PVDN)",
"href": "https://www.kaggle.com/datasets/saralajew/provident-vehicle-detection-at-night-pvdn",
"size_storage": "38",
"size_hours": "-",
"frames": "59746",
"numberOfScenes": "346",
"samplingRate": "18",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "object detection",
"annotations": "bounding boxes for oncoming vehicles and their light projections/reflections",
"licensing": "Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)",
"relatedDatasets": "-",
"publishDate":"2021-09-27",
"lastUpdate": "-",
"paperTitle": "A Dataset for Provident Vehicle Detection at Night",
"relatedPaper": "https://arxiv.org/pdf/2105.13236.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/IROS51168.2021.9636162"
},
{
"id": "Street Learn",
"href": "https://sites.google.com/view/streetlearn/dataset",
"size_storage": "-",
"size_hours": "-",
"frames": "143,000",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "The StreetLearn dataset is a limited set of Google Street View images",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "The dataset is freely available upon request with a license agreement",
"relatedDatasets": "-",
"publishDate":"2019-03-04",
"lastUpdate": "",
"paperTitle": "The StreetLearn Environment and Dataset",
"relatedPaper": "https://arxiv.org/pdf/1903.01292.pdf",
"location": "Manhattan and Pittsburgh, USA",
"rawData": "-",
"DOI": "-"
},
{
"id": "Multi Vehicle Stereo Event Camera",
"href": "https://daniilidis-group.github.io/mvsec/",
"size_storage": "187",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "11",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, VI-Sensor, gps",
"sensorDetail": "2x DAVIS m346B camera 346x60 83Â° 50Hz, 1x Velodyne Puck LITE 360Â° 20Hz, 1x Skybotix integrated VI-sensor stereo camera: 2 x Aptina MT9V034, 1x UBLOX NEO-M8N gps",
"recordingPerspective": "ego-perspective, bird's eye",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "ground truth pose",
"licensing": "Creative Commons Attribution-ShareAlike 4.0 International License",
"relatedDatasets": "-",
"publishDate":"2017-12-10",
"lastUpdate": "2018-09-26",
"paperTitle": "The Multivehicle Stereo Event Camera Dataset: An Event Camera Dataset for 3D Perception",
"relatedPaper": "https://arxiv.org/abs/1801.10202",
"location": "-",
"rawData": "-",
"DOI": "10.1109/LRA.2018.2800793"
},
{
"id": "AMUSE",
"href": "http://www.cvl.isy.liu.se/research/datasets/amuse/",
"size_storage": "1263",
"size_hours": "-",
"frames": "117440",
"numberOfScenes": "7",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, height, gps/imu, accelerometer",
"sensorDetail": "1x Point Grey Ladybug 3^2 camera, 6x synchronized global shutter color cameras 616x1616 360Â° 30Hz, 1x XSens MTi AHRS3 imu sensor, 1x uBlox AEK-4P GPS sensor, 1x Kistler Correvit S-350 Aqua velocity sensor, 1x Kistler HF-500C Height sensor",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "-",
"licensing": "Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported License",
"relatedDatasets": "-",
"publishDate":"2013-06-23",
"lastUpdate": "-",
"paperTitle": "A Multi-sensor Traffic Scene Dataset with Omnidirectional Video",
"relatedPaper": "http://liu.diva-portal.org/smash/get/diva2:623885/FULLTEXT01.pdf",
"location": "Sweden",
"rawData": "-",
"DOI": "10.1109/CVPRW.2013.110"
},
{
"id": "Cheddar Gorge",
"href": "https://www.researchgate.net/publication/228428941_The_Cheddar_Gorge_Data_Set"
},
{
"id": "Annotated Laser",
"href": "https://journals.sagepub.com/doi/pdf/10.1177/0278364910389840"
},
{
"id": "DDD 17",
"href": "https://www.paperswithcode.com/dataset/ddd17"
},
{
"id": "BLVD",
"href": "https://github.com/VCCIV/BLVD",
"size_storage": "42.7",
"size_hours": "-",
"frames": "120000",
"numberOfScenes": "654",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "2x multi-view color cameras 1920x500 30Hz, 1x Velodyne HDL-64E lidar 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "dynamic 4D (3D+temporal) tracking, 5D (4D+interactive) interactive event recognition and intention prediction",
"annotations": "5D semantics, 3D bounding boxes, 4D object IDs, 5D interactive event and 5D intention",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2019-05-20",
"lastUpdate": "-",
"paperTitle": "BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous Driving",
"relatedPaper": "https://arxiv.org/abs/1903.06405",
"location": "Changshu, Jiangsu Province, China",
"rawData": "-",
"DOI": "10.1109/ICRA.2019.8793523"
},
{
"id": "FLIR Thermal",
"href": "https://www.flir.com/oem/adas/adas-dataset-form/",
"size_storage": "16.74",
"size_hours": "-",
"frames": "26442",
"numberOfScenes": "26442",
"samplingRate": "24",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x Teledyne FLIR Tau 2 640x512 45Â°, 1x Teledyne FLIR Blackfly S BFS-U3-51S5C (IMX250) camera 52.8Â°",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding box with class labels",
"licensing": "Free to use",
"relatedDatasets": "-",
"publishDate":"-",
"lastUpdate": "-",
"paperTitle": "-",
"relatedPaper": "-",
"location": "-",
"rawData": "-",
"DOI": "-"
},
{
"id": "Multispectral Object Detection",
"href": "https://deepai.org/publication/multispectral-object-detection-with-deep-learning"
},
{
"id": "CityPersons",
"href": "https://arxiv.org/abs/1702.05693"
},
{
"id": "Tsinghua Daimler Cyclist Detection",
"href": "http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Tsinghua-Daimler_Cyclist_Detec/tsinghua-daimler_cyclist_detec.html",
"size_storage": "-",
"size_hours": "-",
"frames": "11467",
"numberOfScenes": "-",
"samplingRate": "25",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "cyclist detection",
"annotations": "2d bounding boxes",
"licensing": "Freely available non-commercial purposes",
"relatedDatasets": "-",
"publishDate":"2016-06-19",
"lastUpdate": "-",
"paperTitle": "A new benchmark for vision-based cyclist detection",
"relatedPaper": "https://ieeexplore.ieee.org/document/7535515",
"location": "Beijing, China",
"rawData": "-",
"DOI": "10.1109/IVS.2016.7535515"
},
{
"id": "TUD Brussels",
"href": "https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/people-detection-pose-estimation-and-tracking/multi-cue-onboard-pedestrian-detection/",
"size_storage": "3.0",
"size_hours": "-",
"frames": "1016",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "hand held camera 720x576",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding boxes for pedestrians",
"licensing": "-",
"relatedDatasets": "TUD-MotionPairs",
"publishDate":"2009-06-20",
"lastUpdate": "2010-04-13",
"paperTitle": "Multi-cue onboard pedestrian detection",
"relatedPaper": "https://ieeexplore.ieee.org/document/5206638",
"location": "-",
"rawData": "-",
"DOI": "10.1109/CVPR.2009.5206638"
},
{
"id": "ETH Pedestrian",
"href": "https://data.vision.ee.ethz.ch/cvl/aess/dataset/",
"size_storage": "-",
"size_hours": "-",
"frames": "4788",
"numberOfScenes": "4788",
"samplingRate": "13",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x pair of AVT Marlins F033C 640x480 13-14Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d/3d bounding boxes",
"licensing": "Free to use",
"relatedDatasets": "-",
"publishDate":"2008-08-05",
"lastUpdate": "-",
"paperTitle": "A mobile vision system for robust multi-person tracking",
"relatedPaper": "https://ieeexplore.ieee.org/document/4587581",
"location": "-",
"rawData": "Yes",
"DOI": "10.1109/CVPR.2008.4587581"
},
{
"id": "RoadSaW",
"href": "https://www.viscoda.com/index.php/downloads/roadsaw-dataset",
"size_storage": "80",
"size_hours": "-",
"frames": "720000",
"numberOfScenes": "250",
"samplingRate": "-",
"lengthOfScenes": "",
"sensors": "camera, MARWIS",
"sensorDetail": "1x FLIR Blackfly 3.2MP, MARWIS: Mobile Advanced Road Weather Information Sensor",
"recordingPerspective": "Bird's Eye View",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "road surface wetness labels, surface types",
"licensing": "Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",
"relatedDatasets": "VLMV - Vehicle Lane Merge Visual Benchmark",
"publishDate":"2022-08-23",
"lastUpdate": "-",
"paperTitle": "RoadSaW: A Large-Scale Dataset for Camera-Based Road Surface and Wetness Estimation",
"relatedPaper": "https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Cordes_RoadSaW_A_Large-Scale_Dataset_for_Camera-Based_Road_Surface_and_Wetness_CVPRW_2022_paper.pdf",
"location": "test track near Hannover/Germany",
"rawData": "-",
"DOI": "10.1109/CVPRW56347.2022.00490"
},
{
"id": "VLMV - Vehicle Lane Merge Visual Benchmark",
"href": "https://www.viscoda.com/index.php/downloads/vlmv-benchmark",
"size_storage": "231",
"size_hours": "-",
"frames": "300000",
"numberOfScenes": "85",
"samplingRate": "25",
"lengthOfScenes": "30--60",
"sensors": "camera, gps",
"sensorDetail": "4x Blackfly 3.2MP 1824x1536, multiple GNSS RTK modules 5Hz ",
"recordingPerspective": "roadside masts (10m height)",
"dataType":"Real",
"mapData": "No",
"benchmark": "Lane merging visual and Trajectory Planning, 3d object detection, 3d object tracking",
"annotations": "ground truth 3D reference positions from GNSS modules, speed, acceleration, heading",
"licensing": "Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",
"relatedDatasets": "RoadSaW",
"publishDate":"2021-05-05",
"lastUpdate": "-",
"paperTitle": "Vehicle Lane Merge Visual Benchmark",
"relatedPaper": "https://ieeexplore.ieee.org/document/9412960",
"location": "test track near Paris/France",
"rawData": "-",
"DOI": "10.1109/ICPR48806.2021.9412960"
},
{
"id": "Daimler Pedestrian Segmentation Benchmark",
"href": "http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/daimler_pedestrian_benchmark_d.html",
"size_storage": "0.053",
"size_hours": "-",
"frames": "785",
"numberOfScenes": "785",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "Pedestrian Segmentation",
"annotations": "semantic segmentation",
"licensing": "Freely available to academic and non-academic entities for non-commercial purposes",
"relatedDatasets": "Daimler Pedestrian Path Prediction Benchmark Dataset, Daimler Stereo Pedestrian Detection Benchmark Dataset, Tsinghua-Daimler Cyclist Detection Benchmark Dataset",
"publishDate":"2013-08-133",
"lastUpdate": "-",
"paperTitle": "PedCut: an iterative framework for pedestrian segmentation combining shape models and multiple data cues",
"relatedPaper": "http://www.gavrila.net/Publications/bmvc13.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.5244/C.27.66"
},
{
"id": "Daimler Pedestrian Path Prediction Benchmark",
"href": "http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Pedestrian_Path_Predict_GCPR_1/pedestrian_path_predict_gcpr_1.html",
"size_storage": "13.05",
"size_hours": "-",
"frames": "19612",
"numberOfScenes": "68",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "Pedestrian Path Prediction",
"annotations": "2d bounding box with class labels",
"licensing": "Freely available to academic and non-academic entities for non-commercial purposes",
"relatedDatasets": "Daimler Pedestrian Segmentation Benchmark Dataset, Tsinghua-Daimler Cyclist Detection Benchmark Dataset",
"publishDate":"2013-08-25",
"lastUpdate": "-",
"paperTitle": "Pedestrian Path Prediction with Recursive Bayesian Filters: A Comparative Study",
"relatedPaper": "https://link.springer.com/chapter/10.1007/978-3-642-40602-7_18",
"location": "-",
"rawData": "-",
"DOI": "10.1007/978-3-642-40602-7_18"
},
{
"id": "Daimler Stereo Pedestrian Detection Benchmark",
"href": "http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Stereo_Ped__Detection_/daimler_stereo_ped__detection_.html",
"size_storage": "12.1",
"size_hours": "0.45",
"frames": "21790",
"numberOfScenes": "21790",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "Stereo Pedestrian Detection",
"annotations": "2d bounding box",
"licensing": "Freely available to academic and non-academic entities for non-commercial purposes",
"relatedDatasets": "Daimler Pedestrian Segmentation Benchmark Dataset, Tsinghua-Daimler Cyclist Detection Benchmark Dataset, Daimler Pedestrian Path Prediction Benchmark Dataset",
"publishDate":"2011-06-05",
"lastUpdate": "2012-11-11",
"paperTitle": "A New Benchmark for Stereo-based Pedestrian Detection",
"relatedPaper": "http://www.gavrila.net/iv11.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/IVS.2011.5940480"
},
{
"id": "Small Obstacle",
"href": "https://small-obstacle-dataset.github.io/",
"size_storage": "10.6",
"size_hours": "-",
"frames": "2927",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "1x ZED Stereo camera, 1x Velodyne Puck (VLP-16) lidar",
"recordingPerspective": "ego-perspective",
"dataType":"Real and Synthetic",
"mapData": "No",
"benchmark": "-",
"annotations": "pixel wise semantic segmentation",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2020-03-12",
"lastUpdate": "-",
"paperTitle": "LiDAR guided Small obstacle Segmentation",
"relatedPaper": "https://arxiv.org/pdf/2003.05970.pdf",
"location": "India",
"rawData": "-",
"DOI": "10.48550/arXiv.2003.05970"
},
{
"id":"GROUNDED",
"href": "https://lgprdata.com/",
"size_storage": "750",
"size_hours": "12",
"frames": "-",
"numberOfScenes": "108",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu, odometer, radar",
"sensorDetail": "1x Point Grey Grasshopper camera 1928x1448 6Hz, 1x Velodyne HDL-64 lidar 360Â° 10Hz, 1x OXTS RT3003 INS gps, 1x Localizing Ground Penetrating Radar (LGPR) Sensor components include a 12 element radar array, a switch matrix, an OXTS RTK-GPS unit 126Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "Localization in weather, Multi-lane mapping",
"annotations": "-",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2021-06-12",
"lastUpdate": "-",
"paperTitle": "GROUNDED: The Localizing Ground Penetrating Radar Evaluation Dataset",
"relatedPaper": "http://www.roboticsproceedings.org/rss17/p080.pdf",
"location": "Massachusetts, USA",
"rawData": "-",
"DOI": "10.15607/RSS.2021.XVII.080"
},
{
"id":"highD",
"href": "https://www.highd-dataset.com/",
"size_storage": "-",
"size_hours": "16.5",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "25",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x quadcopter DJI Phantom 4 Pro Plus camera 4096x2160 25Hz",
"recordingPerspective": "top-view",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "2d bounding boxes with vehicle speed and id",
"licensing": "Freely available for non-commercial use only upon registration",
"relatedDatasets": "inD, rounD, exiD and uniD datasets",
"publishDate":"2018-11-04",
"lastUpdate": "-",
"paperTitle": "The highD Dataset: A Drone Dataset of Naturalistic Vehicle Trajectories on German Highways for Validation of Highly Automated Driving Systems",
"relatedPaper": "https://arxiv.org/ftp/arxiv/papers/1810/1810.05642.pdf",
"location": "Cologne, Germany",
"rawData": "-",
"DOI": "10.1109/ITSC.2018.8569552"
},
{
"id":"inD",
"href": "https://www.ind-dataset.com/",
"size_storage": "-",
"size_hours": "10",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "25",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x quadcopter DJI Phantom 4 Pro camera 4096x2160 25Hz",
"recordingPerspective": "top-view",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "2d bounding box for objects, trajectory information",
"licensing": "Freely available for non-commercial use only upon registration",
"relatedDatasets": "highD, rounD, exiD and uniD datasets",
"publishDate":"2020-10-19",
"lastUpdate": "-",
"paperTitle": "The inD Dataset: A Drone Dataset of Naturalistic Road User Trajectories at German Intersections",
"relatedPaper": "https://arxiv.org/pdf/1911.07602.pdf",
"location": "Aachen, Germany",
"rawData": "-",
"DOI": "10.1109/IV47402.2020.9304839"
},
{
"id":"TTI Core",
"href": "https://www.toyota-ti.ac.jp/Lab/Denshi/COIN/Ontology/TTICore-0.03/",
"relatedPaper":"http://ceur-ws.org/Vol-1486/paper_9.pdf"  
},
{
"id":"rounD",
"href": "https://www.round-dataset.com/",
"size_storage": "-",
"size_hours": "6",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "25",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x DJI Phantom 4 Pro quadcopter camera 4096x2160 25Hz",
"recordingPerspective": "top-view",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "2d bounding boxes for objects, trajectory tracking information",
"licensing": "Freely available for non-commercial use only upon registration",
"relatedDatasets": "highD, inD, exiD and uniD",
"publishDate":"2020-09-20",
"lastUpdate": "-",
"paperTitle": "The rounD Dataset: A Drone Dataset of Road User Trajectories at Roundabouts in Germany",
"relatedPaper": "https://ieeexplore.ieee.org/document/9294728",
"location": "Aachen and Alsdorf, Germany",
"rawData": "-",
"DOI": "10.1109/ITSC45102.2020.9294728"
},
{
"id":"exiD",
"href": "https://www.exid-dataset.com/",
"size_storage": "-",
"size_hours": "16.1",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "25",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x DJI Matrice 200 Serie V2 equipped with a DJI Zenmuse gimbal camera 4096x2160 25Hz",
"recordingPerspective": "top-view",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "2d bounding boxes for objects, trajectory tracking information",
"licensing": "Freely available for non-commercial use only upon registration",
"relatedDatasets": "highD, inD, rounD and uniD datasets",
"publishDate":"2022-06-04",
"lastUpdate": "-",
"paperTitle": "The exiD Dataset: A Real-World Trajectory Dataset of Highly Interactive Highway Scenarios in Germany",
"relatedPaper": "https://ieeexplore.ieee.org/document/9827305",
"location": "Germany",
"rawData": "-",
"DOI": "10.1109/IV51971.2022.9827305"
},
{
"id":"Stanford Drone",
"href": "https://cvgl.stanford.edu/projects/uav_data/",
"size_storage": "69",
"size_hours": "-",
"frames": "929499",
"numberOfScenes": "8",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x Quadcopter camera 1400x1904",
"recordingPerspective": "top-view",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding boxes for objects",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License",
"relatedDatasets": "-",
"publishDate":"2016-08-01",
"lastUpdate": "-",
"paperTitle": "Learning Social Etiquette: Human Trajectory Understanding In Crowded Scenes",
"relatedPaper": "https://link.springer.com/chapter/10.1007/978-3-319-46484-8_33",
"location": "Stanford campus, USA",
"rawData": "-",
"DOI": "10.1007/978-3-319-46484-8_33"
},
{
"id":"Five Roundabouts",
"href": "http://its.acfr.usyd.edu.au/datasets-2/five-roundabouts-dataset/",
"size_storage": "0.5",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "25",
"lengthOfScenes": "-",
"sensors": "lidar",
"sensorDetail": "6x 4 beam ibeo LUX lidars 110Â° 25Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "trajectory tracking information",
"licensing": "Creative Commons Attribution 4.0",
"relatedDatasets": "naturalistic-intersection-driving-dataset",
"publishDate":"2019-05-13",
"lastUpdate": "-",
"paperTitle": "Naturalistic Driver Intention and Path Prediction Using Recurrent Neural Networks",
"relatedPaper": "https://arxiv.org/abs/1807.09995",
"location": "Sydney, Australia",
"rawData": "-",
"DOI": "10.1109/TITS.2019.2913166"
},
{
"id":"Automatum (Open Highway)",
"href": "https://automatum-data.com/#dataset",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "12",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x DJI Mavic Mini with 2K camera resolution",
"recordingPerspective": "top view",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding box",
"licensing": "Creative Commons Attribution-NonDerivative License (CC BY-ND)",
"relatedDatasets": "Highways Ramps, Highway ALKS",
"publishDate":"2021-07-11",
"lastUpdate": "-",
"paperTitle": "AUTOMATUM DATA: Drone-based highway dataset for the development and validation of automated driving software for research and commercial applications",
"relatedPaper": "https://automatumdata.blob.core.windows.net/opendataset/IV21_Automatum.Data.eng.pdf",
"location": "Germany",
"rawData": "-",
"DOI": "10.1109/IV48863.2021.9575442"
},
{
"id":"TAF-BW",
"href": "https://github.com/fzi-forschungszentrum-informatik/test-area-autonomous-driving-dataset",
"size_storage": "-",
"size_hours": "0.1",
"frames": "-",
"numberOfScenes": "2",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "traffic camera",
"recordingPerspective": "Bird's Eye",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "tracking id and bounding box",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License",
"relatedDatasets": "-",
"publishDate":"2018-12-31",
"lastUpdate": "-",
"paperTitle": "Towards Large Scale Urban Traffic Reference Data: Smart Infrastructure in the Test Area Autonomous Driving Baden-Wurttemberg",
"relatedPaper": "https://www.researchgate.net/profile/Tobias-Fleck/publication/327449884_Towards_Large_Scale_Urban_Traffic_Reference_Data_Smart_Infrastructure_in_the_Test_Area_Autonomous_Driving_Baden-Wurttemberg/links/5bbcbddd299bf1049b785126/Towards-Large-Scale-Urban-Traffic-Reference-Data-Smart-Infrastructure-in-the-Test-Area-Autonomous-Driving-Baden-Wuerttemberg.pdf",
"location": " Karlsruhe, Germany",
"rawData": "",
"DOI": "10.1007/978-3-030-01370-7_75"
},
{
"id":"K-Radar",
"href": "https://github.com/kaist-avelab/K-Radar",
"size_storage": "11748",
"size_hours": "-",
"frames": "35000",
"numberOfScenes": "58",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "4x ZED2i stereo cameras 1280x720 110Â° 30Hz, 1x os2-64 long range Lidar 360Â° 10Hz, 1x os1-128 high res. Lidar 360Â° 10Hz, 1x RETINA-4ST 4D Radar 107Â° 10Hz, 1x GPS500 C94-M8P3 RTK-GPS 1Hz, 2x IMUs (built in LiDaR) 100Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "Annotated 3d bounding boxes with doppler, range, azimuth, and elevation dimensions",
"licensing": "Attribution-NonCommercial-NoDerivatives (CC BY-NC-ND)",
"relatedDatasets": "K-Lane",
"publishDate":"2022-06-16",
"lastUpdate": "-",
"paperTitle": "K-Radar: 4D Radar Object Detection for Autonomous Driving in Various Weather Conditions",
"relatedPaper":"https://arxiv.org/abs/2206.08171",
"location": " Dae-jeon city and Gang-won province, South Korea",
"rawData": "-",
"DOI": "10.48550/arXiv.2206.08171"
},
{
"id":"Road Scene Graph",
"href": "https://github.com/TianYafu/road-status-graph-dataset"
},
{
"id":"R3 Driving",
"href": "https://github.com/rllab-snu/R3-Driving-Dataset",
"size_storage": "-",
"size_hours": "-",
"frames": "62755",
"numberOfScenes": "382",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "1x Mobileye ELD camera, 3x Velodyne VLP-16 Hi-Res lidar, 1x Inertial Lab INS D gnss/imu sensor",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "Complete dataset available on request",
"relatedDatasets": "-",
"publishDate":"2021-09-16",
"lastUpdate": "-",
"paperTitle": "Towards Defensive Autonomous Driving: Collecting and Probing Driving Demonstrations of Mixed Qualities",
"relatedPaper": "https://arxiv.org/pdf/2109.07995.pdf",
"location": "Gwanak (Seoul), Siheung, and Incheon, Korea",
"rawData": "-",
"DOI": "10.48550/arXiv.2109.07995"
},
{
"id": "EISATS",
"href": "https://ccv.wordpress.fos.auckland.ac.nz/eisats/",
"size_storage": "",
"size_hours": "",
"frames": "",
"numberOfScenes": "",
"samplingRate": "",
"lengthOfScenes": "",
"sensors": "",
"sensorDetail": "",
"benchmark": "",
"annotations": "",
"licensing": "",
"relatedDatasets": "",
"publishDate":" ",
"lastUpdate": "",
"relatedPaper": ""
},
{
"id": "Ford CAMPUS",
"href": "http://robots.engin.umich.edu/SoftwareData/Ford",
"size_storage": "197",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "1x Point Grey Ladybug3 omnidirectional camera 1600x600 8Hz, 1x Velodyne HDL-64E lidar 360Â° 10Hz, 2x Riegl LMS-Q120 lidars 80Â°, 1x Applanix POS-LV 420 INS with Trimble GPS 100Hz, 1x Xsens MTi-G IMU 100Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "SLAM, iterative closest point (ICP), and 3D object detection and recognition",
"annotations": "-",
"licensing": "Freely available for non-commercial use",
"relatedDatasets": "NCLT",
"publishDate":"2011-11-01",
"lastUpdate": "-",
"paperTitle": "Ford Campus vision and lidar data set",
"relatedPaper": "https://journals.sagepub.com/doi/pdf/10.1177/0278364911400640",
"location": "Dearborn Michigan, USA",
"rawData": "-",
"DOI": "10.1177/0278364911400640"
},
{
"id": "NCLT",
"href": "http://robots.engin.umich.edu/nclt/",
"size_storage": "2824",
"size_hours": "34.9",
"frames": "-",
"numberOfScenes": "27",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "1x Pointgrey Ladybug3 (LB3) camera system 1600x1200 5Hz, 1x Velodyne HDL-32E lidar 360Â° 10Hz, 1x Hokuyo UTM-30LX lidar 270Â°, 1x Hokuyo URG-04LX lidar 240Â°, 1x Microstrain 3DM-GX3-45 IMU 100Hz, 1x KVH DSP-3000 single-axis FOG, 1x Garmin 18x 5Hz, 1x NovAtel DL-4 plus RTK GPS 1Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "Open Data Commons Open Database License (ODbL) v1.0",
"relatedDatasets": "Ford CAMPUS",
"publishDate":"2015-12-24",
"lastUpdate": "2019-03-24",
"paperTitle": "University of Michigan North Campus Long-Term Vision and Lidar Dataset",
"relatedPaper": "http://robots.engin.umich.edu/publications/ncarlevaris-2015a.pdf",
"location": "University of Michigan North Campus, USA",
"rawData": "Yes",
"DOI": "10.1177/0278364915614638"
},
{
"id": "Argoverse 1 Stereo",
"href": "https://www.argoverse.org/data.html#stereo-link",
"size_storage": "14.2",
"size_hours": "-",
"frames": "6624",
"numberOfScenes": "74",
"samplingRate": "5",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "2x lidar 40Â° 10Hz, 7x ring cameras 1920x1200 combined 360Â° 30Hz, 2x front-view facing stereo cameras 2056x2464 5Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "3d bounding boxes",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (âCC BY-NC-SA 4.0â)",
"relatedDatasets": "Argoverse 3D Tracking, Argoverse 1 Maps, Argoverse 1 Motion Forecasting and  Argoverse 2",
"publishDate":"2019-06-15",
"lastUpdate": "-",
"paperTitle": "Argoverse: 3D Tracking and Forecasting With Rich Maps",
"relatedPaper": "https://arxiv.org/pdf/1911.02620.pdf",
"location": "Miami and Pittsburgh, USA",
"rawData": "-",
"DOI": "10.1109/CVPR.2019.00895"
},
{
"id": "uniD",
"href": "https://www.unid-dataset.com/",
"size_storage": "",
"size_hours": "",
"frames": "",
"numberOfScenes": "",
"samplingRate": "",
"lengthOfScenes": "",
"sensors": "",
"sensorDetail": "",
"benchmark": "",
"annotations": "",
"licensing": "",
"relatedDatasets": "",
"publishDate":" ",
"lastUpdate": "",
"relatedPaper": ""
},
{
"id": "USyd Campus",
"href": "http://its.acfr.usyd.edu.au/datasets/usyd-campus-dataset/",
"size_storage": "-",
"size_hours": "40",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps, imu, wheel encoders",
"sensorDetail": "6x NVIDIA 2Mega SF3322 automotive GMSL cameras 1928x1208 30Hz , 1x 3D Velodyne Puck VLP-16 10 Hz 360Â°, 1x VN-100 IMU, 1x U-Blox NEO-M8P real-time kinematics GNSS",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "Semantic Segmentation, Road class label",
"licensing": "Freely available for non-commercial purposes",
"relatedDatasets": "Five Roundabouts Dataset,  Naturalistic Intersection Driving Dataset, High Resolution Fused GPS and Dead Reckoning",
"publishDate": "2020-06-05",
"lastUpdate": "-",
"paperTitle": "Developing and Testing Robust Autonomy: The University of Sydney Campus Data Set",
"relatedPaper": "https://ieeexplore.ieee.org/document/9109704",
"location": "Sydney, Australia",
"rawData": "-",
"DOI": "10.1109/MITS.2020.2990183"
},
{
"id": "SemKITTI-DVPS",
"href": "https://github.com/joe-siyuan-qiao/ViP-DeepLab",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "Images taken from SemanticKITTI dataset and modified with annotations",
"recordingPerspective": "ego-perspetive",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "panoptic segmentation, monocular depth",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike",
"relatedDatasets": "SemanticKITTI",
"publishDate":"2021-06-20",
"lastUpdate": "-",
"paperTitle": "ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation",
"relatedPaper": "https://arxiv.org/pdf/2012.05258",
"location": "Karlsruhe, Germany",
"rawData": "Yes",
"DOI": "10.1109/CVPR46437.2021.00399"
},
{
"id": "Cityscapes-DVPS",
"href": "https://github.com/joe-siyuan-qiao/ViP-DeepLab",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "Images taken from Cityscapes dataset and modified with annotations",
"recordingPerspective": "ego-perspetive",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "panoptic segmentation, monocular depth",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike",
"relatedDatasets": "Cityscapes",
"publishDate":"2021-06-20",
"lastUpdate": "-",
"paperTitle": "ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation",
"relatedPaper": "https://arxiv.org/pdf/2012.05258",
"location": "-",
"rawData": "Yes",
"DOI": "10.1109/CVPR46437.2021.00399"
},
{
"id": "IDDA",
"href": "https://idda-dataset.github.io/home/",
"size_storage": "1000",
"size_hours": "-",
"frames": "1006800",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, depth sensor, semantic segmentation sensor",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Synthetic",
"mapData": "-",
"benchmark": "semantic segmentation",
"annotations": "semantic segmentation",
"licensing": "Available upon registration",
"relatedDatasets": "-",
"publishDate":"2020-07-14",
"lastUpdate": "2022-05-10",
"paperTitle": "IDDA: A Large-Scale Multi-Domain Dataset for Autonomous Driving",
"relatedPaper": "https://arxiv.org/pdf/2004.08298.pdf",
"location": "CARLA",
"rawData": "Yes",
"DOI": "10.1109/LRA.2020.3009075"
},
{
"id": "UTBM EU LTD",
"href": "https://epan-utbm.github.io/utbm_robocar_dataset/",
"size_storage": "-",
"size_hours": "2.93",
"frames": "-",
"numberOfScenes": "13",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, radar, gps/imu",
"sensorDetail": "2x stereo cameras a front-facing Bumblebee XB3 and a back-facing Bumblebee2, 2x Velodyne HDL-32E lidars 360Â° 10Hz, 2x Pixelink PL-B742F industrial cameras 185Â°, 1x ibeo LUX 4L lidar 85Â°, 1x Continental ARS 308 radar, 1x SICK LMS100-10000 laser rangefinder 270Â° 12.5Hz, 1x Magellan ProFlex 500 GNSS, 1x Xsens MTi-28A53G25 IMU 100Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "lidar odometry",
"annotations": "-",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License",
"relatedDatasets": "-",
"publishDate": "2020-08-01",
"lastUpdate": "2022-06-29",
"paperTitle": "EU Long-term Dataset with Multiple Sensors for Autonomous Driving",
"relatedPaper": "https://arxiv.org/pdf/1909.03330.pdf",
"location": "MontbÃ©liard, France",
"rawData": "Yes",
"DOI": "10.1109/IROS45743.2020.9341406"
},
{
"id": "MOTSynth",
"href": "https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=42",
"size_storage": "-",
"size_hours": "16",
"frames": "1375200",
"numberOfScenes": "764",
"samplingRate": "20",
"lengthOfScenes": "90",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Synthetic",
"mapData": "No",
"benchmark": "-",
"annotations": "bounding boxes and pose, segmentation masks, depth maps",
"licensing": "Free to use",
"relatedDatasets": "-",
"publishDate":"2021-10-10",
"lastUpdate": "-",
"paperTitle": "MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?",
"relatedPaper": "https://arxiv.org/pdf/2108.09518.pdf",
"location": "Grand Theft Auto V game",
"rawData": "-",
"DOI": "10.1109/ICCV48922.2021.01067"
},
{
"id": "Argoverse 2 Sensor",
"href": "https://www.argoverse.org/av2.html",
"size_storage": "1000",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "1000",
"samplingRate": "10",
"lengthOfScenes": "15",
"sensors": "camera, lidar, gps",
"sensorDetail": "7x high-resolution ring cameras 2048x1550 combined 360Â° 20Hz, 2x front-view facing stereo cameras 20Hz, 2x VLP-32C lidar sensors 360Â° 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "3d cuboids/bounding boxes around objects",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (âCC BY-NC-SA 4.0â)",
"relatedDatasets": "Argoverse 1 and Argoverse 2 series of datasets",
"publishDate":"2021-08-21",
"lastUpdate": "-",
"paperTitle": "Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting",
"relatedPaper": "https://arxiv.org/abs/2301.00493",
"location": "Austin, Detroit, Miami, Palo Alto, Pittsburgh, and Washington D.C, USA",
"rawData": "Yes",
"DOI": "-"
},
{
"id": "Argoverse 2 Motion Forecasting",
"href": "https://www.argoverse.org/av2.html",
"size_storage": "58",
"size_hours": "763",
"frames": "-",
"numberOfScenes": "250000",
"samplingRate": "10",
"lengthOfScenes": "11",
"sensors": "camera, lidar, gps",
"sensorDetail": "7x high-resolution ring cameras 2048x1550 combined 360Â° 20Hz, 2x front-view facing stereo cameras 20Hz, 2x VLP-32C lidar sensors 360Â° 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "trajectory tracking information",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (âCC BY-NC-SA 4.0â)",
"relatedDatasets": "Argoverse 1 and Argoverse 2 series of datasets",
"publishDate":"2021-08-21",
"lastUpdate": "-",
"paperTitle": "Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting",
"relatedPaper": "https://arxiv.org/abs/2301.00493",
"location": "Pittsburgh and Miami, USA",
"rawData": "Yes",
"DOI": "-"
},
{
"id": "Argoverse 2 Lidar",
"href": "https://www.argoverse.org/av2.html",
"size_storage": "5000",
"size_hours": "-",
"frames": "6000000",
"numberOfScenes": "20000",
"samplingRate": "10",
"lengthOfScenes": "30",
"sensors": "camera, lidar, gps",
"sensorDetail": "7x high-resolution ring cameras 2048x1550 combined 360Â° 20Hz, 2x front-view facing stereo cameras 20Hz, 2x VLP-32C lidar sensors 360Â° 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "-",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (âCC BY-NC-SA 4.0â)",
"relatedDatasets": "Argoverse 1 and Argoverse 2 series of datasets",
"publishDate":"2021-08-21",
"lastUpdate": "-",
"paperTitle": "Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting",
"relatedPaper": "https://arxiv.org/abs/2301.00493",
"location": "Austin, Detroit, Miami, Palo Alto, Pittsburgh, and Washington D.C, USA",
"rawData": "Yes",
"DOI": "-"
},
{
"id": "Argoverse 2 Map Change",
"href": "https://www.argoverse.org/av2.html",
"size_storage": "1000",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "1000",
"samplingRate": "-",
"lengthOfScenes": "45",
"sensors": "camera, lidar, gps",
"sensorDetail": "7x high-resolution ring cameras 2048x1550 combined 360Â° 20Hz, 2x front-view facing stereo cameras 20Hz, 2x VLP-32C lidar sensors 360Â° 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "3D lane boundaries",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (âCC BY-NC-SA 4.0â)",
"relatedDatasets": "Argoverse 1 and Argoverse 2 series of datasets",
"publishDate":"2021-08-21",
"lastUpdate": "-",
"paperTitle": "Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting",
"relatedPaper": "https://arxiv.org/abs/2301.00493",
"location": "Austin, Detroit, Miami, Palo Alto, Pittsburgh and Washington, D.C, USA",
"rawData": "Yes",
"DOI": "-"
},
{
  "id": "CODA",
  "href": "https://coda-dataset.github.io/",
  "size_storage": "-",
  "size_hours": "-",
  "frames": "-",
  "numberOfScenes": "10000",
  "samplingRate": "-",
  "lengthOfScenes": "-",
  "sensors": "camera",
  "sensorDetail": "The dataset has been constructed from KITTI, nuScenes, SODA10M and ONCE datasets",
  "recordingPerspective": "ego-perspective",
  "dataType":"Real",
  "mapData": "No",
  "benchmark": "anomaly detection, object detection",
  "annotations": "Image domain tags (i.e., periods and weather conditions) and 2D bounding boxes",
  "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (âCC BY-NC-SA 4.0â)",
  "relatedDatasets": "CODA2022",
  "publishDate":"2021-11-24",
  "lastUpdate": "2023-01-15",
  "paperTitle": "CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving",
  "relatedPaper": "https://arxiv.org/pdf/2203.07724.pdf",
  "location": "-",
  "rawData": "-",
  "DOI": "10.48550/arXiv.2203.07724"
},
 {
   "id":"CODA2022",
   "href": "https://coda-dataset.github.io/",
   "relatedDatasets": "CODA"
 },
{
"id": "Synthetic Discrepancy",
"href": "https://github.com/cvlab-epfl/detecting-the-unexpected",
"size_storage": "0.2",
"size_hours": "-",
"frames": "60",
"numberOfScenes": "60",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "Original images gathered online and synthetic discrpencies inserted 1280x720",
"recordingPerspective": "ego-perspective",
"dataType":"Synthetic",
"mapData": "No",
"benchmark": "-",
"annotations": "bounding box and semantic segmentation",
"licensing": "Free to use",
"relatedDatasets": "-",
"publishDate":"2020-10-27",
"lastUpdate": "-",
"paperTitle": "Detecting the Unexpected via Image Resynthesis",
"relatedPaper": "https://arxiv.org/abs/1904.07595",
"location": "-",
"rawData": "-",
"DOI": "10.1109/ICCV.2019.00224"
},
{
"id": "SODA10M",
"href": "https://soda-2d.github.io/",
"size_storage": "2005.6",
"size_hours": "27833",
"frames": "10000000",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "Mobile cameras with recording applications 1920x1080 resolution",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "self-supervised learning for dataset labelling",
"annotations": "Image tags (i.e., weather conditions, location scenes, periods) for all images and 2D bounding boxes",
"licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (âCC BY-NC-SA 4.0â)",
"relatedDatasets": "-",
"publishDate":"2021-06-08",
"lastUpdate": "2021-11-09",
"paperTitle": "SODA10M: A Large-Scale 2D Self/Semi-Supervised Object Detection Dataset for Autonomous Driving",
"relatedPaper": "https://arxiv.org/pdf/2106.11118.pdf",
"location": "32 cities across China",
"rawData": "Yes",
"DOI": "10.48550/arXiv.2106.11118"
},
{
"id": "A9",
"href": "https://innovation-mobility.com/en/a9-dataset/",
"size_storage": "17",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "4x Basler ace acA1920-50gc traffic camera 1920x1200, 1x  Ouster OS1-64 360Â°",
"recordingPerspective": "Bird's Eye",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "",
"annotations": "3d bounding boxes",
"licensing": "Available upon registration",
"relatedDatasets": "-",
"publishDate":"2022-06-04",
"lastUpdate": "-",
"paperTitle": "A9-Dataset: Multi-Sensor Infrastructure-Based Dataset for Mobility Research",
"relatedPaper": "https://arxiv.org/pdf/2204.06527.pdf",
"location": "Providentia++ test field near Munich, Germany",
"rawData": "-",
"DOI": "10.1109/IV51971.2022.9827401"
},
{
"id": "MuIRan",
"href": "https://sites.google.com/view/mulran-pr/dataset",
"size_storage": "387",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "lidar, radar, gps/imu",
"sensorDetail": "1x Ouster OS1-64 64 channel lidar 360Â° 10Hz, 1x Navtech CIR204-H radar 360Â° 4Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "trajectory information",
"licensing": "Available upon registration",
"relatedDatasets": "-",
"publishDate":"2019-05-24",
"lastUpdate": "2021-05-31",
"paperTitle": "MulRan: Multimodal Range Dataset for Urban Place Recognition",
"relatedPaper": "https://ieeexplore.ieee.org/document/9197298",
"location": "South Korea",
"rawData": "Yes",
"DOI": "10.1109/ICRA40945.2020.9197298"
},
{
"id": "Rope3D",
"href": "https://thudair.baai.ac.cn/rope",
"size_storage": "-",
"size_hours": "-",
"frames": "50009",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "1x HESAI Pandar 40P 40 laser beams lidar 360Â° 10/20Hz, 1x Jaguar Prime from Innovusion 300 beams lidar 6-20Hz, RGB cameras 1920x1080 30-60Hz",
"recordingPerspective": "ego-perspective, bird's eye",
"dataType":"Real",
"mapData": "-",
"benchmark": "-",
"annotations": "2D and 3D bounding boxes of the obstacle objects as well as their category attributes, occlusions, and truncated states",
"licensing": "Available upon registration",
"relatedDatasets": "DAIR-V2X",
"publishDate":"2022-03-25",
"lastUpdate": "-",
"paperTitle": "Rope3D: The Roadside Perception Dataset for Autonomous Driving and Monocular 3D Object Detection",
"relatedPaper": "https://arxiv.org/pdf/2203.13608.pdf",
"location": "China",
"rawData": "-",
"DOI": "10.48550/arXiv.2203.13608"
},
{
  "id": "AugKITTI",
  "relatedPaper": "https://arxiv.org/pdf/2203.00214.pdf",
  "paperTitle":"Understanding the Challenges When 3D Semantic Segmentation Faces Class Imbalanced and OOD Data"
},
{
"id": "MAVD Multimodal Audio-Visual Detection",
"href": "http://multimodal-distill.cs.uni-freiburg.de/",
"size_storage": "-",
"size_hours": "-",
"frames": "113000",
"numberOfScenes": "24",
"samplingRate": "-",
"lengthOfScenes": "900",
"sensors": "camera, lidar, microphone, gps/imu",
"sensorDetail": "2x FLIR Blackfly 23S3C RGB camera 1920x650, 2x FLIR ADK cameras for thermal images 1920x650, 8x monophonic microphones",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding boxes",
"licensing": "Freely available for non commercial use only",
"relatedDatasets": "-",
"publishDate":"2021-04-16",
"lastUpdate": "-",
"paperTitle": "There is More than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking with Sound by Distilling Multimodal Knowledge",
"relatedPaper": "https://arxiv.org/pdf/2103.01353.pdf",
"location": "Freiburg im Breisgau and nearby towns, Germany",
"rawData": "-",
"DOI": "10.1109/CVPR46437.2021.01144"
},
{
"id": "KITTI-360-APS",
"href": "http://amodal-panoptic.cs.uni-freiburg.de/",
"size_storage": "0.5",
"size_hours": "-",
"frames": "61168",
"numberOfScenes": "9",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "Images taken from KITTI dataset and introduce amodal segmentation, image resolution of 1408x376",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "Amodal panoptic segmentation",
"annotations": "Amodal panoptic segmentation",
"licensing": "Freely available for non-commercial purposes",
"relatedDatasets": "BDD100K-APS",
"publishDate":"2022-02-23",
"lastUpdate": "-",
"paperTitle": "Amodal Panoptic Segmentation",
"relatedPaper": "https://arxiv.org/abs/2202.11542",
"location": "Karlsruhe, Germany",
"rawData": "",
"DOI": "10.48550/arXiv.2202.11542"
},
{
"id": "BDD100K-APS",
"href": "http://amodal-panoptic.cs.uni-freiburg.de/",
"size_storage": "-",
"size_hours": "-",
"frames": "3030",
"numberOfScenes": "15",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "Images taken from BDD100K dataset and introduce amodal segmentation, image resolution of 1408x376",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "Amodal panoptic segmentation",
"annotations": "Amodal panoptic segmentation",
"licensing": "Freely available for non-commercial purposes",
"relatedDatasets": "KITTI-360-APS",
"publishDate":"2022-02-23",
"lastUpdate": "-",
"paperTitle": "Amodal Panoptic Segmentation",
"relatedPaper": "https://arxiv.org/abs/2202.11542",
"location": "New York, Berkeley, San Francisco and Bay Area, USA",
"rawData": "-",
"DOI": "10.48550/arXiv.2202.11542"
},
{
  "id": "Boreas",
  "href": "https://www.boreas.utias.utoronto.ca/#/",
  "relatedPaper": "https://arxiv.org/pdf/2203.10168.pdf",
  "paperTitle": "Boreas: A Multi-Season Autonomous Driving Dataset"
},
{
"id": "DGL-MOTS",
"href": "https://goodproj13.github.io/DGL-MOTS/",
"size_storage": "-",
"size_hours": "-",
"frames": "12000",
"numberOfScenes": "40",
"samplingRate": "17",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x GoPro HERO8 17Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "pixel-level mask and temporal tracking label across frames for objects ",
"licensing": "-",
"relatedDatasets": "-",
"publishDate":"2019-06-15",
"lastUpdate": "-",
"paperTitle": "MOTS: Multi-Object Tracking and Segmentation",
"relatedPaper": "https://arxiv.org/pdf/2110.07790.pdf",
"location": "USA",
"rawData": "Yes",
"DOI": "10.1109/CVPR.2019.00813"
},
{
"id": "OpenMPD",
"href": "http://openmpd.com/",
"size_storage": "-",
"size_hours": "-",
"frames": "15000",
"numberOfScenes": "180",
"samplingRate": "-",
"lengthOfScenes": "20",
"sensors": "camera, lidar",
"sensorDetail": "6x cameras 1024x968 20Hz, 4x lidars combined 360Â° 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d/3d bounding boxes, 2d/3d semantic segmentation",
"licensing": "Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",
"relatedDatasets": "IPS300+",
"publishDate":"2022-01-14",
"lastUpdate": "-",
"paperTitle": "OpenMPD: An Open Multimodal Perception Dataset for Autonomous Driving",
"relatedPaper": "https://ieeexplore.ieee.org/document/9682587",
"location": "Beijing, China",
"rawData": "Yes",
"DOI": "10.1109/TVT.2022.3143173"
},
{
"id": "IPS300+",
"href": "http://openmpd.com/column/IPS300",
"size_storage": "-",
"size_hours": "-",
"frames": "14198",
"numberOfScenes": "180",
"samplingRate": "-",
"lengthOfScenes": "20",
"sensors": "camera, lidar, gps",
"sensorDetail": "2x Sensing-SG5 color cameras Sony IMX490RGGB 1920x1080 30Hz, 1x Robosense Ruby-Lite Lidar 360Â° 10Hz, 1x BS-280 GPS",
"recordingPerspective": "Bird's Eye",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "3d bounding boxes",
"licensing": "Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",
"relatedDatasets": "OpenMPD",
"publishDate":"2021-06-05",
"lastUpdate": "-",
"paperTitle": "IPS300+: a Challenging multi-modal data sets for Intersection Perception System",
"relatedPaper": "https://arxiv.org/abs/2106.02781",
"location": "China",
"rawData": "Yes",
"DOI": "10.1109/icra46639.2022.9811699"
},
{
  "id": "LISA Amazon-MLSL Vehicle Attributes (LAVA)",
  "relatedPaper": "https://arxiv.org/pdf/2112.00942.pdf"
},
{
"id": "MONA: The Munich Motion Dataset of Natural Driving",
"href":"https://commonroad.in.tum.de/",
"size_storage": "4300",
"size_hours": "130",
"frames": "-",
"numberOfScenes": "260",
"samplingRate": "25",
"lengthOfScenes": "1800",
"sensors": "camera",
"sensorDetail": "3x Sony FDR-AX43 3840x2160 25Hz",
"recordingPerspective": "Bird's Eye View",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "trajectory information",
"licensing": "Available for non-commercial and commercial use.",
"relatedDatasets": "-",
"publishDate":"2022-10-08",
"lastUpdate": "-",
"paperTitle": "MONA: The Munich Motion Dataset of Natural Driving",
"relatedPaper": "https://ieeexplore.ieee.org/document/9922263",
"location": "Munich, Germany",
"rawData": "Yes",
"DOI": "10.1109/ITSC55140.2022.9922263"
},
{
"id": "Street Obstacle Sequences (SOS)",
"href": "https://zenodo.org/record/7144906#.Y074l3ZBxD8",
"size_storage": "3.3",
"size_hours": "-",
"frames": "1129",
"numberOfScenes": "20",
"samplingRate": "25",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic segmentation of out of distribution object label",
"licensing": "Creative Commons Attribution 4.0 International",
"relatedDatasets": "CARLA-WildLife (CWL) and Wuppertal Obstacle Sequences (WOS)",
"publishDate":"2022-10-05",
"lastUpdate": "-",
"paperTitle": "Two Video Data Sets for Tracking and Retrieval of Out of Distribution Objects",
"relatedPaper": "https://arxiv.org/pdf/2210.02074.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.48550/arXiv.2210.02074"
},
{
"id": "Wuppertal Obstacle Sequences (WOS)",
"href": "https://zenodo.org/record/7148534#.Y08MPnZBxD8",
"size_storage": "24.3",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "44",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic segmentation of out of distribution object labels",
"licensing": "Creative Commons Attribution 4.0 International",
"relatedDatasets": "Street Obstacle Sequences (SOS) and CARLA-WildLife (CWL)",
"publishDate":"2022-10-05",
"lastUpdate": "-",
"paperTitle": "Two Video Data Sets for Tracking and Retrieval of Out of Distribution Objects",
"relatedPaper": "https://arxiv.org/pdf/2210.02074.pdf",
"location": "-",
"rawData": "Yes",
"DOI": "10.48550/arXiv.2210.02074"
},
{
"id": "CARLA-WildLife (CWL)",
"href": "https://zenodo.org/record/7144840#.Y08KbHZBxD8",
"size_storage": "4.1",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "26",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Synthetic",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic segmentation of out of distribution object labels",
"licensing": "Creative Commons Attribution 4.0 International",
"relatedDatasets": "Street Obstacle Sequences (SOS) and Wuppertal Obstacle Sequences (WOS)",
"publishDate":"2022-10-05",
"lastUpdate": "-",
"paperTitle": "Two Video Data Sets for Tracking and Retrieval of Out of Distribution Objects",
"relatedPaper": "https://arxiv.org/pdf/2210.02074.pdf",
"location": "CARLA",
"rawData": "-",
"DOI": "10.48550/arXiv.2210.02074"
},  
{
"id": "R-U-MAAD",
"href":"https://github.com/againerju/r_u_maad",
"size_storage": "",
"size_hours": "",
"frames": "",
"numberOfScenes": "160",
"samplingRate": "10",
"lengthOfScenes": "",
"sensors": "",
"sensorDetail": "Data taken from Argoverse dataset and anomalies introduced",
"recordingPerspective": "Bird's Eye View",
"dataType":"",
"mapData": "",
"benchmark": "Unsupervised Anomaly Detection in Multi-Agent Trajectories",
"annotations": "",
"licensing": "",
"relatedDatasets": "",
"publishDate":"2022-09-05",
"lastUpdate": "-",
"paperTitle": "A Benchmark for Unsupervised Anomaly Detection in Multi-Agent Trajectories",
"relatedPaper": "https://arxiv.org/pdf/2209.01838.pdf",
"location": "",
"rawData": "",
"DOI": "10.48550/arXiv.2209.01838"
}, 
{
  "id": "Autonomous Platform Inertial",
  "href": "https://github.com/ansfl/Navigation-Data-Project/",
  "relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9684368",
  "paperTitle":"The Autonomous Platforms Inertial Dataset",
  "DOI": "10.1109/ACCESS.2022.3144076"
},
{
"id": "PIE",
"href": "https://data.nvision2.eecs.yorku.ca/PIE_dataset/",
"size_storage": "-",
"size_hours": "6",
"frames": "909480",
"numberOfScenes": "53",
"samplingRate": "30",
"lengthOfScenes": "600",
"sensors": "camera, gps",
"sensorDetail": "1x monocular dashboard camera Waylens Horizon 1920x1080 157Â° 30Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding boxes for pedestrians with occlusion flages, text labels for pedestrian actions, crossing intention confidence scores, gps data and vehicle informatiom for every frame",
"licensing": "MIT License",
"relatedDatasets": "JAAD Dataset",
"publishDate":"201-10-27",
"lastUpdate": "-",
"paperTitle": "PIE: A Large-Scale Dataset and Models for Pedestrian Intention Estimation and Trajectory Prediction",
"relatedPaper": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Rasouli_PIE_A_Large-Scale_Dataset_and_Models_for_Pedestrian_Intention_Estimation_ICCV_2019_paper.pdf",
"location": "Toronto, Canada",
"rawData": "Yes",
"DOI": "10.1109/ICCV.2019.00636"
},
{
"id": "HAD",
"href": "https://usa.honda-ri.com/had",
"size_storage": "-",
"size_hours": "32",
"frames": "-",
"numberOfScenes": "5675",
"samplingRate": "10",
"lengthOfScenes": "20",
"sensors": "camera",
"sensorDetail": "Videos obtained from the larger HDD dataset from the same research group",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "Human voice describing action and attention of the driver seperately",
"licensing": "Available for research for people affiliated with a university upon registration",
"relatedDatasets": "HDBD, HDD, HEV-I, HSD, LOKI, TITAN and EPOSH",
"publishDate":"2019-06-15",
"lastUpdate": "-",
"paperTitle": "Grounding Human-To-Vehicle Advice for Self-Driving Vehicles",
"relatedPaper": "https://arxiv.org/abs/1911.06978",
"location": "San Francisco Bay Area, USA",
"rawData": "-",
"DOI": "10.1109/CVPR.2019.01084"
},
{
"id": "HDBD",
"href": "https://usa.honda-ri.com/hdbd",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera, eye tracker",
"sensorDetail": "-",
"recordingPerspective": "ego-perspective",
"dataType":"Synthetic",
"mapData": "No",
"benchmark": "-",
"annotations": "semantic segmentation of images, driver gaze map, vehice speed, throtle and steering data",
"licensing": "Available for research for people affiliated with a university upon registration",
"relatedDatasets": "HAD, HDD, HEV-I, HSD, LOKI, TITAN and EPOSH",
"publishDate":"2022-05-23",
"lastUpdate": "-",
"paperTitle": "Incorporating Gaze Behavior Using Joint Embedding With Scene Context for Driver Takeover Detection",
"relatedPaper": "https://ieeexplore.ieee.org/document/9747779",
"location": "Unreal Engine",
"rawData": "-",
"DOI": "10.1109/ICASSP43922.2022.9747779"
},
{
"id": "HDD",
"href": "https://usa.honda-ri.com/hdd",
"size_storage": "150",
"size_hours": "104",
"frames": "-",
"numberOfScenes": "-",
"samplingRate": "30",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu, can",
"sensorDetail": "3x Point Grey Grasshopper 3 video cameras 1920x1200 30Hz, 1x Velodyne HDL-64E S2 3D LiDAR 360Â° 10Hz, 1 x GeneSys Eletronik GmbH Automotive Dynamic Motion Analyzer with DGPS 120Hz, 1x Vehicle Controller Area Network 100Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "Yes",
"benchmark": "-",
"annotations": "driver behaviour label",
"licensing": "Available for research for people affiliated with a university upon registration",
"relatedDatasets": "HDBD, HAD, HEV-I, HSD, LOKI, TITAN and EPOSH",
"publishDate":"2018-06-18",
"lastUpdate": "-",
"paperTitle": "Toward Driving Scene Understanding: A Dataset for Learning Driver Behavior and Causal Reasoning",
"relatedPaper": "https://arxiv.org/abs/1811.02307",
"location": "San Francisco Bay Area, USA",
"rawData": "-",
"DOI": "10.1109/CVPR.2018.00803"
},
{
"id": "HEV-I",
"href": "https://usa.honda-ri.com/hevi",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "230",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x RGB camera with 1920x1200 10Hz",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "location and tracking information of objects for predicting future ego-motion",
"licensing": "Available for research for people affiliated with a university upon registration",
"relatedDatasets": "HDBD, HAD, HDD, HSD, LOKI, TITAN and EPOSH",
"publishDate":"2019-05-20",
"lastUpdate": "-",
"paperTitle": "Egocentric Vision-based Future Vehicle Localization for Intelligent Driving Assistance Systems",
"relatedPaper": "https://arxiv.org/pdf/1809.07408.pdf",
"location": "San Francisco Bay Area, USA",
"rawData": "-",
"DOI": "10.1109/ICRA.2019.8794474"
},
{
"id": "HSD",
"href": "https://usa.honda-ri.com/hsd",
"size_storage": "60",
"size_hours": "80",
"frames": "-",
"numberOfScenes": "20000",
"samplingRate": "30",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "2x Pointgrey Grasshopper (80FOV), 1x Grasshopper (100FOV)",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "road places, road types, weather, and road surface conditions",
"licensing": "Available for research for people affiliated with a university upon registration",
"relatedDatasets": "HDBD, HAD, HDD, HEV-I, LOKI, TITAN and EPOSH",
"publishDate":"2019-05-01",
"lastUpdate": "-",
"paperTitle": "Dynamic Traffic Scene Classification with Space-Time Coherence",
"relatedPaper": "https://arxiv.org/pdf/1905.12708.pdf",
"location": "San Francisco Bay area, USA",
"rawData": "Yes",
"DOI": "10.1109/ICRA.2019.8794137"
},
{
"id": "LOKI",
"href": "https://usa.honda-ri.com/loki",
"size_storage": "-",
"size_hours": "-",
"frames": "-",
"numberOfScenes": "644",
"samplingRate": "5",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps",
"sensorDetail": "1x color SEKONIX SF332X-10X video camera 1928x1280 30Hz, 4x Velodyne VLP-32C 3D LiDARs 10Hz, 1x MTi-G-710-GNSS/INS-2A8G4 gyroscope/gps",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "driver intention and vehicle trajectory prediction",
"annotations": "2d/3d bounding boxes, driver intention labels, environmental labels, contextual labels",
"licensing": "Available for research for people affiliated with a university upon registration",
"relatedDatasets": "HDBD, HAD, HDD, HEV-I, HSD, TITAN and EPOSH",
"publishDate":"2021-10-10",
"lastUpdate": "-",
"paperTitle": "LOKI: Long Term and Key Intentions for Trajectory Prediction",
"relatedPaper": "https://arxiv.org/pdf/2108.08236.pdf",
"location": "Tokyo, Japan",
"rawData": "-",
"DOI": "10.1109/ICCV48922.2021.00966"
},
{
"id": "TITAN",
"href": "https://usa.honda-ri.com/titan",
"size_storage": "-",
"size_hours": "-",
"frames": "75262",
"numberOfScenes": "700",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "1x GoPro Hero 7 camera",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d bounding boxes for vehicles and pedestrians, pedestrian/vehicles action attributes and trajectory information",
"licensing": "Available for research for people affiliated with a university upon registration",
"relatedDatasets": "HDBD, HAD, HDD, HEV-I, HSD, LOKI and EPOSH",
"publishDate":"2020-06-13",
"lastUpdate": "-",
"paperTitle": "TITAN: Future Forecast Using Action Priors",
"relatedPaper": "https://arxiv.org/pdf/2003.13886.pdf",
"location": "Tokyo, Japan",
"rawData": "-",
"DOI": "10.1109/CVPR42600.2020.01120"
},
{
"id": "EPOSH",
"href": "https://usa.honda-ri.com/eposh",
"size_storage": "",
"size_hours": "",
"frames": "",
"numberOfScenes": "560",
"samplingRate": "",
"lengthOfScenes": "",
"sensors": "camera",
"sensorDetail": "GoPro Hero 7 camera",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "",
"benchmark": "",
"annotations": "",
"licensing": "Dataset and paper yet to be made available",
"relatedDatasets": "",
"publishDate":"",
"lastUpdate": "",
"paperTitle": "",
"relatedPaper": "",
"location": "San Francisco Bay Area, USA",
"rawData": "",
"DOI": ""
},
{
"id": "SeeingThroughFog",
"href": "https://github.com/princeton-computational-imaging/SeeingThroughFog",
"size_storage": "-",
"size_hours": "-",
"frames": "1400000",
"numberOfScenes": "-",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera, radar, lidar, imu, weather sensor",
"sensorDetail": "2x stereo cameras 1920x1024 30Hz, 1 gated camera 1280x720 120Hz, 1 FMCW radar 15Hz, 2x Velodyne lidars 10Hz, 1 FIR camera 640x480 30Hz, 1 Airmar WX150 weather sensor (temperature, wind speed and humidity)",
"recordingPerspective": "Ego-perspective",
"dataType":"real",
"mapData":"No",
"benchmark": "-",
"annotations": "weather of scenes in frames",
"licensing": "Freely available for research and teaching purposes",
"relatedDatasets": "Gated2Gated, Gated2Depth, PointCloudDeNoising",
"publishDate":"2019-02-24",
"lastUpdate": "-",
"paperTitle": "Seeing Through FogWithout Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather",
"relatedPaper": "https://arxiv.org/abs/1902.08913",
"location": "Germany, Sweden, Denmark and Finland",
"rawData": "-",
"DOI": "10.1109/CVPR42600.2020.01170"
},
{
"id": "Gated2Gated",
"href": "https://github.com/princeton-computational-imaging/Gated2Gated#gated2gated--self-supervised-depth-estimation-from-gated-images",
"size_storage": "-",
"size_hours": "-",
"frames": "130000",
"numberOfScenes": "1835",
"samplingRate": "10",
"lengthOfScenes": "-",
"sensors": "camera, radar, lidar, imu, weather sensor",
"sensorDetail": "2x stereo cameras 1920x1024 30Hz, 1 gated camera 1280x720 120Hz, 1 FMCW radar 15Hz, 2x Velodyne lidars 10Hz, 1 FIR camera 640x480 30Hz, 1 Airmar WX150 weather sensor (temperature, wind speed and humidity)",
"recordingPerspective": "Ego-perspective",
"dataType":"real",
"mapData":"No",
"benchmark": "-",
"annotations": "-",
"licensing": "Freely available for research and teaching purposes",
"relatedDatasets": "SeeingThroughFog, PointCloudDeNoising, Gated2Depth",
"publishDate":"2021-12-04",
"lastUpdate": "-",
"paperTitle": "Gated2Gated: Self-Supervised Depth Estimation from Gated Images",
"relatedPaper": "https://arxiv.org/pdf/2112.02416.pdf",
"location": "Germany, Sweden, Denmark and Finland",
"rawData": "-",
"DOI": "10.48550/arXiv.2112.02416"
},
{
"id": "Gated2Depth",
"href": "https://github.com/gruberto/Gated2Depth",
"size_storage": "-",
"size_hours": "-",
"frames": "17686",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar",
"sensorDetail": "1x Aptina AR0230 stereo camera 1920x1080 30Hz, 1x Velodyne HDL64-S3 lidar, 1x gated camera 10bit images 1280x720 120Hz",
"recordingPerspective": "Ego-perspective",
"dataType":"Real",
"mapData":"No",
"benchmark": "-",
"annotations": "-",
"licensing": "Freely available for research and teaching purposes",
"relatedDatasets": "SeeingThroughFog, PointCloudDeNoising, Gated2Gated",
"publishDate":"2020-02-13",
"lastUpdate": "2020-04-12",
"paperTitle": "Gated2Depth: Real-Time Dense Lidar From Gated Images",
"relatedPaper": "https://arxiv.org/pdf/1902.04997.pdf",
"location": "Germany, Denmark and Sweden",
"rawData": "-",
"DOI": "10.1109/ICCV.2019.00159"
},
{
"id": "RUGD: Robot Unstructured Ground Driving",
"href": "http://rugd.vision/",
"size_storage": "5.4",
"size_hours": "-",
"frames": "37000",
"numberOfScenes": "-",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "1x Prosilica GT2750C camera 1376x1110, 1x Velodyne HDL-32 LiDAR, 1x Garmin GPS receiver, 1x Microstrain GX3-25 IMU",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "semantic segmentation",
"annotations": "",
"licensing": "Freely available for research purposes",
"relatedDatasets": "-",
"publishDate":"2019-11-08",
"lastUpdate": "-",
"paperTitle": "A RUGD Dataset for Autonomous Navigation and Visual Perception in Unstructured Outdoor Environments",
"relatedPaper": "http://rugd.vision/pdfs/RUGD_IROS2019.pdf",
"location": "-",
"rawData": "-",
"DOI": "10.1109/IROS40897.2019.8968283"
},
{
"id": "CURE-TSR",
"href": "https://github.com/olivesgatech/CURE-TSR",
"size_storage": "-",
"size_hours": "-",
"frames": "2206106",
"numberOfScenes": "2206106",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "Images containg traffic signs cropped from CURE-TSD dataset",
"recordingPerspective": "ego-perspective",
"dataType":"Real and Synthetic",
"mapData": "No",
"benchmark": "Traffic Sign Detection",
"annotations": "object class labels",
"licensing": "Available upon registration",
"relatedDatasets": "CURE-TSD",
"publishDate":"2017-12-07",
"lastUpdate": "-",
"paperTitle": "CURE-TSR: Challenging Unreal and Real Environments for Traffic Sign Recognition",
"relatedPaper": "https://arxiv.org/pdf/1712.02463.pdf",
"location": "Belgium and Unreal Engine 4",
"rawData": "-",
"DOI": "10.48550/arXiv.1712.02463"
},
{
"id": "CURE-TSD",
"href": "https://github.com/olivesgatech/CURE-TSD",
"size_storage": "-",
"size_hours": "-",
"frames": "1720000",
"numberOfScenes": "5733",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera",
"sensorDetail": "Video sequences taken from BelgiumTS and different weather conditions and video quality simulated",
"recordingPerspective": "ego-perspective",
"dataType":"Real and Synthetic",
"mapData": "No",
"benchmark": "-",
"annotations": "object class labels",
"licensing": "Available upon registration",
"relatedDatasets": "CURE-TSR",
"publishDate":"2019-08-09",
"lastUpdate": "-",
"paperTitle": "Traffic Sign Detection Under Challenging Conditions: A Deeper Look into Performance Variations and Spectral Characteristics",
"relatedPaper": "https://arxiv.org/abs/1908.11262",
"location": "Belgium",
"rawData": "-",
"DOI": "10.1109/TITS.2019.2931429"
},
{
"id": "GOOSE",
"href": "https://goose-dataset.de/",
"size_storage": "28.8",
"size_hours": "24",
"frames": "10000",
"numberOfScenes": "234",
"samplingRate": "-",
"lengthOfScenes": "-",
"sensors": "camera, lidar, gps/imu",
"sensorDetail": "VLS128 roof LiDAR sensor + 4 RGB cameras (MuCAR-3 platform)",
"recordingPerspective": "ego-perspective",
"dataType":"Real",
"mapData": "No",
"benchmark": "-",
"annotations": "2d semantic segmentation, 3d semantic segmentation",
"licensing": "CC BY-SA 4.0 License",
"relatedDatasets": "GOOSE MuCAR-3",
"publishDate":"2023-10-25",
"lastUpdate": "-",
"paperTitle": "The GOOSE Dataset for Perception in Unstructured Environments",
"relatedPaper": "https://arxiv.org/abs/2310.16788",
"location": "Germany",
"rawData": "Yes",
"DOI": "10.48550/arXiv.2310.16788"
},
{
  "id": "DeepAccident",
  "href": "https://deepaccident.github.io/",
  "size_storage": "313",
  "size_hours": "24",
  "frames": "-",
  "numberOfScenes": "691",
  "samplingRate": "-",
  "lengthOfScenes": "-",
  "sensors": "multi-view camera, multi-view lidar",
  "sensorDetail": "ego camera (6 perspectives), infrastructure camera (6 perspectives), ego lidar (32-channel), infrastructure lidar (32-channel) (CARLA)",
  "recordingPerspective": "ego-perspective, infrastructure",
  "dataType":"Synthetic",
  "mapData": "Yes",
  "benchmark": "available",
  "annotations": "bev semantic segmentation, 3d bounding boxes",
  "licensing": "Free to use",
  "relatedDatasets": "DeepAccident",
  "publishDate":"2023-04-14",
  "lastUpdate": "-",
  "paperTitle": "DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving",
  "relatedPaper": "https://ojs.aaai.org/index.php/AAAI/article/view/28370",
  "location": "CARLA",
  "rawData": "Yes",
  "DOI": "10.1609/aaai.v38i6.28370"
  },
  {
    "id": "UTD19",
    "href": "https://utd19.ethz.ch/",
    "size_storage": "313",
    "size_hours": "33000",
    "frames": "170000000",
    "numberOfScenes": "-",
    "samplingRate": "-",
    "lengthOfScenes": "180 - 300",
    "sensors": "ground loop detectors",
    "sensorDetail": "from 23541 stationary detectors on urban roads",
    "recordingPerspective": "-",
    "dataType":"Real",
    "mapData": "No",
    "benchmark": "available",
    "annotations": "-",
    "licensing": "Freely available for non commercial use only",
    "relatedDatasets": "-",
    "publishDate":"2019-11-08",
    "lastUpdate": "-",
    "paperTitle": "Understanding traffic capacity of urban networks",
    "relatedPaper": "https://www.nature.com/articles/s41598-019-51539-5",
    "location": "global (40 cities)",
    "rawData": "No",
    "DOI": "10.1038/s41598-019-51539-5"
    }
]
