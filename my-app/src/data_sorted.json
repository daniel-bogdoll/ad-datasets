[
    {
        "id": "KITTI",
        "href": "http://www.cvlibs.net/datasets/kitti/",
        "size_hours": "6",
        "size_storage": "180",
        "frames": "-",
        "numberOfScenes": "50",
        "samplingRate": "10",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu",
        "sensorDetail": "2 greyscale cameras 1.4 MP, 2 color cameras 1.4 MP, 1 lidar 64 beams 360° 10Hz, 1 inertial and GPS navigation system",
        "benchmark": "stereo, optical flow, visual odometry, slam, 3d object detection, 3d object tracking",
        "annotations": "3d bounding boxes",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0",
        "relatedDatasets": "Semantic KITTI, KITTI-360",
        "publishDate": "2012-03-01",
        "lastUpdate": "2021-02-01",
        "paperTitle": "Vision meets Robotics: The KITTI Dataset",
        "relatedPaper": "http://www.cvlibs.net/publications/Geiger2013IJRR.pdf",
        "location": "Karlsruhe, Germany",
        "rawData": "Yes",
        "DOI": "10.1177%2F0278364913491297",
        "citationCount": 4814,
        "completionStatus": "complete"
    },
    {
        "id": "Cars Dataset",
        "href": "https://ai.stanford.edu/~jkrause/cars/car_dataset.html",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "16185",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "camera",
        "recordingPerspective": "-",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "car make, model, year",
        "licensing": "freely available for non-commercial research and educational use",
        "relatedDatasets": "BMW-10",
        "publishDate": "2013",
        "lastUpdate": "-",
        "paperTitle": "3D Object Representations for Fine-Grained Categorization",
        "relatedPaper": "https://ai.stanford.edu/~jkrause/papers/3drr13.pdf",
        "location": "Sourced from internet",
        "rawData": "-",
        "DOI": "10.1109/ICCVW.2013.77",
        "citationCount": 1668,
        "completionStatus": "complete"
    },
    {
        "id": "nuScenes",
        "href": "https://www.nuscenes.org/",
        "size_hours": "15",
        "size_storage": "-",
        "frames": "1400000",
        "numberOfScenes": "1000",
        "samplingRate": "-",
        "lengthOfScenes": "20",
        "sensors": "camera, lidar, radar, gps/imu",
        "sensorDetail": "1x lidar 32 channels 360° 20Hz, 5x long range radar 13Hz, 6x camera 1600x1200 12Hz, 1x gps/imu 1000Hz",
        "benchmark": "3d object detection, tracking, trajectory (prediction), lidar segmentation, panoptic segmentation & tracking",
        "annotations": "semantic category, attributes, 3d bounding boxes ",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",
        "relatedDatasets": "nuImages",
        "publishDate": "2019-03-01",
        "lastUpdate": "2020-12-01",
        "paperTitle": "nuScenes: A multimodal dataset for autonomous driving",
        "relatedPaper": "https://arxiv.org/pdf/1903.11027.pdf",
        "location": "Boston, USA and Singapore",
        "rawData": "Yes",
        "DOI": "10.1109/cvpr42600.2020.01164",
        "citationCount": 1411,
        "completionStatus": "complete"
    },
    {
        "id": "nuImages",
        "href": "https://www.nuscenes.org/nuimages",
        "size_hours": "150",
        "size_storage": "-",
        "frames": "1200000",
        "numberOfScenes": "93000",
        "samplingRate": "2",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, radar, gps/imu",
        "sensorDetail": "1x lidar 32 channels 360° 20Hz, 5x long range radar 13Hz, 6x camera 1600x1200 12Hz, 1x gps/imu 1000Hz",
        "benchmark": "-",
        "annotations": "instance masks, 2d bounding boxes, semantic segmentation masks, attribute annotations",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",
        "relatedDatasets": "nuScenes",
        "publishDate": "2020-07-01",
        "lastUpdate": "-",
        "location": "Boston, USA and Singapore",
        "rawData": "Yes",
        "DOI": "10.1109/cvpr42600.2020.01164",
        "citationCount": 1411,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Synthia",
        "href": "https://synthia-dataset.net/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "213400",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x rgb camera 960x720 100°",
        "recordingPerspective": "ego-perspective",
        "dataType": "Synthetic",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "semantic segmentation",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0",
        "relatedDatasets": "-",
        "publishDate": "2016-06-27",
        "lastUpdate": "2019-10-27",
        "paperTitle": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes",
        "relatedPaper": "https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.pdf",
        "location": "Unity development platform",
        "rawData": "-",
        "DOI": "10.1109/CVPR.2016.352",
        "citationCount": 1406,
        "completionStatus": "complete"
    },
    {
        "id": "GTA5",
        "href": "https://download.visinf.tu-darmstadt.de/data/from_games/",
        "size_storage": "57.05",
        "size_hours": "-",
        "frames": "24966",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "camera 1914x1052",
        "recordingPerspective": "ego-perspective",
        "dataType": "Synthetic",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "semantic segmentation of different classes",
        "licensing": "Freely available for research and educational purposes",
        "relatedDatasets": "-",
        "publishDate": "2016-04-08",
        "lastUpdate": "-",
        "paperTitle": "Playing for Data: Ground Truth from Computer Games",
        "relatedPaper": "https://arxiv.org/pdf/1608.02192.pdf",
        "location": "Game: Grand Theft Auto 5",
        "rawData": "-",
        "DOI": "10.1007/978-3-319-46475-6_7",
        "citationCount": 1177,
        "completionStatus": "complete"
    },
    {
        "id": "CamVid",
        "href": "https://www.kaggle.com/carlolepelaars/camvid",
        "size_storage": "-",
        "size_hours": "0.16",
        "frames": "701",
        "numberOfScenes": "1",
        "samplingRate": "30",
        "lengthOfScenes": "600",
        "sensors": "camera",
        "sensorDetail": "1x CCD Panasonic HVX200 digital camera 960x720 30Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "semantic segmentation",
        "licensing": "Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",
        "relatedDatasets": "-",
        "publishDate": "2009-01-15",
        "lastUpdate": "-",
        "paperTitle": "Semantic object classes in video: A high-definition ground truth database",
        "relatedPaper": "https://pdf.sciencedirectassets.com/271524/1-s2.0-S0167865508X00169/1-s2.0-S0167865508001220/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEGEaCXVzLWVhc3QtMSJHMEUCIQCA11sHV8h2EpCAzXyQ0V4VP%2F%2FtSTtdmwWBVRbF4T8AMwIgXu%2BDzZ9%2FZ5Matmw%2BNuJxkagSqNnDCZObjFIe3pMY2sgq2wQIif%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAEGgwwNTkwMDM1NDY4NjUiDF3yb0q0O6GdJqshbiqvBGiWqPb8EEGrHPUFfAVoMjtnfz4EEc6DVL5rwsMRYK7K5%2FddZ3n3M82ULw%2FjzUKCkbxBijZtK8o2tl%2BLKeIZ5y3Feep%2FIjIIhQ%2FjiZeni0N07G1%2FCQq6SOZivayRtp7CUjdgTXDq4zduoNEYNE1CZ4hhTLWK2RFjlPNmdOGUxCZbT2MNy27kbcwnjQw6u%2BF5Ro6L1q%2FI9bul4%2Foc0W40c0LnEQXMe5z8hTBKEFMwT5cEUf9%2FpnS%2BskWugGgsSNFiACeEJ0NETcVhuF6FLbDICqLE%2F7qw8wG0gzbDm2SljpYiw0gkW5gxczQE3Ru3sZ%2BQuUmh1zdCk4kt%2FazqaKZJqlvCnLgiQWuF%2BRNK4Hpteh%2B4i517UrWEAU8fpXuAyMVZG8JVG6xR%2FpCHWU7H1VNEA%2FbrL5zLkYp5SzX5xFzumsVeIJ37u69HuBgFFvKOn%2BmPtasGf5TudFpcA%2FE7d7C7lcES0AvfYbhyh%2FFqHCfG4s1MwfU6qJ61nZcYSMZgEFIiTlZGR80vgkYxCvIP0sNlgdOCQHArjxYHJ6yzjkPBFQ90c2snBDkfYfWcKrvWY%2FdGp4x8TBdaqrY07YEASRAOdd%2BJJUonHQDSRUJKmXibsNBM40z1qGflHPVL0wWAM3YITapAaLPxSagAqTPJ5AoOSXElsogZYoTHHXJU8KNIblOaz6XnKx32TLnw4FRkEeVBTwjOEFr5EA4srS1qbXlJsjV0aObvKBsr9Ja1UItx3CowtcCKlgY6qQFO6pFryXLKH4V7tldhJn%2BcQ35KUZOua1QBWSBkg38vxrjhQfEyvKCns7X21TxnzARlljVwzj9rL3HtZ7co4%2BT6pxwkYvj%2FKsTotHp5qtV5dYAI4w0dLNCsPz33OdLYqbBoIfr1OQ5USj9MvuO2k2e2voHKW1SmusBW5goBvuPNuWe4dHdFZAYtvtwGAs6iCSxbAYCPv6I%2BdPySt7vGydS05xTW%2FcPUZNGE&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220704T090151Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYWIZ3RZPY%2F20220704%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=f4dcb0c82201f26036b5850b0f38e19def847297bc9cc18b614ee094af344449&hash=8eef19412becfacf9bb21603db158b613ef51da58582217142ebc507be134724&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0167865508001220&tid=spdf-ec513c9a-f60a-4935-a0ec-6f936e44745a&sid=a0422e5a30875748cf6a74770c17f8e2edf3gxrqb&type=client&ua=4d50575004060303040a&rr=7256adbaee0c694c",
        "location": "United Kingdom",
        "rawData": "Yes",
        "DOI": "10.1016/j.patrec.2008.04.005",
        "citationCount": 933,
        "completionStatus": "complete"
    },
    {
        "id": "Oxford Robot Car",
        "href": "https://robotcar-dataset.robots.ox.ac.uk/",
        "size_hours": "210",
        "size_storage": "23150",
        "frames": "-",
        "numberOfScenes": "100",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, ins/gps",
        "sensorDetail": "1x camera Bumblebee XB3 1280x960x3 16Hz, 3x camera Grasshopper2 1024x1024 12Hz, 2x lidar SICK LMS-151 270° 50Hz, 1x lidar SICK LD-MRS 90° 4 plane 12.5Hz, 1x NovAtel SPAN-CPT ALIGN 50Hz GPS+INS",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International",
        "relatedDatasets": "Oxford Radar Robot Car",
        "publishDate": "2016-11-01",
        "lastUpdate": "2020-02-01",
        "paperTitle": "1 Year, 1000km: The Oxford RobotCar Dataset",
        "relatedPaper": "https://robotcar-dataset.robots.ox.ac.uk/images/robotcar_ijrr.pdf",
        "location": "Oxford, UK",
        "rawData": "Yes",
        "DOI": "10.1177%2F0278364916679498",
        "citationCount": 857,
        "completionStatus": "complete"
    },
    {
        "id": "Caltech Pedestrian",
        "href": "http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/",
        "size_storage": "-",
        "size_hours": "10",
        "frames": "1000000",
        "numberOfScenes": "137",
        "samplingRate": "30",
        "lengthOfScenes": "60",
        "sensors": "camera",
        "sensorDetail": "1x camera 640x480 30Hz",
        "benchmark": "pedestrian detection",
        "annotations": "bounding boxes",
        "licensing": "-",
        "relatedDatasets": "-",
        "publishDate": "2010-03-01",
        "lastUpdate": "2019-01-01",
        "paperTitle": "Pedestrian Detection: A Benchmark",
        "relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5206631",
        "location": "Loa Angeles, USA",
        "rawData": "Yes",
        "DOI": "10.1109/CVPR.2009.5206631",
        "citationCount": 711,
        "completionStatus": "complete"
    },
    {
        "id": "GTSRB",
        "href": "https://benchmark.ini.rub.de/gtsrb_news.html",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "50000",
        "numberOfScenes": "-",
        "samplingRate": "25",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x Prosilica GC 1380CH 1360x1024 25Hz",
        "recordingPerspective": "ego-perspetive",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "Traffic Sign Recognition",
        "annotations": "2d bounding box",
        "licensing": "Free to use",
        "relatedDatasets": "GTSDB",
        "publishDate": "2010-12-01",
        "lastUpdate": "2011-01-19",
        "paperTitle": "The German Traffic Sign Recognition Benchmark: A multi-class classification competition",
        "relatedPaper": "https://www.ini.rub.de/upload/file/1470692848_f03494010c16c36bab9e/StallkampEtAl_GTSRB_IJCNN2011.pdf",
        "location": "Germany",
        "rawData": "-",
        "DOI": "10.1109/IJCNN.2011.6033395",
        "citationCount": 710,
        "completionStatus": "complete"
    },
    {
        "id": "Caltech Lanes",
        "href": "http://www.mohamedaly.info/datasets/caltech-lanes",
        "size_storage": "0.55",
        "size_hours": "-",
        "frames": "1225",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "2d splines for lane markings",
        "licensing": "-",
        "relatedDatasets": "-",
        "publishDate": "2008-06-04",
        "lastUpdate": "-",
        "paperTitle": "Real time Detection of Lane Markers in Urban Streets",
        "relatedPaper": "https://ieeexplore.ieee.org/abstract/document/4621152?casa_token=V3xXln8DOeUAAAAA:h4ALqmxoJhgv3D2Szr9llQIc0UwNNkGNXzHR2486xb478J9dm_Pthf2ay4Zdl3-uPtl-BMi762BM",
        "location": "Pasadena, California, USA",
        "rawData": "-",
        "DOI": "10.1109/IVS.2008.4621152",
        "citationCount": 693,
        "completionStatus": "complete"
    },
    {
        "id": "Mapillary Vistas",
        "href": "https://www.mapillary.com/dataset/vistas",
        "size_storage": "",
        "size_hours": "-",
        "frames": "25000",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "-",
        "recordingPerspective": "-",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "semantic image segmentation and instance-specific image segmentation",
        "annotations": "instance specific object annotations",
        "licensing": " Creative Commons Attribution NonCommercial Share Alike (CC BY-NC-SA)",
        "relatedDatasets": "Mapillary Traffic Sign",
        "publishDate": "2017-12-25",
        "lastUpdate": "2021-01-18",
        "paperTitle": "The Mapillary Vistas Dataset for Semantic Understanding of Street Scene",
        "relatedPaper": "https://ieeexplore.ieee.org/document/8237796",
        "location": "Europe, North and South America, Asia, Africa and Oceania",
        "rawData": "-",
        "DOI": "10.1109/ICCV.2017.534",
        "citationCount": 662,
        "completionStatus": "complete"
    },
    {
        "id": "Waymo Open Perception",
        "href": "https://waymo.com/open/data/perception/",
        "size_hours": "10.83",
        "size_storage": "-",
        "frames": "390000",
        "numberOfScenes": "1950",
        "samplingRate": "10",
        "lengthOfScenes": "20",
        "sensors": "camera, lidar",
        "sensorDetail": "5x cameras (front and sides) 1920x1280 & 1920x1040, 1x mid-range lidar, 4x short-range lidars",
        "benchmark": "2d detection, 3d detection, 2d tracking, 3d tracking",
        "annotations": "3d bounding boxes (lidar), 2d bounding boxes (camera)",
        "licensing": "freely available for non-commercial purposes",
        "relatedDatasets": "Waymo Open Motion",
        "publishDate": "2019-08-01",
        "lastUpdate": "2020-03-01",
        "paperTitle": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
        "relatedPaper": "https://arxiv.org/pdf/1912.04838.pdf",
        "location": "San Francisco, Mountain View, Los Angeles, Detroit, Seattle and Phoenix, USA",
        "rawData": "Yes",
        "DOI": "10.1109/CVPR42600.2020.00252",
        "citationCount": 661,
        "completionStatus": "complete"
    },
    {
        "id": "Beyond PASCAL",
        "href": "https://yuxng.github.io/Xiang_WACV_03242014.pdf",
        "size_storage": "8.5",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "recordingPerspective": "-",
        "dataType": "Real",
        "mapData": "No",
        "sensorDetail": "-",
        "benchmark": "anomaly detection, 3D object detection and pose estimation",
        "annotations": "label landmarks of the CAD model on the 2D image",
        "licensing": "-",
        "relatedDatasets": "-",
        "publishDate": "2014-03-26",
        "lastUpdate": "-",
        "paperTitle": "Beyond PASCAL: A Benchmark for 3D Object Detection in the Wild",
        "relatedPaper": "https://cvgl.stanford.edu/papers/xiang_wacv14.pdf",
        "location": "-",
        "rawData": "-",
        "DOI": "10.1109/WACV.2014.6836101",
        "citationCount": 625,
        "completionStatus": "complete"
    },
    {
        "id": "GTSDB",
        "href": "https://benchmark.ini.rub.de/gtsdb_news.html",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "900",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x Prosilica GC 1380CH camera 1360x1024",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "Traffic Sign Detection",
        "annotations": "2d bounding box",
        "licensing": "-",
        "relatedDatasets": "GTSRB",
        "publishDate": "2012-12-01",
        "lastUpdate": "2013-08-09",
        "paperTitle": "Detection of traffic signs in real-world images: The German traffic sign detection benchmark",
        "relatedPaper": "https://ieeexplore.ieee.org/document/6706807",
        "location": "Bochum, Germany",
        "rawData": "-",
        "DOI": "10.1109/IJCNN.2013.6706807",
        "citationCount": 536,
        "completionStatus": "complete"
    },
    {
        "id": "BDD100k",
        "href": "https://www.bdd100k.com/",
        "size_storage": "1800",
        "size_hours": "1111",
        "frames": "120000000",
        "numberOfScenes": "100000",
        "samplingRate": "30",
        "lengthOfScenes": "40",
        "sensors": "camera, gps/imu",
        "sensorDetail": "crowd-sourced therefore no fixed setup, camera (720p) and gps/imu",
        "benchmark": "object detection, instance segmentation, multiple object tracking, segmentation tracking, semantic segmentation, lane marking, drivable area, image tagging, imitation learning, domain adaption",
        "annotations": "bounding boxes, instance segmentation, semantic segmentation, box tracking, semantic tracking, drivable area",
        "licensing": "BSD 3-Clause",
        "relatedDatasets": "-",
        "publishDate": "2020-04-01",
        "lastUpdate": "-",
        "paperTitle": "BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning",
        "relatedPaper": "https://arxiv.org/pdf/1805.04687.pdf",
        "location": "New York, Berkeley, San Francisco and Bay Area, USA",
        "rawData": "Yes",
        "citationCount": 525,
        "completionStatus": "complete"
    },
    {
        "id": "LISA Traffic Sign Dataset",
        "href": "http://cvrr.ucsd.edu/LISA/lisa-traffic-sign-dataset.html",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "6610",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "2d bounding boxes with description",
        "licensing": "Academic license",
        "relatedDatasets": "LISA Traffic Light Dataset",
        "publishDate": "2012-10-19",
        "lastUpdate": "-",
        "paperTitle": "Vision-Based Traffic Sign Detection and Analysis for Intelligent Driver Assistance Systems: Perspectives and Survey",
        "relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6335478&casa_token=iD5J89e1Y0MAAAAA:OKU-qJJPvSmZk1wJGlPb4G3Z7SF12nHZdr7mEV03pwgl2Q8ASZlH7T2zbo5n65e4yBniT_S5jQfn&tag=1",
        "location": "USA",
        "rawData": "-",
        "DOI": "10.1109/TITS.2012.2209421",
        "citationCount": 517,
        "completionStatus": "complete"
    },
    {
        "id": "Argoverse Motion Forecasting",
        "href": "https://www.argoverse.org/",
        "size_storage": "4.81",
        "size_hours": "320",
        "frames": "16227850",
        "numberOfScenes": "324557",
        "samplingRate": "10",
        "lengthOfScenes": "5",
        "sensors": "camera, lidar, gps",
        "sensorDetail": "2x lidar 32 beam 40° 10Hz, 7x ring cameras 1920x1200 combined 360° 30Hz, 2x front-view facing stereo cameras 0.2986m baseline 2056x2464 5Hz",
        "benchmark": "forecasting",
        "annotations": "semantic vector map, rasterized map, trajectories",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",
        "relatedDatasets": "Argoverse 3D Tracking",
        "publishDate": "2019-06-01",
        "lastUpdate": "-",
        "paperTitle": "Argoverse: 3D Tracking and Forecasting with Rich Maps",
        "relatedPaper": "https://arxiv.org/pdf/1911.02620.pdf",
        "location": "Miami and Pittsburgh, USA",
        "rawData": "No",
        "DOI": "10.1109/CVPR.2019.00895",
        "citationCount": 488,
        "completionStatus": "complete"
    },
    {
        "id": "Argoverse 3D Tracking",
        "href": "https://www.argoverse.org/",
        "size_storage": "254.4",
        "size_hours": "1",
        "frames": "44000",
        "numberOfScenes": "113",
        "samplingRate": "30",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps",
        "sensorDetail": "2x lidar 40° 10Hz, 7x ring cameras 1920x1200 combined 360° 30Hz, 2x front-view facing stereo cameras 2056x2464 5Hz",
        "benchmark": "tracking",
        "annotations": "semantic vector map, rasterized map, 3d bounding boxes",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public (CC BY-NC-SA 4.0)",
        "relatedDatasets": "Argoverse Motion Forecasting",
        "publishDate": "2019-06-01",
        "lastUpdate": "-",
        "paperTitle": "Argoverse: 3D Tracking and Forecasting with Rich Maps",
        "relatedPaper": "https://arxiv.org/pdf/1911.02620.pdf",
        "location": "Miami and Pittsburgh, USA",
        "rawData": "Yes",
        "DOI": "10.1109/CVPR.2019.00895",
        "citationCount": 488,
        "completionStatus": "complete"
    },
    {
        "id": "Argoverse 1 Stereo",
        "href": "https://www.argoverse.org/data.html#stereo-link",
        "size_storage": "14.2",
        "size_hours": "-",
        "frames": "6624",
        "numberOfScenes": "74",
        "samplingRate": "5",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar",
        "sensorDetail": "2x lidar 40° 10Hz, 7x ring cameras 1920x1200 combined 360° 30Hz, 2x front-view facing stereo cameras 2056x2464 5Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "3d bounding boxes",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (“CC BY-NC-SA 4.0”)",
        "relatedDatasets": "Argoverse 3D Tracking, Argoverse 1 Maps, Argoverse 1 Motion Forecasting and  Argoverse 2",
        "publishDate": "2019-06-15",
        "lastUpdate": "-",
        "paperTitle": "Argoverse: 3D Tracking and Forecasting With Rich Maps",
        "relatedPaper": "https://arxiv.org/pdf/1911.02620.pdf",
        "location": "Miami and Pittsburgh, USA",
        "rawData": "-",
        "DOI": "10.1109/CVPR.2019.00895",
        "citationCount": 488,
        "completionStatus": "complete"
    },
    {
        "id": "Stanford Drone Dataset",
        "href": "https://cvgl.stanford.edu/projects/uav_data/",
        "size_storage": "69",
        "size_hours": "-",
        "frames": "929499",
        "numberOfScenes": "8",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x Quadcopter camera 1400x1904",
        "recordingPerspective": "top-view",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "2d bounding boxes for objects",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License",
        "relatedDatasets": "-",
        "publishDate": "2016-08-01",
        "lastUpdate": "-",
        "paperTitle": "Learning Social Etiquette: Human Trajectory Understanding In Crowded Scenes",
        "relatedPaper": "https://link.springer.com/chapter/10.1007/978-3-319-46484-8_33",
        "location": "Stanford campus, USA",
        "rawData": "-",
        "DOI": "10.1007/978-3-319-46484-8_33",
        "citationCount": 470,
        "completionStatus": "complete"
    },
    {
        "id": "TT100K",
        "href": "http://cg.cs.tsinghua.edu.cn/traffic-sign/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "100000",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "traffic-sign detection and classification",
        "annotations": "2d bounding boxes and class labels",
        "licensing": "Creative Commons Attribution-NonCommercial (CC-BY-NC",
        "relatedDatasets": "-",
        "publishDate": "2016-06-27",
        "lastUpdate": "-",
        "paperTitle": "Traffic-Sign Detection and Classification in the Wild",
        "relatedPaper": "http://cg.cs.tsinghua.edu.cn/traffic-sign/0682.pdf",
        "location": "China",
        "rawData": "-",
        "DOI": "10.1109/CVPR.2016.232",
        "citationCount": 385,
        "completionStatus": "complete"
    },
    {
        "id": "CULane Dataset",
        "href": "https://xingangpan.github.io/projects/CULane.html",
        "size_storage": "42.5",
        "size_hours": "-",
        "frames": "133235",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "traffic lanes with cubic splines",
        "licensing": "freely available for non-commercial purpose",
        "relatedDatasets": "-",
        "publishDate": "2018-04-27",
        "lastUpdate": "-",
        "paperTitle": "Spatial as Deep: Spatial CNN for Traffic Scene Understanding",
        "relatedPaper": "https://arxiv.org/pdf/1712.06080",
        "location": "Beijing, China",
        "rawData": "-",
        "DOI": "10.1609/aaai.v32i1.12301",
        "citationCount": 347,
        "completionStatus": "complete"
    },
    {
        "id": "highD",
        "href": "https://www.highd-dataset.com/",
        "size_storage": "-",
        "size_hours": "16.5",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "25",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x quadcopter DJI Phantom 4 Pro Plus camera 4096x2160 25Hz",
        "recordingPerspective": "top-view",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "2d bounding boxes with vehicle speed and id",
        "licensing": "Freely available for non-commercial use only upon registration",
        "relatedDatasets": "inD, rounD, exiD and uniD datasets",
        "publishDate": "2018-11-04",
        "lastUpdate": "-",
        "paperTitle": "The highD Dataset: A Drone Dataset of Naturalistic Vehicle Trajectories on German Highways for Validation of Highly Automated Driving Systems",
        "relatedPaper": "https://arxiv.org/ftp/arxiv/papers/1810/1810.05642.pdf",
        "location": "Cologne, Germany",
        "rawData": "-",
        "DOI": "10.1109/ITSC.2018.8569552",
        "citationCount": 315,
        "completionStatus": "complete"
    },
    {
        "id": "Ford CAMPUS",
        "href": "http://robots.engin.umich.edu/SoftwareData/Ford",
        "size_storage": "197",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu",
        "sensorDetail": "1x Point Grey Ladybug3 omnidirectional camera 1600x600 8Hz, 1x Velodyne HDL-64E lidar 360° 10Hz, 2x Riegl LMS-Q120 lidars 80°, 1x Applanix POS-LV 420 INS with Trimble GPS 100Hz, 1x Xsens MTi-G IMU 100Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "SLAM, iterative closest point (ICP), and 3D object detection and recognition",
        "annotations": "-",
        "licensing": "Freely available for non-commercial use",
        "relatedDatasets": "NCLT",
        "publishDate": "2011-11-01",
        "lastUpdate": "-",
        "paperTitle": "Ford Campus vision and lidar data set",
        "relatedPaper": "https://journals.sagepub.com/doi/pdf/10.1177/0278364911400640",
        "location": "Dearborn Michigan, USA",
        "rawData": "-",
        "DOI": "10.1177/0278364911400640",
        "citationCount": 275,
        "completionStatus": "complete"
    },
    {
        "id": "VPGNet",
        "href": "https://arxiv.org/abs/1710.06288",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "21097",
        "numberOfScenes": "-",
        "samplingRate": "30",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "lane and road markings, vanishing point",
        "licensing": "Available for non commercial research purposes on registration",
        "relatedDatasets": "-",
        "publishDate": "2017-10-22",
        "lastUpdate": "-",
        "paperTitle": "VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition",
        "relatedPaper": "https://openaccess.thecvf.com/content_ICCV_2017/papers/Lee_VPGNet_Vanishing_Point_ICCV_2017_paper.pdf",
        "location": "Seoul, South Korea",
        "rawData": "-",
        "DOI": "10.1109/ICCV.2017.215",
        "citationCount": 247,
        "completionStatus": "complete"
    },
    {
        "id": "Road Damage",
        "href": "https://github.com/sekilab/RoadDamageDetector/",
        "size_storage": "2.4",
        "size_hours": "-",
        "frames": "13135",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "2d bounding box with class labels",
        "licensing": "Creative Commons Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0)",
        "relatedDatasets": "Road Damage Dataset 2018",
        "publishDate": "2018-06-30",
        "lastUpdate": "2020-06-02",
        "paperTitle": "Road Damage Detection and Classification Using Deep Neural Networks with Smartphone Images",
        "relatedPaper": "https://arxiv.org/pdf/1801.09454.pdf",
        "location": "Japan",
        "rawData": "-",
        "DOI": "10.1111/mice.12387",
        "citationCount": 246,
        "completionStatus": "complete"
    },
    {
        "id": "ApolloScape",
        "href": "http://apolloscape.auto/",
        "size_hours": "100",
        "size_storage": "-",
        "frames": "143906",
        "numberOfScenes": "-",
        "samplingRate": "30",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, imu/gnss",
        "sensorDetail": "2x VUX-1HA laser scanners 360°, 1x VMX-CS6 camera system, 1x measuring head with gnss/imu, 2x high frontal cameras 3384 ×2710",
        "benchmark": "2d image parsing, 3d car instance understanding, landmark segmentation, self-localization, trajectory prediction, 3d detection, 3d tracking, stereo",
        "annotations": "high density 3d point cloud map, per-pixel, per-frame semantic image label, lane mark label semantic instance segmentation, geo-tagged",
        "licensing": "freely available for non-commercial purposes",
        "relatedDatasets": "-",
        "publishDate": "2018-03-01",
        "lastUpdate": "2020-09-01",
        "paperTitle": "The ApolloScape Open Dataset for Autonomous Driving and its Application",
        "relatedPaper": "https://arxiv.org/pdf/1803.06184.pdf",
        "location": "Beijing, Shanghai and Shenzhen, China",
        "rawData": "Yes",
        "citationCount": 244,
        "completionStatus": "complete"
    },
    {
        "id": "NCLT",
        "href": "http://robots.engin.umich.edu/nclt/",
        "size_storage": "2824",
        "size_hours": "34.9",
        "frames": "-",
        "numberOfScenes": "27",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu",
        "sensorDetail": "1x Pointgrey Ladybug3 (LB3) camera system 1600x1200 5Hz, 1x Velodyne HDL-32E lidar 360° 10Hz, 1x Hokuyo UTM-30LX lidar 270°, 1x Hokuyo URG-04LX lidar 240°, 1x Microstrain 3DM-GX3-45 IMU 100Hz, 1x KVH DSP-3000 single-axis FOG, 1x Garmin 18x 5Hz, 1x NovAtel DL-4 plus RTK GPS 1Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Open Data Commons Open Database License (ODbL) v1.0",
        "relatedDatasets": "Ford CAMPUS",
        "publishDate": "2015-12-24",
        "lastUpdate": "2019-03-24",
        "paperTitle": "University of Michigan North Campus Long-Term Vision and Lidar Dataset",
        "relatedPaper": "http://robots.engin.umich.edu/publications/ncarlevaris-2015a.pdf",
        "location": "University of Michigan North Campus, USA",
        "rawData": "Yes",
        "DOI": "10.1177/0278364915614638",
        "citationCount": 224,
        "completionStatus": "complete"
    },
    {
        "id": "TUD Brussels",
        "href": "https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/people-detection-pose-estimation-and-tracking/multi-cue-onboard-pedestrian-detection/",
        "size_storage": "3.0",
        "size_hours": "-",
        "frames": "1016",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "hand held camera 720x576",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "2d bounding boxes for pedestrians",
        "licensing": "-",
        "relatedDatasets": "TUD-MotionPairs",
        "publishDate": "2009-06-20",
        "lastUpdate": "2010-04-13",
        "paperTitle": "Multi-cue onboard pedestrian detection",
        "relatedPaper": "https://ieeexplore.ieee.org/document/5206638",
        "location": "-",
        "rawData": "-",
        "DOI": "10.1109/CVPR.2009.5206638",
        "citationCount": 214,
        "completionStatus": "complete"
    },
    {
        "id": "DDAD",
        "href": "https://github.com/AdrienGaidon-TRI/DDAD",
        "size_storage": "254",
        "size_hours": "-",
        "frames": "21200",
        "numberOfScenes": "435",
        "samplingRate": "10",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar",
        "sensorDetail": "6x cameras 2.4MP 1936x1216 10Hz, 1x Luminar-H2 Lidar sensor 360° 10Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License",
        "relatedDatasets": "-",
        "publishDate": "2020-06-19",
        "lastUpdate": "-",
        "paperTitle": "3D Packing for Self-Supervised Monocular Depth Estimation",
        "relatedPaper": "https://arxiv.org/pdf/1905.02693.pdf",
        "location": "USA (Ann Arbor, San Francisco Bay Area, Detroit, Cambridge and Massachusetts), Japan (Tokyo and Odaiba)",
        "rawData": "-",
        "DOI": "10.1109/CVPR42600.2020.00256",
        "citationCount": 204,
        "completionStatus": "complete"
    },
    {
        "id": "Malaga Stereo and Laser Urban",
        "href": "https://www.mrpt.org/MalagaUrbanDataset",
        "size_storage": "4.76",
        "size_hours": "1.55",
        "frames": "-",
        "numberOfScenes": "15",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu",
        "sensorDetail": "1x Point Grey Research Bumblebee 2 stereo camera 1024x768 20Hz, 3x Hokuyo UTM-30LX laser scanners 270°, 2x SICK LMS-200 laser scanners, 1x xSens MTi imu 100Hz, 2x mmGPS devices from Topcon gps",
        "recordingPerspective": "ego-perspective and bird's eye",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "-",
        "relatedDatasets": "-",
        "publishDate": "2013-10-09",
        "lastUpdate": "2018-09-13",
        "paperTitle": "The Málaga urban dataset: High-rate stereo and LiDAR in a realistic urban scenario",
        "relatedPaper": "https://journals.sagepub.com/doi/10.1177/0278364913507326",
        "location": "Málaga, Spain",
        "rawData": "-",
        "DOI": "10.1177/0278364913507326",
        "citationCount": 202,
        "completionStatus": "complete"
    },
    {
        "id": "Brain4Cars",
        "href": "http://brain4cars.com/",
        "size_storage": "16",
        "size_hours": "-",
        "frames": "2000000",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, speed logger, gps",
        "sensorDetail": "-",
        "recordingPerspective": "Face camera, ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "driving events (lane changes, turns, driving straight)",
        "licensing": "free for educational and non-commercial purposes",
        "relatedDatasets": "-",
        "publishDate": "2015-12-13",
        "lastUpdate": "-",
        "paperTitle": "Car that Knows Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models",
        "relatedPaper": "http://brain4cars.com/pdfs/iccv2015.pdf",
        "location": "USA",
        "rawData": "-",
        "DOI": "10.1109/ICCV.2015.364",
        "citationCount": 182,
        "completionStatus": "complete"
    },
    {
        "id": "comma.ai",
        "href": "http://research.comma.ai/",
        "size_storage": "80",
        "size_hours": "7.25",
        "frames": "-",
        "numberOfScenes": "10",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License",
        "relatedDatasets": "-",
        "publishDate": "2016-08-03",
        "lastUpdate": "-",
        "paperTitle": "Learning a Driving Simulator",
        "relatedPaper": "https://arxiv.org/pdf/1608.01230",
        "location": "-",
        "rawData": "-",
        "DOI": "10.48550/arXiv.1608.01230",
        "citationCount": 182,
        "completionStatus": "complete"
    },
    {
        "id": "CARLA100",
        "href": "https://github.com/felipecode/coiltraine/blob/master/docs/exploring_limitations.md",
        "relatedPaper": "https://arxiv.org/pdf/1904.08980.pdf",
        "citationCount": 181,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Multi Vehicle Stereo Event Camera",
        "href": "https://daniilidis-group.github.io/mvsec/",
        "size_storage": "187",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "11",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, VI-Sensor, gps",
        "sensorDetail": "2x DAVIS m346B camera 346x60 83° 50Hz, 1x Velodyne Puck LITE 360° 20Hz, 1x Skybotix integrated VI-sensor stereo camera: 2 x Aptina MT9V034, 1x UBLOX NEO-M8N gps",
        "recordingPerspective": "ego-perspective, bird's eye",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "ground truth pose",
        "licensing": "Creative Commons Attribution-ShareAlike 4.0 International License",
        "relatedDatasets": "-",
        "publishDate": "2017-12-10",
        "lastUpdate": "2018-09-26",
        "paperTitle": "The Multivehicle Stereo Event Camera Dataset: An Event Camera Dataset for 3D Perception",
        "relatedPaper": "https://ieeexplore.ieee.org/document/8288670",
        "location": "-",
        "rawData": "-",
        "DOI": "10.1109/LRA.2018.2800793",
        "citationCount": 170,
        "completionStatus": "complete"
    },
    {
        "id": "Stanford Track Collection",
        "href": "https://cs.stanford.edu/people/teichman/stc/",
        "size_storage": "2",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "10",
        "lengthOfScenes": "-",
        "sensors": "lidar, gps/imu",
        "sensorDetail": "1x Velodyne HDL-64E S2 lidar 360° 10Hz, 1x Applanix LV-420 gps 200Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Freely available for non-commercial research",
        "relatedDatasets": "-",
        "publishDate": "2011-05-09",
        "lastUpdate": "2012-06-01",
        "paperTitle": "Towards 3D object recognition via classification of arbitrary object tracks",
        "relatedPaper": "https://cs.stanford.edu/people/teichman/papers/icra2011.pdf",
        "location": "-",
        "rawData": "-",
        "DOI": "10.1109/ICRA.2011.5979636",
        "citationCount": 161,
        "completionStatus": "complete"
    },
    {
        "id": "LISA Traffic Light Dataset",
        "href": "http://cvrr.ucsd.edu/LISA/datasets.html",
        "size_storage": "5",
        "size_hours": "0.75",
        "frames": "43007",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x Point Grey's Bumblebee XB3 (BBX3-13S2C-60) stereo camera 1280x960 ",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "2d bounding boxes with the description of state of traffic light",
        "licensing": "Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",
        "relatedDatasets": "LISA Traffic Sign Dataset",
        "publishDate": "2016-02-03",
        "lastUpdate": "-",
        "paperTitle": "Vision for Looking at Traffic Lights: Issues, Survey, and Perspectives",
        "relatedPaper": "https://ieeexplore.ieee.org/document/7398055",
        "location": "San Diego, California, USA",
        "rawData": "-",
        "DOI": "10.1109/TITS.2015.2509509",
        "citationCount": 153,
        "completionStatus": "complete"
    },
    {
        "id": "INTERACTION dataset",
        "href": "https://interaction-dataset.com/",
        "size_storage": "-",
        "size_hours": "16.5",
        "frames": "594588",
        "numberOfScenes": "-",
        "samplingRate": "10",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "drones & traffic cameras 3840x2160 30Hz downscaled to 10Hz",
        "benchmark": "motion prediction",
        "annotations": "2d bounding boxes, semantic map, motion/trajectories",
        "licensing": "freely available for non-commercial purposes",
        "relatedDatasets": "-",
        "publishDate": "2019-09-01",
        "lastUpdate": "-",
        "relatedPaper": "https://arxiv.org/pdf/1910.03088.pdf",
        "location": "USA, China, Germany and Bulgaria",
        "rawData": "Yes",
        "citationCount": 152,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Oxford Radar Robot Car",
        "href": "https://oxford-robotics-institute.github.io/radar-robotcar-dataset/",
        "size_storage": "4700",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "32",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, radar, gps/imu",
        "sensorDetail": "1 x Navtech CTS350-X Millimetre-Wave FMCW radar 4 Hz, 2 x Velodyne HDL-32E LIDAR 360°32 planes 20 Hz, 1 x Point Grey Bumblebee XB3 trinocular stereo camera 1280×960×3 16 Hz 66°3 x Point Grey Grasshopper2 1024×1024 11.1 Hz 180°, 2 x SICK LMS-151 2D LIDAR 270° 50Hz, 1 x NovAtel SPAN-CPT ALIGN inertial and GPS navigation system 6 axis 50Hz,",
        "benchmark": "-",
        "annotations": "ground truth data",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International",
        "relatedDatasets": "Oxford Robot Car",
        "publishDate": "2020-02-01",
        "lastUpdate": "-",
        "relatedPaper": "https://arxiv.org/pdf/1909.01300.pdf",
        "location": "Oxford",
        "rawData": "Yes",
        "citationCount": 145,
        "completionStatus": "partially Complete"
    },
    {
        "id": "India Driving Dataset",
        "href": "https://idd.insaan.iiit.ac.in/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "10004",
        "numberOfScenes": "182",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1080p & 720p stereo image",
        "benchmark": "Pixel-Level Semantic Segmentation Task, Instance-Level Semantic Segmentation Task",
        "annotations": "semantic segmentation",
        "licensing": "-",
        "relatedDatasets": "-",
        "publishDate": "2018-11-01",
        "lastUpdate": "-",
        "relatedPaper": "https://idd.insaan.iiit.ac.in/media/publications/idd-650.pdf",
        "location": "Bangalore and Hyderabad, India",
        "rawData": "Yes",
        "DOI": "10.1109/WACV.2019.00190",
        "citationCount": 141,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Bosch Small Traffic Lights Dataset",
        "href": "https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "13427",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "raw 12bit HDR images with a red-clear-clear-blue filter 1280x720 & reconstructed 8-bit RGB color images 1280x720",
        "benchmark": "-",
        "annotations": "bounding boxes, state",
        "licensing": "freely available for non-commercial purposes",
        "relatedDatasets": "-",
        "publishDate": "2017-05-01",
        "lastUpdate": "-",
        "relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7989163",
        "location": "-",
        "rawData": "Yes",
        "DOI": "10.1109/ICRA.2017.7989163",
        "citationCount": 139,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Bosch TL",
        "href": "https://github.com/asimonov/Bosch-TL-Dataset",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "13427",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "camera 1280x720",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "2d bounding boxes",
        "licensing": "freely available for non-commercial use",
        "relatedDatasets": "-",
        "publishDate": "2017-07-24",
        "lastUpdate": "-",
        "paperTitle": "A deep learning approach to traffic lights: Detection, tracking, and classification",
        "relatedPaper": "https://ieeexplore.ieee.org/document/7989163",
        "location": "El Camino Real in the San Francisco Bay Area, California",
        "rawData": "-",
        "DOI": "10.1109/ICRA.2017.7989163",
        "citationCount": 139,
        "completionStatus": "complete"
    },
    {
        "id": "TME Motorway",
        "href": "http://cmp.felk.cvut.cz/data/motorway/",
        "size_storage": "-",
        "size_hours": "0.45",
        "frames": "30000",
        "numberOfScenes": "28",
        "samplingRate": "20",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar",
        "sensorDetail": "1x stereo camera 1024x768 grayscale 32° 20Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "bounding boxes around vehicles",
        "licensing": "Freely available for commercial and non-commercial research",
        "relatedDatasets": "-",
        "publishDate": "2012-09-16",
        "lastUpdate": "-",
        "paperTitle": "A System for Real-time Detection and Tracking of Vehicles from a Single Car-mounted Camera",
        "relatedPaper": "http://cmp.felk.cvut.cz/data/motorway/paper/itsc2012.pdf",
        "location": "Northern Italy",
        "rawData": "-",
        "DOI": "10.1109/ITSC.2012.6338748",
        "citationCount": 130,
        "completionStatus": "complete"
    },
    {
        "id": "HDD",
        "href": "https://usa.honda-ri.com/hdd",
        "size_storage": "150",
        "size_hours": "104",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "30",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu, can",
        "sensorDetail": "3x Point Grey Grasshopper 3 video cameras 1920x1200 30Hz, 1x Velodyne HDL-64E S2 3D LiDAR 360° 10Hz, 1 x GeneSys Eletronik GmbH Automotive Dynamic Motion Analyzer with DGPS 120Hz, 1x Vehicle Controller Area Network 100Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "driver behaviour label",
        "licensing": "Available for research for people affiliated with a university upon registration",
        "relatedDatasets": "HDBD, HAD, HEV-I, HSD, LOKI, TITAN and EPOSH",
        "publishDate": "2018-06-18",
        "lastUpdate": "-",
        "paperTitle": "Toward Driving Scene Understanding: A Dataset for Learning Driver Behavior and Causal Reasoning",
        "relatedPaper": "https://usa.honda-ri.com/documents/248678/249773/CVPR_18_HDD_Yi_Ting-v2.pdf/bb391444-9687-7b3b-0b34-c3534f15904f",
        "location": "San Francisco Bay Area, USA",
        "rawData": "-",
        "DOI": "10.1109/CVPR.2018.00803",
        "citationCount": 130,
        "completionStatus": "complete"
    },
    {
        "id": "KITTI-360",
        "href": "http://www.cvlibs.net/datasets/kitti-360/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "400000",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu",
        "sensorDetail": "2x 180° fisheye camera, 1x 90° perspective stereo camera, 1x Velodyne HDL-64E & SICK LMS 200 laser scanning unit in pushbroom configuration",
        "benchmark": "-",
        "annotations": "semantic instance segmentation",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0",
        "relatedDatasets": "KITTI",
        "publishDate": "2015-11-01",
        "lastUpdate": "2021-04-01",
        "relatedPaper": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Xie_Semantic_Instance_Annotation_CVPR_2016_paper.pdf",
        "location": "Karlsruhe, Germany",
        "rawData": "Yes",
        "DOI": "10.1109/CVPR.2016.401",
        "citationCount": 129,
        "completionStatus": "partially Complete"
    },
    {
        "id": "DR(eye)VE",
        "href": "https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=8",
        "size_storage": "-",
        "size_hours": "6.16",
        "frames": "555000",
        "numberOfScenes": "74",
        "samplingRate": "-",
        "lengthOfScenes": "300",
        "sensors": "camera, eye tracker",
        "sensorDetail": "1x roof mounted GARMIN VirbX camera 1080p 25Hz, 1x SMI ETG 2w Eye Tracking Glasses (ETG) 60Hz, 1x frontal camera 720p 30Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "Gaze Maps, GPS, Speed and Course",
        "licensing": "Freely available for non-commercial use upon registration",
        "relatedDatasets": "-",
        "publishDate": "2018-06-08",
        "lastUpdate": "-",
        "paperTitle": "Predicting the Driver's Focus of Attention: The DR(eye)VE Project",
        "relatedPaper": "https://arxiv.org/pdf/1705.03854.pdf",
        "location": "-",
        "rawData": "-",
        "DOI": "10.1109/TPAMI.2018.2845370",
        "citationCount": 121,
        "completionStatus": "complete"
    },
    {
        "id": "JAAD",
        "href": "https://paperswithcode.com/dataset/jaad",
        "size_storage": "3.1",
        "size_hours": "-",
        "frames": "82032",
        "numberOfScenes": "346",
        "samplingRate": "30",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x camera 1920x1080 110°, 1x camera 1920x1080 170°, 1x camera 1280x720 100°",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "2d bounding boxes, behavioral tags, contextual tags",
        "licensing": "MIT License",
        "relatedDatasets": "PIE Dataset",
        "publishDate": "2016-09-15",
        "lastUpdate": "-",
        "paperTitle": "Are They Going to Cross? A Benchmark Dataset and Baseline for Pedestrian Crosswalk Behavior",
        "relatedPaper": "https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w3/Rasouli_Are_They_Going_ICCV_2017_paper.pdf",
        "location": "North America and Europe",
        "rawData": "-",
        "DOI": "10.1109/ICCVW.2017.33",
        "citationCount": 117,
        "completionStatus": "complete"
    },
    {
        "id": "KAIST Multi-Spectral Day/Night",
        "href": "http://multispectral.kaist.ac.kr",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "25",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu, thermal camera",
        "sensorDetail": "2x PointGrey Flea3 RGB camera 1280 × 960, 1x FLIR A655Sc thermal camera 640x480 50Hz, 1x Velodyne HDL-32E 3D LiDAR 360° 32 beams 10Hz, 1x OXTS RT2002 gps/ins 100Hz",
        "benchmark": "object detection, vision sensor enhancement, depth estimation, multi-spectral colorization",
        "annotations": "dense depth map, bounding boxes",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0",
        "relatedDatasets": "-",
        "publishDate": "2017-12",
        "lastUpdate": "-",
        "relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8293689",
        "location": "-",
        "rawData": "Yes",
        "DOI": "10.1109/TITS.2018.2791533",
        "citationCount": 114,
        "completionStatus": "partially Complete"
    },
    {
        "id": "WoodScape",
        "href": "https://paperswithcode.com/dataset/woodscape",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "10000",
        "numberOfScenes": "-",
        "samplingRate": "30",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu, odometer",
        "sensorDetail": "4x 1MPx RGB fisheye cameras 190°, 1x Velodyne HDL-64E lidar 20Hz, 1x NovAtel Propak6 & SPAN-IGM-A1 gnss/imu, 1x Garmin 18x GNSS Positioning with SPS, Odometry signals from the vehicle bus",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "semantic segmentation, monocular depth estimation, 2D & 3D bounding boxes,  visual odometry, visual SLAM, motion segmentation, soiling detection and end-to-end driving (driving controls)",
        "licensing": "Freely available for non-commercial research",
        "relatedDatasets": "-",
        "publishDate": "2019-10-27",
        "lastUpdate": "2021-11-16",
        "paperTitle": "WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving",
        "relatedPaper": "https://arxiv.org/pdf/1905.01489.pdf",
        "location": "USA, Europe, and China",
        "rawData": "-",
        "DOI": "10.1109/ICCV.2019.00940",
        "citationCount": 105,
        "completionStatus": "complete"
    },
    {
        "id": "H3D",
        "href": "https://usa.honda-ri.com/H3D",
        "size_storage": "-",
        "size_hours": "0.77",
        "frames": "27721",
        "numberOfScenes": "160",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu",
        "sensorDetail": "3x color PointGrey Grasshopper3 video cameras 1920x1200 90°/80° 30Hz, 1x Velodyne HDL-64E LiDAR 64 beams 360° 10Hz, 1x GeneSys Eletronik GmbH Automotive Dynamic Motion Analyzer 100Hz",
        "recordingPerspective": "Ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "3d bounding boxes",
        "licensing": "-",
        "relatedDatasets": "-",
        "publishDate": "2019-03-01",
        "lastUpdate": "-",
        "paperTitle": "The H3D Dataset for Full-Surround 3D Multi-Object Detection and Tracking in Crowded Urban Scenes",
        "relatedPaper": "https://arxiv.org/pdf/1903.01568.pdf",
        "location": "San Francisco Bay Area, USA",
        "rawData": "Yes",
        "DOI": "10.1109/ICRA.2019.8793925",
        "citationCount": 103,
        "completionStatus": "complete"
    },
    {
        "id": "KAIST Urban",
        "href": "https://irap.kaist.ac.kr/dataset/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "18",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu",
        "sensorDetail": "2x Velodyne VLP-16 16 channel lidar 360° 10Hz, 2x SICK LMS-511 1 channel lidar 190° 100Hz, 1x stereo camera 1280x560 10Hz",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0",
        "relatedDatasets": "-",
        "publishDate": "2017-09-01",
        "lastUpdate": "2019-06-01",
        "relatedPaper": "https://irap.kaist.ac.kr/dataset/papers/IJRR2019_dataset.pdf",
        "location": "Seoul, Pangyo, Daejeon, Suwon and Dongtan, Korea",
        "rawData": "Yes",
        "DOI": "10.1177%2F0278364919843996",
        "citationCount": 97,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Complex Urban Dataset",
        "href": "https://sites.google.com/view/complex-urban-dataset",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "41",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu, altimeter",
        "sensorDetail": "2x FLIR FL3-U3-20E4C-C Global shutter color camera 1280x560 10Hz, 2x Velodyne VLP-16 16 CH LiDAR 360° 10Hz, 2x SICK LMS-511 1 CH LiDAR 190° 100Hz, 1x U-Blox EVK-7P Consumer-level GPS 10Hz, 1x SOKKIA GRX 2 VRS-RTK GPS 1Hz, 1x Xsens MTi-300 Consumer-level AHRS imu 200Hz, 1x  myPressure Altimeter sensor 10Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 License",
        "relatedDatasets": "",
        "publishDate": "2017-09-13",
        "lastUpdate": "2019-06-17",
        "paperTitle": "Complex urban dataset with multi-level sensors from highly diverse urban environments",
        "relatedPaper": "https://journals.sagepub.com/doi/pdf/10.1177/0278364919843996",
        "location": "South Korea",
        "rawData": "Yes",
        "DOI": "10.1177/0278364919843996",
        "citationCount": 97,
        "completionStatus": "complete"
    },
    {
        "id": "Ground Truth Stixel",
        "href": "http://www.6d-vision.com/ground-truth-stixel-dataset",
        "size_storage": "3.2",
        "size_hours": "-",
        "frames": "78500",
        "numberOfScenes": "318",
        "samplingRate": "25",
        "lengthOfScenes": "-",
        "sensors": "camera, radar",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "stixel measurements",
        "licensing": "freely available for of scholarly and technical work",
        "relatedDatasets": "-",
        "publishDate": "2013-06-23",
        "lastUpdate": "-",
        "paperTitle": "Exploiting the Power of Stereo Confidences",
        "relatedPaper": "http://wwwlehre.dhbw-stuttgart.de/~sgehrig/stixelGroundTruthDataset/stixel.pdf",
        "location": "-",
        "rawData": "-",
        "DOI": "10.1109/CVPR.2013.45",
        "citationCount": 95,
        "completionStatus": "complete"
    },
    {
        "id": "inD",
        "href": "https://www.ind-dataset.com/",
        "size_storage": "-",
        "size_hours": "10",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "25",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x quadcopter DJI Phantom 4 Pro camera 4096x2160 25Hz",
        "recordingPerspective": "top-view",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "2d bounding box for objects, trajectory information",
        "licensing": "Freely available for non-commercial use only upon registration",
        "relatedDatasets": "highD, rounD, exiD and uniD datasets",
        "publishDate": "2020-10-19",
        "lastUpdate": "-",
        "paperTitle": "The inD Dataset: A Drone Dataset of Naturalistic Road User Trajectories at German Intersections",
        "relatedPaper": "https://arxiv.org/pdf/1911.07602.pdf",
        "location": "Aachen, Germany",
        "rawData": "-",
        "DOI": "10.1109/IV47402.2020.9304839",
        "citationCount": 95,
        "completionStatus": "complete"
    },
    {
        "id": "Synscapes",
        "href": "https://7dlabs.com/synscapes-overview",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "25000",
        "numberOfScenes": "25000",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "RGB images in PNG format 1440x720 & upscaled version 2048x1024",
        "benchmark": "-",
        "annotations": "2d bounding boxes, 3d bounding boxes, occlusion, truncation, semantic segmentation,instance segmentation, depth segmentation, scene metadata",
        "licensing": "freely available for non-commercial purposes",
        "relatedDatasets": "-",
        "publishDate": "2018-10-01",
        "lastUpdate": "-",
        "relatedPaper": "https://arxiv.org/pdf/1810.08705v1.pdf",
        "location": "-",
        "rawData": "-",
        "citationCount": 90,
        "completionStatus": "partially Complete"
    },
    {
        "id": "WildDash",
        "href": "https://wilddash.cc/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "156",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "various sources, e.g. YouTube",
        "benchmark": "semantic segmentation, instance segmentation, panoptic segmentation",
        "annotations": "semantic segmentation, instance segmentation",
        "licensing": "CC-BY-NC 4.0 ",
        "relatedDatasets": "-",
        "publishDate": "2018-02-01",
        "lastUpdate": "2020-06-01",
        "relatedPaper": "https://openaccess.thecvf.com/content_ECCV_2018/papers/Oliver_Zendel_WildDash_-_Creating_ECCV_2018_paper.pdf",
        "location": "All over the world",
        "rawData": "Yes",
        "DOI": "10.1007/978-3-030-01231-1_25",
        "citationCount": 88,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Lyft Level5 Prediction",
        "href": "https://level-5.global/data/prediction/",
        "size_hours": "1118",
        "size_storage": "-",
        "frames": "42500000",
        "numberOfScenes": "170000",
        "samplingRate": "10",
        "lengthOfScenes": "25",
        "sensors": "camera, lidar, radar",
        "sensorDetail": "7 cameras with 360° view, 3 lidars with 40-64 channels at 10Hz, 5 radars",
        "benchmark": "-",
        "annotations": "semantic map \"annotations\", trajectories",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 (CC-BY-NC-SA-4.0)",
        "relatedDatasets": "Lyft Level5 Perception",
        "publishDate": "2020-06-01",
        "lastUpdate": "-",
        "relatedPaper": "https://arxiv.org/pdf/2006.14480v1.pdf",
        "location": "Palo Alto, USA",
        "rawData": "No",
        "citationCount": 88,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Lyft Level5 Perception",
        "href": "https://level-5.global/data/perception/",
        "size_hours": "2.5",
        "size_storage": "-",
        "frames": "-",
        "numberOfScenes": "366",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar",
        "sensorDetail": "-",
        "benchmark": "-",
        "annotations": "3d bounding boxes, rasterised road geometry",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 (CC-BY-NC-SA-4.0)",
        "relatedDatasets": "Lyft Level5 Prediction",
        "publishDate": "2019-07-01",
        "lastUpdate": "-",
        "relatedPaper": "https://arxiv.org/pdf/2006.14480v1.pdf",
        "location": "Palo Alto, USA",
        "rawData": "Yes",
        "citationCount": 88,
        "completionStatus": "partially Complete"
    },
    {
        "id": "One Thousand and One Hours",
        "href": "https://level-5.global/data/prediction/",
        "size_storage": "-",
        "size_hours": "1118",
        "frames": "-",
        "numberOfScenes": "170000",
        "samplingRate": "-",
        "lengthOfScenes": "25",
        "sensors": "camera, lidar, radar",
        "sensorDetail": "-",
        "recordingPerspective": "top view",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "trajectories",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike license (CC-BY-NC-SA-4.0)",
        "relatedDatasets": "Level 5 Perception Dataset",
        "publishDate": "2020-06-25",
        "lastUpdate": "-",
        "paperTitle": "One Thousand and One Hours: Self-driving Motion Prediction Dataset",
        "relatedPaper": "https://arxiv.org/pdf/2006.14480.pdf",
        "location": "Palo Alto, California, USA",
        "rawData": "-",
        "DOI": "10.48550/arXiv.2006.14480",
        "citationCount": 88,
        "completionStatus": "complete"
    },
    {
        "id": "BoxCars116k",
        "href": "https://github.com/JakubSochor/BoxCars",
        "relatedPaper": "https://ieeexplore.ieee.org/document/8307405",
        "DOI": "10.1109/TITS.2018.2799228",
        "citationCount": 86,
        "completionStatus": "partially Complete"
    },
    {
        "id": "A2D2",
        "href": "https://www.a2d2.audi/a2d2/en.html",
        "size_storage": "2300",
        "size_hours": "-",
        "frames": "433833",
        "numberOfScenes": "3",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu",
        "sensorDetail": "5x lidar 16 channels 360° 10Hz, 1x front centre camera 1920x1208 30Hz, 5x surround cameras1920x1208 30Hz, vehicle bus data",
        "benchmark": "-",
        "annotations": "semantic segmentation, point cloud segmentation, instance segmentation, 3d bounding boxes",
        "licensing": "Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0)",
        "relatedDatasets": "-",
        "publishDate": "2020-04-01",
        "lastUpdate": "-",
        "relatedPaper": "https://arxiv.org/pdf/2004.06320.pdf",
        "location": "Three cities in the south of Germany",
        "rawData": "Yes",
        "citationCount": 85,
        "completionStatus": "partially Complete"
    },
    {
        "id": "UAH-DriveSet",
        "href": "http://www.robesafe.uah.es/personal/eduardo.romera/uah-driveset/",
        "size_storage": "-",
        "size_hours": "8.33",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, gps",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Freely available to academic and non-academic entities for non-commercial purposes such as academic research, teaching, scientific publications, or personal experimentation",
        "relatedDatasets": "-",
        "publishDate": "2016-11-01",
        "lastUpdate": "-",
        "paperTitle": "Need Data for Driver Behaviour Analysis? Presenting the Public UAH-DriveSet",
        "relatedPaper": "http://www.robesafe.uah.es/personal/eduardo.romera/pdfs/Romera16itsc.pdf",
        "location": "-",
        "rawData": "Yes",
        "DOI": "10.1109/ITSC.2016.7795584",
        "citationCount": 83,
        "completionStatus": "complete"
    },
    {
        "id": "Five Roundabouts Dataset",
        "href": "http://its.acfr.usyd.edu.au/datasets-2/five-roundabouts-dataset/",
        "size_storage": "0.5",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "25",
        "lengthOfScenes": "-",
        "sensors": "lidar",
        "sensorDetail": "6x 4 beam ibeo LUX lidars 110° 25Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "trajectory tracking information",
        "licensing": "Creative Commons Attribution 4.0",
        "relatedDatasets": "naturalistic-intersection-driving-dataset",
        "publishDate": "2019-05-13",
        "lastUpdate": "-",
        "paperTitle": "Naturalistic Driver Intention and Path Prediction Using Recurrent Neural Networks",
        "relatedPaper": "https://ieeexplore.ieee.org/document/8713418",
        "location": "Sydney, Australia",
        "rawData": "-",
        "DOI": "10.1109/TITS.2019.2913166",
        "citationCount": 82,
        "completionStatus": "complete"
    },
    {
        "id": "PIE",
        "href": "https://data.nvision2.eecs.yorku.ca/PIE_dataset/",
        "size_storage": "-",
        "size_hours": "6",
        "frames": "909480",
        "numberOfScenes": "53",
        "samplingRate": "30",
        "lengthOfScenes": "600",
        "sensors": "camera, gps",
        "sensorDetail": "1x monocular dashboard camera Waylens Horizon 1920x1080 157° 30Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "2d bounding boxes for pedestrians with occlusion flages, text labels for pedestrian actions, crossing intention confidence scores, gps data and vehicle informatiom for every frame",
        "licensing": "MIT License",
        "relatedDatasets": "JAAD Dataset",
        "publishDate": "201-10-27",
        "lastUpdate": "-",
        "paperTitle": "PIE: A Large-Scale Dataset and Models for Pedestrian Intention Estimation and Trajectory Prediction",
        "relatedPaper": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Rasouli_PIE_A_Large-Scale_Dataset_and_Models_for_Pedestrian_Intention_Estimation_ICCV_2019_paper.pdf",
        "location": "Toronto, Canada",
        "rawData": "Yes",
        "DOI": "10.1109/ICCV.2019.00636",
        "citationCount": 78,
        "completionStatus": "complete"
    },
    {
        "id": "LostAndFound",
        "href": "http://www.6d-vision.com/lostandfounddataset",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "21040",
        "numberOfScenes": "112",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "stereo camera setup baseline 21cm 2048x1024",
        "benchmark": "anomaly detection",
        "annotations": "semantic segmentation",
        "licensing": "freely available for non-commercial purposes",
        "relatedDatasets": "-",
        "publishDate": "2016-09-01",
        "lastUpdate": "-",
        "relatedPaper": "https://arxiv.org/pdf/1609.04653.pdf",
        "location": "-",
        "rawData": "Yes",
        "citationCount": 74,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Tsinghua Daimler Cyclist Detection",
        "href": "http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Tsinghua-Daimler_Cyclist_Detec/tsinghua-daimler_cyclist_detec.html",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "11467",
        "numberOfScenes": "-",
        "samplingRate": "25",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "cyclist detection",
        "annotations": "2d bounding boxes",
        "licensing": "Freely available non-commercial purposes",
        "relatedDatasets": "-",
        "publishDate": "2016-06-19",
        "lastUpdate": "-",
        "paperTitle": "A new benchmark for vision-based cyclist detection",
        "relatedPaper": "https://ieeexplore.ieee.org/document/7535515",
        "location": "Beijing, China",
        "rawData": "-",
        "DOI": "10.1109/IVS.2016.7535515",
        "citationCount": 74,
        "completionStatus": "complete"
    },
    {
        "id": "DBNet",
        "href": "http://www.dbehavior.net/",
        "size_storage": "1610",
        "size_hours": "10",
        "frames": "56800",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "lidar, camera, steering angle meter, gps",
        "sensorDetail": "1x Velodyne HDL-32E laser scanner 360° 10Hz, 1x Velodyne VLP-16 laser scanner, 1x color dashboard camera 1920x1080 30Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "Driving policy prediction",
        "annotations": "-",
        "licensing": "Freely available for non-commercial use upon registration",
        "relatedDatasets": "-",
        "publishDate": "2018-06-23",
        "lastUpdate": "-",
        "paperTitle": "LiDAR-Video Driving Dataset: Learning Driving Policies Effectively",
        "relatedPaper": "https://ieeexplore.ieee.org/document/8578713",
        "location": "-",
        "rawData": "Yes",
        "DOI": "10.1109/CVPR.2018.00615",
        "citationCount": 73,
        "completionStatus": "complete"
    },
    {
        "id": "LiDAR-Video Driving Dataset",
        "href": "https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf",
        "size_storage": "1000",
        "size_hours": "-",
        "frames": "10000",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, accelerometer, steering angle meter",
        "sensorDetail": "1x HDL-32E Velodyne lidar 360° 10Hz, 1x VLP-16 Velodyne lidar, 1x Dashboard camera 1920x1080 30Hz ",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "driver behaviour",
        "annotations": "-",
        "licensing": "-",
        "relatedDatasets": "-",
        "publishDate": "2018-06-18",
        "lastUpdate": "-",
        "paperTitle": "LiDAR-Video Driving Dataset: Learning Driving Policies Effectively",
        "relatedPaper": "https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf",
        "location": "-",
        "rawData": "-",
        "DOI": "10.1109/CVPR.2018.00615",
        "citationCount": 73,
        "completionStatus": "complete"
    },
    {
        "id": "SeeingThroughFog",
        "href": "https://github.com/princeton-computational-imaging/SeeingThroughFog",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "1400000",
        "numberOfScenes": "-",
        "samplingRate": "10",
        "lengthOfScenes": "-",
        "sensors": "camera, radar, lidar, imu, weather sensor",
        "sensorDetail": "2x stereo cameras 1920x1024 30Hz, 1 gated camera 1280x720 120Hz, 1 FMCW radar 15Hz, 2x Velodyne lidars 10Hz, 1 FIR camera 640x480 30Hz, 1 Airmar WX150 weather sensor (temperature, wind speed and humidity)",
        "recordingPerspective": "Ego-perspective",
        "dataType": "real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "weather of scenes in frames",
        "licensing": "Freely available for research and teaching purposes",
        "relatedDatasets": "Gated2Gated, Gated2Depth, PointCloudDeNoising",
        "publishDate": "2019-02-24",
        "lastUpdate": "-",
        "paperTitle": "Seeing Through FogWithout Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather",
        "relatedPaper": "https://www.cs.princeton.edu/~fheide/AdverseWeatherFusion/figures/AdverseWeatherFusion.pdf",
        "location": "Germany, Sweden, Denmark and Finland",
        "rawData": "-",
        "DOI": "10.1109/CVPR42600.2020.01170",
        "citationCount": 73,
        "completionStatus": "complete"
    },
    {
        "id": "HD1K",
        "href": "http://hci-benchmark.iwr.uni-heidelberg.de/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "55",
        "samplingRate": "25",
        "lengthOfScenes": "-",
        "sensors": "lidar, camera, gps/imu",
        "sensorDetail": "1x RIEGL VMX-250-CS6 laser scanner, 1x stereo system with 2 cameras 2560x1080 69.5° 200Hz, 1x Applanix POS-LV 510 gnss & imu unit",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "stereo, optical flow, single image depth prediction, object detection, and semantic segmentation",
        "annotations": "depth and optical flow",
        "licensing": "Freely available for research purposes",
        "relatedDatasets": "-",
        "publishDate": "2018-02-01",
        "lastUpdate": "2018-03-05",
        "paperTitle": "The HCI Benchmark Suite: Stereo And Flow Ground Truth With Uncertainties for Urban Autonomous Driving",
        "relatedPaper": "http://hci-benchmark.iwr.uni-heidelberg.de/media/publications//kondermann2016.pdf",
        "location": "-",
        "rawData": "-",
        "DOI": "10.1109/CVPRW.2016.10",
        "citationCount": 71,
        "completionStatus": "complete"
    },
    {
        "id": "HCI Challenging Stereo",
        "href": "https://hci.iwr.uni-heidelberg.de/benchmarks/Challenging_Data_for_Stereo_and_Optical_Flow",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "10000",
        "numberOfScenes": "11",
        "samplingRate": "100",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "2 x Photon Focus MV1-D1312-160-CL-12",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Freely available for research purposes only",
        "relatedDatasets": "-",
        "publishDate": "2012-03-07",
        "lastUpdate": "-",
        "paperTitle": "Outdoor stereo camera system for the generation of real-world benchmark data sets",
        "relatedPaper": "https://www.spiedigitallibrary.org/journals/Optical-Engineering/volume-51/issue-2/021107/Outdoor-stereo-camera-system-for-the-generation-of-real-world/10.1117/1.OE.51.2.021107.short?SSO=1",
        "location": "-",
        "rawData": "-",
        "DOI": "10.1117/1.OE.51.2.021107",
        "citationCount": 68,
        "completionStatus": "complete"
    },
    {
        "id": "MuIRan",
        "href": "https://sites.google.com/view/mulran-pr/dataset",
        "size_storage": "387",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "lidar, radar, gps/imu",
        "sensorDetail": "1x Ouster OS1-64 64 channel lidar 360° 10Hz, 1x Navtech CIR204-H radar 360° 4Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "trajectory information",
        "licensing": "Available upon registration",
        "relatedDatasets": "-",
        "publishDate": "2019-05-24",
        "lastUpdate": "2021-05-31",
        "paperTitle": "MulRan: Multimodal Range Dataset for Urban Place Recognition",
        "relatedPaper": "https://ieeexplore.ieee.org/document/9197298",
        "location": "South Korea",
        "rawData": "Yes",
        "DOI": "10.1109/ICRA40945.2020.9197298",
        "citationCount": 66,
        "completionStatus": "complete"
    },
    {
        "id": "HEV-I",
        "href": "https://usa.honda-ri.com/hevi",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "230",
        "samplingRate": "10",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x RGB camera with 1920x1200 10Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "location and tracking information of objects for predicting future ego-motion",
        "licensing": "Available for research for people affiliated with a university upon registration",
        "relatedDatasets": "HDBD, HAD, HDD, HSD, LOKI, TITAN and EPOSH",
        "publishDate": "2019-05-20",
        "lastUpdate": "-",
        "paperTitle": "Egocentric Vision-based Future Vehicle Localization for Intelligent Driving Assistance Systems",
        "relatedPaper": "https://arxiv.org/pdf/1809.07408.pdf",
        "location": "San Francisco Bay Area, USA",
        "rawData": "-",
        "DOI": "10.1109/ICRA.2019.8794474",
        "citationCount": 62,
        "completionStatus": "complete"
    },
    {
        "id": "Waymo Open Motion",
        "href": "https://waymo.com/open/data/motion/",
        "size_hours": "574",
        "size_storage": "-",
        "frames": "20670800",
        "numberOfScenes": "103354",
        "samplingRate": "10",
        "lengthOfScenes": "20",
        "sensors": "camera, lidar",
        "sensorDetail": "5x cameras, 5x lidar, ",
        "benchmark": "motion prediction, interaction prediction",
        "annotations": "3d bounding boxes, 3d hd map information",
        "licensing": "freely available for non-commercial purposes",
        "relatedDatasets": "Waymo Open Perception",
        "publishDate": "2021-03-01",
        "lastUpdate": "2021-09-01",
        "relatedPaper": "https://arxiv.org/pdf/2104.10133.pdf",
        "location": "San Francisco, Mountain View, Los Angeles, Detroit, Seattle and Phoenix, USA",
        "rawData": "No",
        "citationCount": 58,
        "completionStatus": "partially Complete"
    },
    {
        "id": "CADC",
        "href": "http://cadcd.uwaterloo.ca/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "75",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "cameras, lidar, gps",
        "sensorDetail": "8x camera Ximea MQ013CG-E2 1280x1024 10Hz,  1x lidar Veldyne VLP-32C 360° 10Hz, 1x NovAtel OEM638 Triple-Frequency GPS, 1x Sensonor STIM300 MEMS 100Hz IMU, 2x Xsens 200Hz IMU",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "2d/3d bounding boxes",
        "licensing": "Creative Commons Attribution-NonCommercial 4.0 International Public License (CC BY-NC 4.0)",
        "relatedDatasets": "-",
        "publishDate": "2020-01-27",
        "lastUpdate": "-",
        "paperTitle": "Canadian Adverse Driving Conditions Dataset",
        "relatedPaper": "https://arxiv.org/pdf/2001.10117.pdf",
        "location": "Waterloo region in Ontorio, Canada",
        "rawData": "-",
        "DOI": "10.1177/0278364920979368",
        "citationCount": 55,
        "completionStatus": "complete"
    },
    {
        "id": "Fishyscapes",
        "href": "https://fishyscapes.com/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "based on the validation set of Cityscapes overlayed with anomalous objects and the original LostAndFound with extended pixel-wise annotations",
        "benchmark": "anomaly detection, semantic segmentation",
        "annotations": "semantic segmentation",
        "licensing": "-",
        "relatedDatasets": "Cityscapes, LostAndFound",
        "publishDate": "2019-09-01",
        "lastUpdate": "-",
        "relatedPaper": "https://arxiv.org/pdf/1904.03215.pdf",
        "location": "-",
        "rawData": "No",
        "citationCount": 52,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Toronto 3D",
        "href": "https://github.com/WeikaiTan/Toronto-3D",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps",
        "sensorDetail": "1x Teledyne Optech Maverick consists of a 32-line LiDAR sensor, a Ladybug 5 panoramic camera, a GNSS system, and a SLAM system",
        "recordingPerspective": "bird's eye view",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "object class labels",
        "licensing": "Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)",
        "relatedDatasets": "-",
        "publishDate": "2020-03-22",
        "lastUpdate": "2020-04-23",
        "paperTitle": "Toronto-3D: A Large-scale Mobile LiDAR Dataset for Semantic Segmentation of Urban Roadways",
        "relatedPaper": "https://openaccess.thecvf.com/content_CVPRW_2020/papers/w11/Tan_Toronto-3D_A_Large-Scale_Mobile_LiDAR_Dataset_for_Semantic_Segmentation_of_CVPRW_2020_paper.pdf",
        "location": "Toronto, Canada",
        "rawData": "-",
        "DOI": "10.1109/CVPRW50498.2020.00109",
        "citationCount": 52,
        "completionStatus": "complete"
    },
    {
        "id": "Synthetic Discrepancy Datasets",
        "href": "https://github.com/cvlab-epfl/detecting-the-unexpected",
        "relatedPaper": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lis_Detecting_the_Unexpected_via_Image_Resynthesis_ICCV_2019_paper.pdf",
        "DOI": "10.1109/ICCV.2019.00224",
        "citationCount": 51,
        "completionStatus": "partially Complete"
    },
    {
        "id": "SemKITTI-DVPS",
        "href": "https://github.com/joe-siyuan-qiao/ViP-DeepLab",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "Images taken from SemanticKITTI dataset and modified with annotations",
        "recordingPerspective": "ego-perspetive",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "panoptic segmentation, monocular depth",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike",
        "relatedDatasets": "SemanticKITTI",
        "publishDate": "2021-06-20",
        "lastUpdate": "-",
        "paperTitle": "ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation",
        "relatedPaper": "https://arxiv.org/pdf/2012.05258",
        "location": "Karlsruhe, Germany",
        "rawData": "Yes",
        "DOI": "10.1109/CVPR46437.2021.00399",
        "citationCount": 48,
        "completionStatus": "complete"
    },
    {
        "id": "Cityscapes-DVPS",
        "href": "https://github.com/joe-siyuan-qiao/ViP-DeepLab",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "Images taken from Cityscapes dataset and modified with annotations",
        "recordingPerspective": "ego-perspetive",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "panoptic segmentation, monocular depth",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike",
        "relatedDatasets": "Cityscapes",
        "publishDate": "2021-06-20",
        "lastUpdate": "-",
        "paperTitle": "ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation",
        "relatedPaper": "https://arxiv.org/pdf/2012.05258",
        "location": "-",
        "rawData": "Yes",
        "DOI": "10.1109/CVPR46437.2021.00399",
        "citationCount": 48,
        "completionStatus": "complete"
    },
    {
        "id": "A*3D",
        "href": "https://github.com/I2RDL2/ASTAR-3D",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "39179",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar",
        "sensorDetail": "2x PointGrey Chameleon3 USB3 Global shutter color cameras 2048x1536 57.3° 55Hz, 1x Velodyne HDL-64ES3 3D-LiDAR 10Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "3d object detection",
        "annotations": "3d bounding boxes",
        "licensing": "Freely available for noncommercial academic research purposes only",
        "relatedDatasets": "-",
        "publishDate": "2019-10-04",
        "lastUpdate": "-",
        "paperTitle": "A 3D Dataset: Towards Autonomous Driving in Challenging Environments",
        "relatedPaper": "https://arxiv.org/pdf/1909.07541.pdf",
        "location": "Singapore",
        "rawData": "-",
        "DOI": "10.1109/ICRA40945.2020.9197385",
        "citationCount": 47,
        "completionStatus": "complete"
    },
    {
        "id": "NightOwls",
        "href": "https://www.nightowls-dataset.org/",
        "size_storage": "-",
        "size_hours": "5.17",
        "frames": "279000",
        "numberOfScenes": "40",
        "samplingRate": "15",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x industry standard 1024x640 camera",
        "benchmark": "pedestrian detection, object detection",
        "annotations": "bounding boxes, attributes, temporal tracking annotations",
        "licensing": "freely available for non-commercial purposes",
        "relatedDatasets": "-",
        "publishDate": "2018-12-01",
        "lastUpdate": "-",
        "relatedPaper": "https://www.robots.ox.ac.uk/~vgg/publications/2018/Neumann18b/neumann18b.pdf",
        "location": "Several cities across Europe",
        "rawData": "Yes",
        "DOI": "10.1007/978-3-030-20887-5_43",
        "citationCount": 43,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Street Hazards",
        "href": "https://github.com/hendrycks/anomaly-seg",
        "size_storage": "2.0",
        "size_hours": "-",
        "frames": "7656",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "No",
        "mapData": "No",
        "benchmark": "Anamoly object segmentation",
        "annotations": "semantic segmentation of anamolies",
        "licensing": "-",
        "relatedDatasets": "-",
        "publishDate": "2019-11-25",
        "lastUpdate": "-",
        "paperTitle": "Scaling Out-of-Distribution Detection for Real-World Settings",
        "relatedPaper": "https://arxiv.org/pdf/1911.11132.pdf",
        "location": "Carla",
        "rawData": "-",
        "DOI": "10.48550/arXiv.1911.11132",
        "citationCount": 42,
        "completionStatus": "complete"
    },
    {
        "id": "The EuroCity Persons Dataset",
        "href": "https://arxiv.org/abs/1805.07193",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "47300",
        "numberOfScenes": "-",
        "samplingRate": "20",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x camera 1920x1080 20Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "object detection",
        "annotations": "2d bounding boxes",
        "licensing": "freely available for research use by eligiible persons",
        "relatedDatasets": "ECP2.5D - Person Localization in Traffic Scenes",
        "publishDate": "2019-03-31",
        "lastUpdate": "2020-11-11",
        "paperTitle": "The EuroCity Persons Dataset: A Novel Benchmark for Object Detection",
        "relatedPaper": "https://arxiv.org/pdf/1805.07193.pdf",
        "location": "12 countries and 31 cities across Europe",
        "rawData": "-",
        "DOI": "10.1109/TPAMI.2019.2897684",
        "citationCount": 41,
        "completionStatus": "complete"
    },
    {
        "id": "Comma2k19",
        "href": "https://github.com/commaai/comma2k19",
        "size_storage": "100",
        "size_hours": "33.65",
        "frames": "-",
        "numberOfScenes": "2019",
        "samplingRate": "-",
        "lengthOfScenes": "60",
        "sensors": "camera, radar, gnss/imu ",
        "sensorDetail": "two different car types, 1x road-facing camera Sony IMX2984 20Hz, 1x gnss u-blox M8 chip5 10Hz, gyro and accelerometer data LSM6DS3 100Hz, magnetometer data AK09911 10Hz",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "MIT",
        "relatedDatasets": "-",
        "publishDate": "2018-12-01",
        "lastUpdate": "-",
        "relatedPaper": "http://export.arxiv.org/pdf/1812.05752",
        "location": "California's 280 highway, USA",
        "rawData": "Yes",
        "citationCount": 34,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Ford Autonomous Vehicle Dataset",
        "href": "https://avdata.ford.com/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu",
        "sensorDetail": "4x HDL-32E Lidars, 4x Flea3 GigE Point Grey Cameras in stereo pairs (front & back) 80° 15Hz,2x Flea3 GigE Point Grey Cameras (sides) 80° 15Hz, 1x Flea3 GigE Point Grey Camera 40° 7Hz, 1x Applanix POS LV gps/imu",
        "benchmark": "-",
        "annotations": "3d point cloud maps, ground reflectivity map",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International",
        "relatedDatasets": "-",
        "publishDate": "2020-03-01",
        "lastUpdate": "-",
        "relatedPaper": "https://s23.q4cdn.com/258866874/files/doc_downloads/2020/03/2003.07969.pdf",
        "location": "Michigan, USA",
        "rawData": "True",
        "DOI": "10.1177/0278364920961451",
        "citationCount": 34,
        "completionStatus": "partially Complete"
    },
    {
        "id": "rounD",
        "href": "https://www.round-dataset.com/",
        "size_storage": "-",
        "size_hours": "6",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "25",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x DJI Phantom 4 Pro quadcopter camera 4096x2160 25Hz",
        "recordingPerspective": "top-view",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "2d bounding boxes for objects, trajectory tracking information",
        "licensing": "Freely available for non-commercial use only upon registration",
        "relatedDatasets": "highD, inD, exiD and uniD",
        "publishDate": "2020-09-20",
        "lastUpdate": "-",
        "paperTitle": "The rounD Dataset: A Drone Dataset of Road User Trajectories at Roundabouts in Germany",
        "relatedPaper": "https://ieeexplore.ieee.org/document/9294728",
        "location": "Aachen and Alsdorf, Germany",
        "rawData": "-",
        "DOI": "10.1109/ITSC45102.2020.9294728",
        "citationCount": 34,
        "completionStatus": "complete"
    },
    {
        "id": "RUGD: Robot Unstructured Ground Driving",
        "href": "http://rugd.vision/",
        "size_storage": "5.4",
        "size_hours": "-",
        "frames": "37000",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu",
        "sensorDetail": "1x Prosilica GT2750C camera 1376x1110, 1x Velodyne HDL-32 LiDAR, 1x Garmin GPS receiver, 1x Microstrain GX3-25 IMU",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "semantic segmentation",
        "annotations": "",
        "licensing": "Freely available for research purposes",
        "relatedDatasets": "-",
        "publishDate": "2019-11-08",
        "lastUpdate": "-",
        "paperTitle": "A RUGD Dataset for Autonomous Navigation and Visual Perception in Unstructured Outdoor Environments",
        "relatedPaper": "http://rugd.vision/pdfs/RUGD_IROS2019.pdf",
        "location": "-",
        "rawData": "-",
        "DOI": "10.1109/IROS40897.2019.8968283",
        "citationCount": 34,
        "completionStatus": "complete"
    },
    {
        "id": "D^2 City",
        "href": "https://outreach.didichuxing.com/d2city",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "11211",
        "samplingRate": "25",
        "lengthOfScenes": "30",
        "sensors": "camera",
        "sensorDetail": "",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "2d bounding boxes and inter-frame tracking labels",
        "licensing": "-",
        "relatedDatasets": "-",
        "publishDate": "2019-04-03",
        "lastUpdate": "-",
        "paperTitle": "D^2-City: A Large-Scale Dashcam Video Dataset of Diverse Traffic Scenarios",
        "relatedPaper": "https://arxiv.org/pdf/1904.01975v1.pdf",
        "location": "5 Chinese cities",
        "rawData": "-",
        "DOI": "10.48550/arXiv.1904.01975",
        "citationCount": 33,
        "completionStatus": "complete"
    },
    {
        "id": "ONCE",
        "href": "https://once-for-auto-driving.github.io/index.html",
        "size_storage": "-",
        "size_hours": "144",
        "frames": "1000000",
        "numberOfScenes": "581",
        "samplingRate": "10",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar",
        "sensorDetail": "7x color cameras 1920x1020 10Hz, 1x 40-beam lidar 360° 10Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "benchmark": "3d object detection",
        "annotations": "2d/3d bounding boxes",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (CC BY-NC-SA 4.0)",
        "relatedDatasets": "ONCE-3DLanes",
        "publishDate": "2021-05-18",
        "lastUpdate": "2021-08-05",
        "paperTitle": "One Million Scenes for Autonomous Driving: ONCE Dataset",
        "relatedPaper": "https://arxiv.org/pdf/2106.11037.pdf",
        "location": "China",
        "rawData": "-",
        "DOI": "10.48550/arXiv.2106.11037",
        "citationCount": 32,
        "completionStatus": "complete"
    },
    {
        "id": "CADP",
        "href": "https://ankitshah009.github.io/accident_forecasting_traffic_camera",
        "size_storage": "-",
        "size_hours": "5.2",
        "frames": "-",
        "numberOfScenes": "1416",
        "samplingRate": "20",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "-",
        "recordingPerspective": "Bird's Eye",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "Object detectiona and accident forecasting",
        "licensing": "freely available for non-commercial use",
        "relatedDatasets": "-",
        "publishDate": "2018-10-04",
        "lastUpdate": "-",
        "paperTitle": "CADP: A Novel Dataset for CCTV Traffic Camera based Accident Analysis",
        "relatedPaper": "https://ppms.cit.cmu.edu/media/project_files/CADP_IEEE_Camera_Ready_Final.pdf",
        "location": "Videos sampled from YouTube",
        "rawData": "-",
        "DOI": "10.1109/AVSS.2018.8639160",
        "citationCount": 31,
        "completionStatus": "complete"
    },
    {
        "id": "LIBRE",
        "href": "https://sites.google.com/g.sp.m.is.nagoya-u.ac.jp/libre-dataset/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, odometry, gps/imu",
        "sensorDetail": "10 different lidars compared (VLS-128, HDL-64S2, HDL-32E, VLP-32C, VLP-16, Pandar64, Pandar40P, OS1-64, OS1-16, RS-Lidar32)",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "Lidar sensors",
        "annotations": "-",
        "licensing": "-",
        "relatedDatasets": "-",
        "publishDate": "2020-10-19",
        "lastUpdate": "-",
        "paperTitle": "LIBRE: The Multiple 3D LiDAR Dataset",
        "relatedPaper": "https://arxiv.org/pdf/2003.06129.pdf",
        "location": "Nagoya, Japan",
        "rawData": "-",
        "DOI": "10.1109/IV47402.2020.9304681",
        "citationCount": 31,
        "completionStatus": "complete"
    },
    {
        "id": "Street Learn",
        "href": "https://sites.google.com/view/streetlearn/dataset",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "143,000",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "The StreetLearn dataset is a limited set of Google Street View images",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "The dataset is freely available upon request with a license agreement",
        "relatedDatasets": "-",
        "publishDate": "2019-03-04",
        "lastUpdate": "",
        "paperTitle": "The StreetLearn Environment and Dataset",
        "relatedPaper": "https://arxiv.org/pdf/1903.01292.pdf",
        "location": "Manhattan and Pittsburgh, USA",
        "rawData": "-",
        "DOI": "-",
        "citationCount": 31,
        "completionStatus": "complete"
    },
    {
        "id": "RADIATE",
        "href": "http://pro.hw.ac.uk/radiate/",
        "size_storage": "-",
        "size_hours": "5",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, radar, gps/imu",
        "sensorDetail": "1x ZED stereo camera 672x376 15Hz, 1x Velodyne HDL-32e LiDAR 32 channel 360° 10Hz, 1x Navtech CTS350-X radar 360°, 1x Advanced Navigation Spatial Dual GPS/IMU",
        "benchmark": "-",
        "annotations": "2d bounding boxes",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International",
        "relatedDatasets": "-",
        "publishDate": "2020-10-01",
        "lastUpdate": "-",
        "relatedPaper": "https://arxiv.org/pdf/2010.09076.pdf",
        "location": "-",
        "rawData": "Yes",
        "citationCount": 30,
        "completionStatus": "partially Complete"
    },
    {
        "id": "RELLIS-3D Dataset",
        "href": "https://unmannedlab.github.io/research/RELLIS-3D",
        "size_storage": "58.1",
        "size_hours": "-",
        "frames": "13556",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu",
        "sensorDetail": "1x Nerian Karmin2 + Nerian SceneScan: 3D StereoCamera 10Hz, 1x RGB Camera: Basler acA1920-50gc camera with 16mm/F18 EDMUND Optics lens 1920x1200 10Hz, 1x Ouster OS1 LiDAR 64 Channels 10 Hz, 1x Velodyne Ultra Puck: 32 Channels 10Hz, Vectornav VN-300 Dual Antenna GNSS/INS 300Hz GPS, 100Hz IMU",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "semantic segmentation of 2d image and 3d point clouds",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License",
        "relatedDatasets": "SemanticUSL: A Dataset for LiDAR Semantic Segmentation Domain Adaptation",
        "publishDate": "2020-11-26",
        "lastUpdate": "2022-01-24",
        "paperTitle": "RELLIS-3D Dataset: Data, Benchmarks and Analysis",
        "relatedPaper": "https://arxiv.org/pdf/2011.12954.pdf",
        "location": "Rellis Campus of Texas A&M University",
        "rawData": "-",
        "DOI": "10.1109/ICRA48506.2021.9561251",
        "citationCount": 30,
        "completionStatus": "complete"
    },
    {
        "id": "DIPLECS Autonomous Driving Datasets",
        "href": "https://cvssp.org/data/diplecs/",
        "size_storage": "-",
        "size_hours": "4",
        "frames": "207364",
        "numberOfScenes": "4",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, steering angle meter, eye tracker",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Freely available for academic purposes",
        "relatedDatasets": "-",
        "publishDate": "2015-10-07",
        "lastUpdate": "-",
        "paperTitle": "How Much of Driving Is Preattentive?",
        "relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7293673",
        "location": "Sweden and United Kingdom",
        "rawData": "-",
        "DOI": "10.1109/TVT.2015.2487826",
        "citationCount": 29,
        "completionStatus": "complete"
    },
    {
        "id": "Waymo Block-NeRF",
        "href": "https://waymo.com/research/block-nerf/",
        "relatedPaper": "https://arxiv.org/abs/2202.05263",
        "citationCount": 29,
        "completionStatus": "partially Complete"
    },
    {
        "id": "PointCloudDeNoising",
        "href": "https://github.com/rheinzler/PointCloudDeNoising",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "175941",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "lidar",
        "sensorDetail": "1 Velodyne VLP32c lidar sensor",
        "recordingPerspective": "Ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "pointwise annotations",
        "licensing": "Freely available for research and teaching purposes",
        "relatedDatasets": "Gated2Depth, Gated2Gated, SeeingThroughFog",
        "publishDate": "2019-12-09",
        "lastUpdate": "-",
        "paperTitle": "CNN-based Lidar Point Cloud De-Noising in Adverse Weather",
        "relatedPaper": "https://ieeexplore.ieee.org/document/8990038",
        "location": "CEREMA's climatic chamber",
        "rawData": "-",
        "DOI": "10.1109/LRA.2020.2972865",
        "citationCount": 28,
        "completionStatus": "complete"
    },
    {
        "id": "UTBM EU LTD",
        "href": "https://epan-utbm.github.io/utbm_robocar_dataset/",
        "size_storage": "-",
        "size_hours": "2.93",
        "frames": "-",
        "numberOfScenes": "13",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, radar, gps/imu",
        "sensorDetail": "2x stereo cameras a front-facing Bumblebee XB3 and a back-facing Bumblebee2, 2x Velodyne HDL-32E lidars 360° 10Hz, 2x Pixelink PL-B742F industrial cameras 185°, 1x ibeo LUX 4L lidar 85°, 1x Continental ARS 308 radar, 1x SICK LMS100-10000 laser rangefinder 270° 12.5Hz, 1x Magellan ProFlex 500 GNSS, 1x Xsens MTi-28A53G25 IMU 100Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "lidar odometry",
        "annotations": "-",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License",
        "relatedDatasets": "-",
        "publishDate": "2020-08-01",
        "lastUpdate": "2022-06-29",
        "paperTitle": "EU Long-term Dataset with Multiple Sensors for Autonomous Driving",
        "relatedPaper": "https://arxiv.org/pdf/1909.03330.pdf",
        "location": "Montbéliard, France",
        "rawData": "Yes",
        "DOI": "10.1109/IROS45743.2020.9341406",
        "citationCount": 27,
        "completionStatus": "complete"
    },
    {
        "id": "MAVD Multimodal Audio-Visual Detection",
        "href": "http://multimodal-distill.cs.uni-freiburg.de/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "113000",
        "numberOfScenes": "24",
        "samplingRate": "-",
        "lengthOfScenes": "900",
        "sensors": "camera, lidar, microphone, gps/imu",
        "sensorDetail": "2x FLIR Blackfly 23S3C RGB camera 1920x650, 2x FLIR ADK cameras for thermal images 1920x650, 8x monophonic microphones",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "2d bounding boxes",
        "licensing": "Freely available for non commercial use only",
        "relatedDatasets": "-",
        "publishDate": "2021-04-16",
        "lastUpdate": "-",
        "paperTitle": "There is More than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking with Sound by Distilling Multimodal Knowledge",
        "relatedPaper": "https://arxiv.org/pdf/2103.01353.pdf",
        "location": "Freiburg im Breisgau and nearby towns, Germany",
        "rawData": "-",
        "DOI": "10.1109/CVPR46437.2021.01144",
        "citationCount": 26,
        "completionStatus": "complete"
    },
    {
        "id": "Talk2Car",
        "href": "https://talk2car.github.io/",
        "size_storage": "300",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "850",
        "samplingRate": "-",
        "lengthOfScenes": "20",
        "sensors": "camera, lidar, radar, gps/imu",
        "sensorDetail": "1x lidar 32 channels 360° 20Hz, 5x long range radar 13Hz, 6x camera 1600x1200 12Hz, 1x gps/imu 1000Hz",
        "recordingPerspective": "Ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "find bounding box for objects based on user commands in natural language",
        "annotations": "bounding box and command in natural language related to the bounding box",
        "licensing": "MIT license",
        "relatedDatasets": "-",
        "publishDate": "2020-03-18",
        "lastUpdate": "2021-10-06",
        "paperTitle": "Talk2Car: Taking Control of Your Self-Driving Car",
        "relatedPaper": "https://arxiv.org/pdf/1909.10838.pdf",
        "location": "Boston and Singapore",
        "rawData": "-",
        "DOI": "10.18653/v1/D19-1215",
        "citationCount": 24,
        "completionStatus": "complete"
    },
    {
        "id": "Seasonal Variation Dataset",
        "href": "http://www.cs.cmu.edu/~aayushb/localization/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, imu/gps",
        "sensorDetail": "1x Point Grey Ladybug 5 panoramic camera 2448 x 2048 10Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Publicly available for research community",
        "relatedDatasets": "Bay Area Dataset, Illumination Changes in a day",
        "publishDate": "2014-06-11",
        "lastUpdate": "-",
        "paperTitle": "Understanding How Camera Configuration and Environmental Conditions Affect Appearance-based Localization",
        "relatedPaper": "http://www.cs.cmu.edu/~aayushb/pubs/LocalizationPaperIV2014.pdf",
        "location": "California bay area and Pittsburgh, USA",
        "rawData": "Yes",
        "DOI": "10.1109/IVS.2014.6856605",
        "citationCount": 24,
        "completionStatus": "complete"
    },
    {
        "id": "Bay Area Dataset",
        "href": "http://www.cs.cmu.edu/~aayushb/localization/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, imu/gps",
        "sensorDetail": "1x Point Grey Ladybug 5 panoramic camera 2448 x 2048 10Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Available on request",
        "relatedDatasets": "Seasonal Variation Dataset, Illumination Changes in a day",
        "publishDate": "2014-06-11",
        "lastUpdate": "-",
        "paperTitle": "Understanding How Camera Configuration and Environmental Conditions Affect Appearance-based Localization",
        "relatedPaper": "http://www.cs.cmu.edu/~aayushb/pubs/LocalizationPaperIV2014.pdf",
        "location": "California bay area and Pittsburgh, USA",
        "rawData": "Yes",
        "DOI": "10.1109/IVS.2014.6856605",
        "citationCount": 24,
        "completionStatus": "complete"
    },
    {
        "id": "Illumination Changes in a day",
        "href": "http://www.cs.cmu.edu/~aayushb/localization/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, imu/gps",
        "sensorDetail": "1x Point Grey Ladybug 5 panoramic camera 2448 x 2048 10Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Available on request",
        "relatedDatasets": "Bay Area Dataset, Seasonal Variation Dataset",
        "publishDate": "2014-06-11",
        "lastUpdate": "-",
        "paperTitle": "Understanding How Camera Configuration and Environmental Conditions Affect Appearance-based Localization",
        "relatedPaper": "http://www.cs.cmu.edu/~aayushb/pubs/LocalizationPaperIV2014.pdf",
        "location": "California bay area and Pittsburgh, USA",
        "rawData": "Yes",
        "DOI": "10.1109/IVS.2014.6856605",
        "citationCount": 24,
        "completionStatus": "complete"
    },
    {
        "id": "HAD",
        "href": "https://usa.honda-ri.com/had",
        "size_storage": "-",
        "size_hours": "32",
        "frames": "-",
        "numberOfScenes": "5675",
        "samplingRate": "10",
        "lengthOfScenes": "20",
        "sensors": "camera",
        "sensorDetail": "Videos obtained from the larger HDD dataset from the same research group",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "Human voice describing action and attention of the driver seperately",
        "licensing": "Available for research for people affiliated with a university upon registration",
        "relatedDatasets": "HDBD, HDD, HEV-I, HSD, LOKI, TITAN and EPOSH",
        "publishDate": "2019-06-15",
        "lastUpdate": "-",
        "paperTitle": "Grounding Human-To-Vehicle Advice for Self-Driving Vehicles",
        "relatedPaper": "https://ieeexplore.ieee.org/document/8953562",
        "location": "San Francisco Bay Area, USA",
        "rawData": "-",
        "DOI": "10.1109/CVPR.2019.01084",
        "citationCount": 24,
        "completionStatus": "complete"
    },
    {
        "id": "DDD 20",
        "href": "https://sites.google.com/view/davis-driving-dataset-2020/home",
        "size_storage": "1300",
        "size_hours": "51",
        "frames": "-",
        "numberOfScenes": "216",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, car parameters",
        "sensorDetail": "1x DAVIS346B 346x260 up to 50Hz, vehicle bus data",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Creative Commons Attribution-ShareAlike 4.0 International",
        "relatedDatasets": "DDD 17",
        "publishDate": "2020-02-01",
        "lastUpdate": "-",
        "relatedPaper": "https://arxiv.org/pdf/2005.08605.pdf",
        "location": "California, USA",
        "rawData": "Yes",
        "citationCount": 23,
        "completionStatus": "partially Complete"
    },
    {
        "id": "DDD20: DAVIS Driving Dataset 2020",
        "href": "https://docs.google.com/document/d/1HM0CSmjO8nOpUeTvmPjopcBcVCk7KXvLUuiZFS6TWSg/pub",
        "size_storage": "1300",
        "size_hours": "51",
        "frames": "-",
        "numberOfScenes": "216",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x 346x260-pixel DAVIS346 camera 56°",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Creative Commons Attribution-ShareAlike 4.0 International License",
        "relatedDatasets": "DDD17",
        "publishDate": "2017-06-05",
        "lastUpdate": "2020-02-01",
        "paperTitle": "DDD20 End-to-End Event Camera Driving Dataset: Fusing Frames and Events with Deep Learning for Improved Steering Prediction",
        "relatedPaper": "https://arxiv.org/pdf/2005.08605.pdf",
        "location": "Various states of USA, Switzerland and Germany",
        "rawData": "Yes",
        "DOI": "10.1109/ITSC45102.2020.9294515",
        "citationCount": 23,
        "completionStatus": "complete"
    },
    {
        "id": "BLVD",
        "href": "https://github.com/VCCIV/BLVD",
        "size_storage": "42.7",
        "size_hours": "-",
        "frames": "120000",
        "numberOfScenes": "654",
        "samplingRate": "10",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu",
        "sensorDetail": "2x multi-view color cameras 1920x500 30Hz, 1x Velodyne HDL-64E lidar 10Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "dynamic 4D (3D+temporal) tracking, 5D (4D+interactive) interactive event recognition and intention prediction",
        "annotations": "5D semantics, 3D bounding boxes, 4D object IDs, 5D interactive event and 5D intention",
        "licensing": "-",
        "relatedDatasets": "-",
        "publishDate": "2019-05-20",
        "lastUpdate": "-",
        "paperTitle": "BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous Driving",
        "relatedPaper": "https://ieeexplore.ieee.org/document/8793523",
        "location": "Changshu, Jiangsu Province, China",
        "rawData": "-",
        "DOI": "10.1109/ICRA.2019.8793523",
        "citationCount": 23,
        "completionStatus": "complete"
    },
    {
        "id": "Gated2Depth",
        "href": "https://github.com/gruberto/Gated2Depth",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "17686",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar",
        "sensorDetail": "1x Aptina AR0230 stereo camera 1920x1080 30Hz, 1x Velodyne HDL64-S3 lidar, 1x gated camera 10bit images 1280x720 120Hz",
        "recordingPerspective": "Ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Freely available for research and teaching purposes",
        "relatedDatasets": "SeeingThroughFog, PointCloudDeNoising, Gated2Gated",
        "publishDate": "2020-02-13",
        "lastUpdate": "2020-04-12",
        "paperTitle": "Gated2Depth: Real-Time Dense Lidar From Gated Images",
        "relatedPaper": "https://arxiv.org/pdf/1902.04997.pdf",
        "location": "Germany, Denmark and Sweden",
        "rawData": "-",
        "DOI": "10.1109/ICCV.2019.00159",
        "citationCount": 23,
        "completionStatus": "complete"
    },
    {
        "id": "CARRADA Dataset",
        "href": "https://github.com/valeoai/carrada_dataset",
        "size_storage": "288",
        "size_hours": "-",
        "frames": "12666",
        "numberOfScenes": "30",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, radar",
        "sensorDetail": "1x camera 1238x1024,  1x radar 180°",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "range-angle Doppler annotations",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (CC BY-NC-SA 4.0)",
        "relatedDatasets": "-",
        "publishDate": "2020-05-04",
        "lastUpdate": "2021-07",
        "paperTitle": "CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations",
        "relatedPaper": "https://arxiv.org/pdf/2005.01456.pdf",
        "location": "-",
        "rawData": "-",
        "DOI": "10.1109/ICPR48806.2021.9413181",
        "citationCount": 22,
        "completionStatus": "complete"
    },
    {
        "id": "4Seasons",
        "href": "https://www.4seasons-dataset.com/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "30",
        "samplingRate": "30",
        "lengthOfScenes": "-",
        "sensors": "camera, imu/rtk-gnss",
        "sensorDetail": "2x cameras stereo baseline 30cm 800x400 (after cropping)",
        "benchmark": "globally consistent reference poses",
        "annotations": "-",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",
        "relatedDatasets": "-",
        "publishDate": "2020-10-01",
        "lastUpdate": "-",
        "relatedPaper": "https://arxiv.org/pdf/2009.06364.pdf",
        "location": "-",
        "rawData": "Yes",
        "citationCount": 21,
        "completionStatus": "partially Complete"
    },
    {
        "id": "DriveU Traffic Light Dataset",
        "href": "https://www.uni-ulm.de/en/in/driveu/projects/driveu-traffic-light-dataset/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "15",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x stereo camera 60°, 1x stereo camera 130°",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "2d bounding boxes",
        "licensing": "freely available for non-commercial research and teaching activities",
        "relatedDatasets": "-",
        "publishDate": "2018-11-27",
        "lastUpdate": "2021-04",
        "paperTitle": "The DriveU Traffic Light Dataset: Introduction and Comparison with Existing Datasets",
        "relatedPaper": "https://www.researchgate.net/profile/Julian-Mueller-14/publication/327808220_The_DriveU_Traffic_Light_Dataset_Introduction_and_Comparison_with_Existing_Datasets/links/5c1910e4a6fdccfc7056b787/The-DriveU-Traffic-Light-Dataset-Introduction-and-Comparison-with-Existing-Datasets.pdf ",
        "location": "10 cities across Germany",
        "rawData": "-",
        "DOI": "10.1109/ICRA.2018.8460737",
        "citationCount": 20,
        "completionStatus": "complete"
    },
    {
        "id": "Unsupervised Llamas",
        "href": "https://unsupervised-llamas.com/llamas/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "100042",
        "numberOfScenes": "14",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "binary lane marker segmentation, lane-dependent pixel-level segmentation and lane border regression",
        "annotations": "dashed lane markings",
        "licensing": "Freely available for non-commercial reseach purposes only",
        "relatedDatasets": "-",
        "publishDate": "2019-10-27",
        "lastUpdate": "-",
        "paperTitle": "Unsupervised Labeled Lane Markers Using Maps",
        "relatedPaper": "https://unsupervised-llamas.com/static/llamas/llamas_preview.pdf",
        "location": "-",
        "rawData": "-",
        "DOI": "10.1109/ICCVW.2019.00111",
        "citationCount": 20,
        "completionStatus": "complete"
    },
    {
        "id": "Cityscapes 3D",
        "href": "https://www.cityscapes-dataset.com/",
        "size_hours": "-",
        "size_storage": "63.141",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "17",
        "lengthOfScenes": "1.8",
        "sensors": "camera, gps, thermometer",
        "sensorDetail": "stereo cameras 22 cm baseline 17Hz, odometry from in-vehicle \"sensors\" & outs\"id\"e temperature & GPS tracks",
        "benchmark": "pixel-level semantic labeling, instance-level semantic labeling, panoptic semantic sabeling 3d vehicle detection",
        "annotations": "dense semantic segmentation, instance segmentation for vehicles & people, 3d bounding boxes",
        "licensing": "freely available for non-commercial purposes",
        "relatedDatasets": "-",
        "publishDate": "2016-02-01",
        "lastUpdate": "2020-10-01",
        "relatedPaper": "https://arxiv.org/pdf/2006.07864.pdf",
        "location": "50 cities in Germany and neighboring countries",
        "rawData": "Yes",
        "citationCount": 19,
        "completionStatus": "partially Complete"
    },
    {
        "id": "MTSD",
        "href": "https://www.mapillary.com/dataset/trafficsign",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "105830",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Map",
        "benchmark": "-",
        "annotations": "2d bounding boxes and sign classification",
        "licensing": "Creative Commons Attribution NonCommercial Share Alike (CC BY-NC-SA)",
        "relatedDatasets": "Mapillary Vistas",
        "publishDate": "2020-11-03",
        "lastUpdate": "-",
        "paperTitle": "The Mapillary Traffic Sign Dataset for Detection and Classification on a Global Scale",
        "relatedPaper": "https://link.springer.com/chapter/10.1007/978-3-030-58592-1_5",
        "location": "Europe, North and South America, Asia, Africa and Oceania",
        "rawData": "-",
        "DOI": "10.1007/978-3-030-58592-1_5",
        "citationCount": 19,
        "completionStatus": "complete"
    },
    {
        "id": "RANUS",
        "href": "https://sites.google.com/site/gmchoe1/ranus",
        "size_storage": "11.4",
        "size_hours": "-",
        "frames": "40000",
        "numberOfScenes": "50",
        "samplingRate": "10",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "2x Point-grey grasshopper cameras (NIR: GS3-U3-41C6NIR-C, RGB: GS3-U3-41C6C-C) 10Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "pixel level semantic segmentation masks",
        "licensing": "-",
        "relatedDatasets": "-",
        "publishDate": "2018-02-02",
        "lastUpdate": "-",
        "paperTitle": "RANUS: RGB and NIR Urban Scene Dataset for Deep Scene Parsing",
        "relatedPaper": "https://joonyoung-cv.github.io/assets/paper/18_ral_ranus.pdf",
        "location": "-",
        "rawData": "-",
        "DOI": "10.1109/LRA.2018.2801390",
        "citationCount": 19,
        "completionStatus": "complete"
    },
    {
        "id": "RoadAnomaly21",
        "href": "https://segmentmeifyoucan.com/datasets",
        "size_storage": "0.05",
        "size_hours": "-",
        "frames": "100",
        "numberOfScenes": "100",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "images from web resources 2048x1024 & 1280x720",
        "benchmark": "anomaly detection",
        "annotations": "semantic segmentation",
        "licensing": "various, see \"https://github.com/SegmentMeIfYouCan/road-anomaly-\"benchmark\"/blob/master/doc/RoadAnomaly/credits.txt\" for detail",
        "relatedDatasets": "RoadObstacle21",
        "publishDate": "2021-04-01",
        "lastUpdate": "-",
        "relatedPaper": "https://arxiv.org/pdf/2104.14812.pdf",
        "location": "-",
        "rawData": "Yes",
        "citationCount": 18,
        "completionStatus": "partially Complete"
    },
    {
        "id": "AMUSE",
        "href": "http://www.cvl.isy.liu.se/research/datasets/amuse/",
        "size_storage": "1263",
        "size_hours": "-",
        "frames": "117440",
        "numberOfScenes": "7",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, height, gps/imu, accelerometer",
        "sensorDetail": "1x Point Grey Ladybug 3^2 camera, 6x synchronized global shutter color cameras 616x1616 360° 30Hz, 1x XSens MTi AHRS3 imu sensor, 1x uBlox AEK-4P GPS sensor, 1x Kistler Correvit S-350 Aqua velocity sensor, 1x Kistler HF-500C Height sensor",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported License",
        "relatedDatasets": "-",
        "publishDate": "2013-06-23",
        "lastUpdate": "-",
        "paperTitle": "A Multi-sensor Traffic Scene Dataset with Omnidirectional Video",
        "relatedPaper": "http://liu.diva-portal.org/smash/get/diva2:623885/FULLTEXT01.pdf",
        "location": "Sweden",
        "rawData": "-",
        "DOI": "10.1109/CVPRW.2013.110",
        "citationCount": 18,
        "completionStatus": "complete"
    },
    {
        "id": "MOTSynth",
        "href": "https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=42",
        "size_storage": "-",
        "size_hours": "16",
        "frames": "1375200",
        "numberOfScenes": "764",
        "samplingRate": "20",
        "lengthOfScenes": "90",
        "sensors": "camera",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Synthetic",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "bounding boxes and pose, segmentation masks, depth maps",
        "licensing": "Free to use",
        "relatedDatasets": "-",
        "publishDate": "2021-10-10",
        "lastUpdate": "-",
        "paperTitle": "MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?",
        "relatedPaper": "https://arxiv.org/pdf/2108.09518.pdf",
        "location": "Grand Theft Auto V game",
        "rawData": "-",
        "DOI": "10.1109/ICCV48922.2021.01067",
        "citationCount": 18,
        "completionStatus": "complete"
    },
    {
        "id": "PREVENTION",
        "href": "https://prevention-dataset.uah.es/",
        "relatedPaper": "https://ieeexplore.ieee.org/document/8917433",
        "DOI": "10.1109/ITSC.2019.8917433",
        "citationCount": 17,
        "completionStatus": "partially Complete"
    },
    {
        "id": "IDDA",
        "href": "https://idda-dataset.github.io/home/",
        "size_storage": "1000",
        "size_hours": "-",
        "frames": "1006800",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, depth sensor, semantic segmentation sensor",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Synthetic",
        "mapData": "-",
        "benchmark": "semantic segmentation",
        "annotations": "semantic segmentation",
        "licensing": "Available upon registration",
        "relatedDatasets": "-",
        "publishDate": "2020-07-14",
        "lastUpdate": "2022-05-10",
        "paperTitle": "IDDA: A Large-Scale Multi-Domain Dataset for Autonomous Driving",
        "relatedPaper": "https://arxiv.org/pdf/2004.08298.pdf",
        "location": "CARLA",
        "rawData": "Yes",
        "DOI": "10.1109/LRA.2020.3009075",
        "citationCount": 17,
        "completionStatus": "complete"
    },
    {
        "id": "RadarScenes",
        "href": "https://radar-scenes.com/",
        "size_storage": "-",
        "size_hours": "4",
        "frames": "-",
        "numberOfScenes": "158",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, radar, odometry",
        "sensorDetail": "4x 77 GHz series production automotive 60° radar sensor, 1x documentary camera",
        "benchmark": "-",
        "annotations": "point-wise",
        "licensing": "Creative Commons Attribution Non Commercial Share Alike 4.0 International",
        "relatedDatasets": "-",
        "publishDate": "2021-03-01",
        "lastUpdate": "-",
        "relatedPaper": "https://arxiv.org/pdf/2104.02493.pdf",
        "location": "-",
        "rawData": "Yes",
        "citationCount": 14,
        "completionStatus": "partially Complete"
    },
    {
        "id": "TAF-BW",
        "href": "https://github.com/fzi-forschungszentrum-informatik/test-area-autonomous-driving-dataset",
        "size_storage": "-",
        "size_hours": "0.1",
        "frames": "-",
        "numberOfScenes": "2",
        "samplingRate": "10",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "traffic camera",
        "recordingPerspective": "Bird's Eye",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "tracking id and bounding box",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License",
        "relatedDatasets": "-",
        "publishDate": "2018-12-31",
        "lastUpdate": "-",
        "paperTitle": "Towards Large Scale Urban Traffic Reference Data: Smart Infrastructure in the Test Area Autonomous Driving Baden-Wurttemberg",
        "relatedPaper": "https://www.researchgate.net/profile/Tobias-Fleck/publication/327449884_Towards_Large_Scale_Urban_Traffic_Reference_Data_Smart_Infrastructure_in_the_Test_Area_Autonomous_Driving_Baden-Wurttemberg/links/5bbcbddd299bf1049b785126/Towards-Large-Scale-Urban-Traffic-Reference-Data-Smart-Infrastructure-in-the-Test-Area-Autonomous-Driving-Baden-Wuerttemberg.pdf",
        "location": " Karlsruhe, Germany",
        "rawData": "",
        "DOI": "10.1007/978-3-030-01370-7_75",
        "citationCount": 14,
        "completionStatus": "complete"
    },
    {
        "id": "Boxy",
        "href": "https://boxy-dataset.com/boxy/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "200000",
        "numberOfScenes": "34",
        "samplingRate": "15",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x mvBlueFOX3-2051 with a Sony IMX250 chip 2464x2056 15Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "bounding box, polygon, and real-time detections",
        "annotations": "2d boxes or 3d cuboids for vehicles",
        "licensing": "Freely available for non-commercial purposes",
        "relatedDatasets": "-",
        "publishDate": "2019-10-27",
        "lastUpdate": "-",
        "paperTitle": "Boxy Vehicle Detection in Large Images",
        "relatedPaper": "https://boxy-dataset.com/static/boxy/boxy_preview.pdf",
        "location": "San Francisco Bay Area, California, USA",
        "rawData": "-",
        "DOI": "10.1109/ICCVW.2019.00112",
        "citationCount": 13,
        "completionStatus": "complete"
    },
    {
        "id": "KITTI-360 PanopticBEV",
        "href": "http://panoptic-bev.cs.uni-freiburg.de/",
        "relatedPaper": "https://ieeexplore.ieee.org/document/9681287",
        "DOI": "10.1109/LRA.2022.3142418",
        "citationCount": 11,
        "completionStatus": "partially Complete"
    },
    {
        "id": "HSD",
        "href": "https://usa.honda-ri.com/hsd",
        "size_storage": "60",
        "size_hours": "80",
        "frames": "-",
        "numberOfScenes": "20000",
        "samplingRate": "30",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "2x Pointgrey Grasshopper (80FOV), 1x Grasshopper (100FOV)",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "road places, road types, weather, and road surface conditions",
        "licensing": "Available for research for people affiliated with a university upon registration",
        "relatedDatasets": "HDBD, HAD, HDD, HEV-I, LOKI, TITAN and EPOSH",
        "publishDate": "2019-05-01",
        "lastUpdate": "-",
        "paperTitle": "Dynamic Traffic Scene Classification with Space-Time Coherence",
        "relatedPaper": "https://arxiv.org/pdf/1905.12708.pdf",
        "location": "San Francisco Bay area, USA",
        "rawData": "Yes",
        "DOI": "10.1109/ICRA.2019.8794137",
        "citationCount": 11,
        "completionStatus": "complete"
    },
    {
        "id": "openDD",
        "href": "https://l3pilot.eu/data/opendd",
        "size_storage": "-",
        "size_hours": "62.7",
        "frames": "6771600",
        "numberOfScenes": "501",
        "samplingRate": "30",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "DJI Phantom 4 3840×2160 camera drone",
        "benchmark": "trajectory predictions",
        "annotations": "2d bounding boxes, trajectories",
        "licensing": "Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) ",
        "relatedDatasets": "-",
        "publishData": "2020-09-01",
        "lastUpdate": "-",
        "relatedPaper": "https://arxiv.org/pdf/2007.08463.pdf",
        "location": "Wolfsburg and Ingolstadt, Germany",
        "rawData": "Yes",
        "citationCount": 10,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Semantic KITTI",
        "href": "http://www.semantic-kitti.org/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "43552",
        "numberOfScenes": "21",
        "samplingRate": "10",
        "lengthOfScenes": "-",
        "sensors": "lidar",
        "sensorDetail": "Velodyne HDL-64E from sequences of the odometry 'benchmark' of the KITTI Vision Benchmark with 360° view",
        "benchmark": "semantic segmentation, panoptic segmentation, 4D panoptic segmentation, moving object segmentation, semantic scene completion",
        "annotations": "semantic segmentation",
        "licensing": "Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) ",
        "relatedDatasets": "KITTI",
        "publishDate": "2019-07-01",
        "lastUpdate": "2021-02-01",
        "relatedPaper": "https://arxiv.org/abs/1904.01416.pdf",
        "location": "Karlsruhe, Germany",
        "rawData": "No",
        "citationCount": 9,
        "completionStatus": "partially Complete"
    },
    {
        "id": "MCity Data Collection",
        "href": "https://arxiv.org/pdf/1912.06258.pdf",
        "size_storage": "11000",
        "size_hours": "50",
        "frames": "-",
        "numberOfScenes": "255",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, radar, gps/imu",
        "sensorDetail": "3x Velodyne Ultra Puck VLP-32C lidar 10Hz, 2x forward-facing cameras 30° 1080P 30Hz,1x backward-facing camera 90° 1080P 30Hz, 1x cabin pose camera 1280×1080 30Hz, 1x cabin head/eyeball camera 640P 30Hz, 1x Ibeo four beam LUX sensor 25Hz, 1x Delphi ESR 2.5 Radar 90° 20Hz,1x NovAtel FlexPak6 with IMU-IGM-S1 and 4G cellular for RTK GPS single antenna 1Hz",
        "benchmark": "-",
        "annotations": "semantic segmentation of objects, traffic lights, traffic signs, lanes",
        "licensing": "-",
        "relatedDatasets": "-",
        "publishDate": "2019-12-01",
        "lastUpdate": "-",
        "relatedPaper": "https://arxiv.org/pdf/1912.06258.pdf",
        "location": "Ann Arbor, USA",
        "rawData": "Yes",
        "citationCount": 9,
        "completionStatus": "partially Complete"
    },
    {
        "id": "nuPlan",
        "href": "https://arxiv.org/abs/2106.11810",
        "size_storage": "-",
        "size_hours": "1500",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu",
        "sensorDetail": "8x cameras 2000x1200 10Hz, 5x lidar 20Hz, 1x imu 100Hz, 1x gnss 20Hz",
        "recordingPerspective": "-",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "autonomous vehicle planning",
        "annotations": "2d/3d bounding boxes",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (CC BY-NC-SA 4.0)",
        "relatedDatasets": "-",
        "publishDate": "2021-12-10",
        "lastUpdate": "-",
        "paperTitle": "nuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles",
        "relatedPaper": "https://arxiv.org/abs/2106.11810",
        "location": "Boston, Pittsburgh, Las Vegas and Singapore",
        "rawData": "-",
        "DOI": "10.48550/arXiv.2106.11810",
        "citationCount": 9,
        "completionStatus": "complete"
    },
    {
        "id": "ROAD",
        "href": "https://github.com/gurkirt/road-dataset",
        "size_hours": "2.83",
        "size_storage": "-",
        "frames": "122000",
        "numberOfScenes": "22",
        "samplingRate": "12",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "benchmark": "agent detection, action detection and road event detection",
        "annotations": "2d/3d bounding boxes, action label and location labels",
        "licensing": "Creative Commons Attribution Share Alike 4.0 International",
        "relatedDatasets": "Oxford Robot Car Dataset (OxRD)",
        "publishDate": "-",
        "lastUpdate": "-",
        "paperTitle": "ROAD: The ROad event Awareness Dataset for Autonomous Driving",
        "relatedPaper": "https://www.computer.org/csdl/api/v1/periodical/trans/tp/5555/01/09712346/1AZL0P4dL1e/download-article/pdf",
        "location": "Oxford, UK",
        "rawData": "-",
        "DOI": "10.1109/TPAMI.2022.3150906",
        "citationCount": 6,
        "completionStatus": "complete"
    },
    {
        "id": "TRoM",
        "href": "http://www.tromai.icoc.me/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "712",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, gps",
        "sensorDetail": "1x PointGray color camera 1280x960",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "Road marking detection",
        "annotations": "Pixel level road markings",
        "licensing": "Freely available for academic use",
        "relatedDatasets": "-",
        "publishDate": "2017-10-16",
        "lastUpdate": "-",
        "paperTitle": "Benchmark for road marking detection: Dataset specification and performance baseline",
        "relatedPaper": "https://ieeexplore.ieee.org/document/8317749",
        "location": "Beijing, China",
        "rawData": "-",
        "DOI": "10.1109/ITSC.2017.8317749",
        "citationCount": 6,
        "completionStatus": "complete"
    },
    {
        "id": "Small Obstacle",
        "href": "https://small-obstacle-dataset.github.io/",
        "size_storage": "10.6",
        "size_hours": "-",
        "frames": "2927",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar",
        "sensorDetail": "1x ZED Stereo camera, 1x Velodyne Puck (VLP-16) lidar",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real and Synthetic",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "pixel wise semantic segmentation",
        "licensing": "-",
        "relatedDatasets": "-",
        "publishDate": "2020-03-12",
        "lastUpdate": "-",
        "paperTitle": "LiDAR guided Small obstacle Segmentation",
        "relatedPaper": "https://arxiv.org/pdf/2003.05970.pdf",
        "location": "India",
        "rawData": "-",
        "DOI": "10.48550/arXiv.2003.05970",
        "citationCount": 6,
        "completionStatus": "complete"
    },
    {
        "id": "PepScenes",
        "href": "https://github.com/huawei-noah/PePScenes",
        "relatedPaper": "https://arxiv.org/pdf/2012.07773.pdf",
        "citationCount": 5,
        "completionStatus": "partially Complete"
    },
    {
        "id": "The Autonomous Platform Inertial Dataset",
        "href": "https://github.com/ansfl/Navigation-Data-Project/",
        "relatedPaper": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9684368",
        "DOI": "10.1109/ACCESS.2022.3144076",
        "citationCount": 5,
        "completionStatus": "partially Complete"
    },
    {
        "id": "LOKI",
        "href": "https://usa.honda-ri.com/loki",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "644",
        "samplingRate": "5",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps",
        "sensorDetail": "1x color SEKONIX SF332X-10X video camera 1928x1280 30Hz, 4x Velodyne VLP-32C 3D LiDARs 10Hz, 1x MTi-G-710-GNSS/INS-2A8G4 gyroscope/gps",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "driver intention and vehicle trajectory prediction",
        "annotations": "2d/3d bounding boxes, driver intention labels, environmental labels, contextual labels",
        "licensing": "Available for research for people affiliated with a university upon registration",
        "relatedDatasets": "HDBD, HAD, HDD, HEV-I, HSD, TITAN and EPOSH",
        "publishDate": "2021-10-10",
        "lastUpdate": "-",
        "paperTitle": "LOKI: Long Term and Key Intentions for Trajectory Prediction",
        "relatedPaper": "https://arxiv.org/pdf/2108.08236.pdf",
        "location": "Tokyo, Japan",
        "rawData": "-",
        "DOI": "10.1109/ICCV48922.2021.00966",
        "citationCount": 5,
        "completionStatus": "complete"
    },
    {
        "id": "The USyd Campus Dataset",
        "href": "http://its.acfr.usyd.edu.au/datasets/usyd-campus-dataset/",
        "size_storage": "-",
        "size_hours": "40",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps, imu, wheel encoders",
        "sensorDetail": "6x NVIDIA 2Mega SF3322 automotive GMSL cameras 1928x1208 30Hz , 1x 3D Velodyne Puck VLP-16 10 Hz 360°, 1x VN-100 IMU, 1x U-Blox NEO-M8P real-time kinematics GNSS",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "Semantic Segmentation, Road class label",
        "licensing": "Freely available for non-commercial purposes",
        "relatedDatasets": "Five Roundabouts Dataset,  Naturalistic Intersection Driving Dataset, High Resolution Fused GPS and Dead Reckoning",
        "publishDate": "2020-06-05",
        "lastUpdate": "-",
        "paperTitle": "Developing and Testing Robust Autonomy: The University of Sydney Campus Data Set",
        "relatedPaper": "https://ieeexplore.ieee.org/document/9109704",
        "location": "Sydney, Australia",
        "rawData": "-",
        "DOI": "10.1109/MITS.2020.2990183",
        "citationCount": 4,
        "completionStatus": "complete"
    },
    {
        "id": "SODA10M",
        "href": "https://soda-2d.github.io/",
        "size_storage": "2005.6",
        "size_hours": "27833",
        "frames": "10000000",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "Mobile cameras with recording applications 1920x1080 resolution",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "self-supervised learning for dataset labelling",
        "annotations": "Image tags (i.e., weather conditions, location scenes, periods) for all images and 2D bounding boxes",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (“CC BY-NC-SA 4.0”)",
        "relatedDatasets": "-",
        "publishDate": "2021-06-08",
        "lastUpdate": "2021-11-09",
        "paperTitle": "SODA10M: A Large-Scale 2D Self/Semi-Supervised Object Detection Dataset for Autonomous Driving",
        "relatedPaper": "https://arxiv.org/pdf/2106.11118.pdf",
        "location": "32 cities across China",
        "rawData": "Yes",
        "DOI": "10.48550/arXiv.2106.11118",
        "citationCount": 4,
        "completionStatus": "complete"
    },
    {
        "id": "A9",
        "href": "https://innovation-mobility.com/en/a9-dataset/",
        "relatedPaper": "https://arxiv.org/pdf/2204.06527.pdf",
        "citationCount": 4,
        "completionStatus": "partially Complete"
    },
    {
        "id": "KITTI-360-APS",
        "href": "http://amodal-panoptic.cs.uni-freiburg.de/",
        "relatedPaper": "https://arxiv.org/pdf/2202.11542.pdf",
        "citationCount": 4,
        "completionStatus": "partially Complete"
    },
    {
        "id": "BDD100K-APS",
        "href": "http://amodal-panoptic.cs.uni-freiburg.de/",
        "relatedPaper": "https://arxiv.org/pdf/2202.11542.pdf",
        "citationCount": 4,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Boreas",
        "href": "https://www.boreas.utias.utoronto.ca/#/",
        "relatedPaper": "https://arxiv.org/pdf/2203.10168.pdf",
        "citationCount": 4,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Cooperative Driving Dataset (CODD)",
        "href": "https://github.com/eduardohenriquearnold/CODD",
        "size_hours": "-",
        "size_storage": "16.7",
        "frames": "13500",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "lidar",
        "sensorDetail": "-",
        "recordingPerspective": "Top Shot",
        "dataType": "Real",
        "benchmark": "-",
        "annotations": "3d bounding boxes",
        "licensing": "Creative Commons Attribution Share Alike 4.0 International",
        "relatedDatasets": "-",
        "publishDate": "2021-11-23",
        "lastUpdate": "-",
        "paperTitle": "Fast and Robust Registration of Partially Overlapping Point Clouds",
        "relatedPaper": "https://arxiv.org/pdf/2112.09922.pdf",
        "location": "CARLA environment",
        "rawData": "-",
        "DOI": "10.1109/LRA.2021.3137888",
        "citationCount": 3,
        "completionStatus": "complete"
    },
    {
        "id": "SHIFT",
        "href": "https://www.vis.xyz/shift/",
        "relatedPaper": "https://arxiv.org/abs/2206.08367",
        "citationCount": 3,
        "completionStatus": "partially Complete"
    },
    {
        "id": "CODA",
        "href": "https://coda-dataset.github.io/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "10000",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "The dataset has been constructed from KITTI, nuScenes, SODA10M and ONCE datasets",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "anomaly detection, object detection",
        "annotations": "Image domain tags (i.e., periods and weather conditions) and 2D bounding boxes",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (“CC BY-NC-SA 4.0”)",
        "relatedDatasets": "-",
        "publishDate": "2021-11-24",
        "lastUpdate": "2022-08-01",
        "paperTitle": "CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving",
        "relatedPaper": "https://arxiv.org/pdf/2203.07724.pdf",
        "location": "-",
        "rawData": "-",
        "DOI": "10.48550/arXiv.2203.07724",
        "citationCount": 3,
        "completionStatus": "complete"
    },
    {
        "id": "DGL-MOTS",
        "href": "https://goodproj13.github.io/DGL-MOTS/",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "12000",
        "numberOfScenes": "40",
        "samplingRate": "17",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x GoPro HERO8 17Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "pixel-level mask and temporal tracking label across frames for objects ",
        "licensing": "-",
        "relatedDatasets": "-",
        "publishDate": "2019-06-15",
        "lastUpdate": "-",
        "paperTitle": "MOTS: Multi-Object Tracking and Segmentation",
        "relatedPaper": "https://arxiv.org/pdf/2110.07790.pdf",
        "location": "USA",
        "rawData": "Yes",
        "DOI": "10.1109/CVPR.2019.00813",
        "citationCount": 2,
        "completionStatus": "complete"
    },
    {
        "id": "DRIV100",
        "href": "https://zenodo.org/record/4389243#.YnvlruhBxD8",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "100",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "Domain adaptation techniques on in-the-wild road-scene videos collected from the Internet",
        "annotations": "pixel level semantic segmentation",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License",
        "relatedDatasets": "-",
        "publishDate": "2021-01-30",
        "lastUpdate": "-",
        "paperTitle": "DRIV100: In-The-Wild Multi-Domain Dataset and Evaluation for Real-World Domain Adaptation of Semantic Segmentation",
        "relatedPaper": "https://arxiv.org/pdf/2102.00150.pdf",
        "location": "Random videos from YouTube",
        "rawData": "-",
        "DOI": "10.5281/zenodo.4389243",
        "citationCount": 1,
        "completionStatus": "complete"
    },
    {
        "id": "MIT-AVT Clustered Driving Scene Dataset",
        "href": "https://ieeexplore.ieee.org/abstract/document/9304677/",
        "size_storage": "4000",
        "size_hours": "3212",
        "frames": "-",
        "numberOfScenes": "1156592",
        "samplingRate": "30",
        "lengthOfScenes": "10",
        "sensors": "camera, imu, gps",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "Seasons, Weather, Lanes, Illuminnation, Simplified Road Type, Others",
        "licensing": "Not released publically",
        "relatedDatasets": "-",
        "publishDate": "2020-11-13",
        "lastUpdate": "-",
        "paperTitle": "MIT-AVT Clustered Driving Scene Dataset: Evaluating Perception Systems in Real-World Naturalistic Driving Scenarios",
        "relatedPaper": "https://ieeexplore.ieee.org/abstract/document/9304677/",
        "location": "Many states across the USA",
        "rawData": "Yes",
        "DOI": "10.1109/IV47402.2020.9304677",
        "citationCount": 1,
        "completionStatus": "complete"
    },
    {
        "id": "Amodal Cityscapes",
        "href": "https://github.com/ifnspaml/AmodalCityscapes",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "3472",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "Dataset taken from Cityscapes and occlusions pasted artficially for amodal segmentation tasks",
        "recordingPerspective": "ego-perspective",
        "dataType": "-",
        "mapData": "No",
        "benchmark": "amodal semantic segmentation method",
        "annotations": "semantic segmentation and amodal semantic segmentation",
        "licensing": "Freely available code to generate the dataset",
        "relatedDatasets": "-",
        "publishDate": "2022-06-01",
        "lastUpdate": "-",
        "paperTitle": "Amodal Cityscapes: A New Dataset, its Generation, and an Amodal Semantic Segmentation Challenge Baseline",
        "relatedPaper": "https://arxiv.org/pdf/2206.00527.pdf",
        "location": "-",
        "rawData": "-",
        "DOI": "10.48550/arXiv.2206.00527",
        "citationCount": 1,
        "completionStatus": "complete"
    },
    {
        "id": "ScribbleKITTI",
        "href": "https://github.com/ouenal/scribblekitti",
        "relatedPaper": "https://arxiv.org/abs/2203.08537",
        "citationCount": 1,
        "completionStatus": "partially Complete"
    },
    {
        "id": "GROUNDED",
        "href": "https://lgprdata.com/",
        "size_storage": "750",
        "size_hours": "12",
        "frames": "-",
        "numberOfScenes": "108",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu, odometer, radar",
        "sensorDetail": "1x Point Grey Grasshopper camera 1928x1448 6Hz, 1x Velodyne HDL-64 lidar 360° 10Hz, 1x OXTS RT3003 INS gps, 1x Localizing Ground Penetrating Radar (LGPR) Sensor components include a 12 element radar array, a switch matrix, an OXTS RTK-GPS unit 126Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "Localization in weather, Multi-lane mapping",
        "annotations": "-",
        "licensing": "-",
        "relatedDatasets": "-",
        "publishDate": "2021-06-12",
        "lastUpdate": "-",
        "paperTitle": "GROUNDED: The Localizing Ground Penetrating Radar Evaluation Dataset",
        "relatedPaper": "http://www.roboticsproceedings.org/rss17/p080.pdf",
        "location": "Massachusetts, USA",
        "rawData": "-",
        "DOI": "10.15607/RSS.2021.XVII.080",
        "citationCount": 1,
        "completionStatus": "complete"
    },
    {
        "id": "Automatum Dataset (Open Highway)",
        "href": "https://automatum-data.com/#dataset",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "12",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x DJI Mavic Mini with 2K camera resolution",
        "recordingPerspective": "top view",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "2d bounding box",
        "licensing": "Creative Commons Attribution-NonDerivative License (CC BY-ND)",
        "relatedDatasets": "Highways Ramps, Highway ALKS",
        "publishDate": "2021-07-11",
        "lastUpdate": "-",
        "paperTitle": "AUTOMATUM DATA: Drone-based highway dataset for the development and validation of automated driving software for research and commercial applications",
        "relatedPaper": "https://automatumdata.blob.core.windows.net/opendataset/IV21_Automatum.Data.eng.pdf",
        "location": "Germany",
        "rawData": "-",
        "DOI": "10.1109/IV48863.2021.9575442",
        "citationCount": 1,
        "completionStatus": "complete"
    },
    {
        "id": "OpenMPD",
        "href": "http://openmpd.com/",
        "relatedPaper": "https://ieeexplore.ieee.org/document/9682587",
        "DOI": "10.1109/TVT.2022.3143173",
        "citationCount": 1,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Gated2Gated",
        "href": "https://github.com/princeton-computational-imaging/Gated2Gated#gated2gated--self-supervised-depth-estimation-from-gated-images",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "130000",
        "numberOfScenes": "1835",
        "samplingRate": "10",
        "lengthOfScenes": "-",
        "sensors": "camera, radar, lidar, imu, weather sensor",
        "sensorDetail": "2x stereo cameras 1920x1024 30Hz, 1 gated camera 1280x720 120Hz, 1 FMCW radar 15Hz, 2x Velodyne lidars 10Hz, 1 FIR camera 640x480 30Hz, 1 Airmar WX150 weather sensor (temperature, wind speed and humidity)",
        "recordingPerspective": "Ego-perspective",
        "dataType": "real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Freely available for research and teaching purposes",
        "relatedDatasets": "SeeingThroughFog, PointCloudDeNoising, Gated2Depth",
        "publishDate": "2021-12-04",
        "lastUpdate": "-",
        "paperTitle": "Gated2Gated: Self-Supervised Depth Estimation from Gated Images",
        "relatedPaper": "https://arxiv.org/pdf/2112.02416.pdf",
        "location": "Germany, Sweden, Denmark and Finland",
        "rawData": "-",
        "DOI": "10.48550/arXiv.2112.02416",
        "citationCount": 1,
        "completionStatus": "complete"
    },
    {
        "id": "WildDash 2",
        "href": "https://wilddash.cc/",
        "relatedPaper": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zendel_Unifying_Panoptic_Segmentation_for_Autonomous_Driving_CVPR_2022_paper.pdf",
        "citationCount": 0,
        "completionStatus": "incomplete"
    },
    {
        "id": "PandaSet",
        "href": "https://pandaset.org/",
        "size_hours": "0.23",
        "size_storage": "-",
        "frames": "48000",
        "numberOfScenes": "103",
        "samplingRate": "-",
        "lengthOfScenes": "8",
        "sensors": "camera, lidar, gps/imu",
        "sensorDetail": "5x wide angle cameras 1920x1080 10Hz, 1x long focus camera 1920x1080 10Hz, 1x mechanical spinning LiDAR 64 channels 360° 10Hz, 1x forward-facing LiDAR 150 channels 60° 10Hz1x mechanical spinning LiDAR, 1x forward-facing LiDAR, 6x cameras, on-board GPS/IMU",
        "benchmark": "-",
        "annotations": "3d bounding boxes, attributes, point cloud segmentation ",
        "licensing": "Creative Commons Attribution 4.0 International Public (CC BY 4.0)",
        "relatedDatasets": "-",
        "publishDate": "2020-04-01",
        "lastUpdate": "-",
        "location": "San Francisco and El Camina Real, USA",
        "rawData": "Yes",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Comma2k19 LD",
        "href": "https://github.com/ASGuard-UCI/ld-metric",
        "relatedPaper": "https://openaccess.thecvf.com/content/CVPR2022/papers/Sato_Towards_Driving-Oriented_Metric_for_Lane_Detection_Models_CVPR_2022_paper.pdf",
        "citationCount": 0,
        "completionStatus": "incomplete"
    },
    {
        "id": "Udacity",
        "href": "https://github.com/udacity/self-driving-car/",
        "size_storage": "223",
        "size_hours": "10",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu",
        "sensorDetail": "monocular color camera 1920x1200, velodyne 32 lidar, gps/imu",
        "benchmark": "-",
        "annotations": "2d bounding boxes",
        "licensing": "MIT",
        "relatedDatasets": "-",
        "publishDate": "2016-09-01",
        "lastUpdate": "-",
        "location": "-",
        "rawData": "True",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "WZ-traffic dataset",
        "href": "https://github.com/Fangyu0505/traffic-scene-recognition",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Nighttime Driving",
        "href": "http://people.ee.ethz.ch/~daid/NightDriving/",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "AIODrive",
        "href": "http://www.aiodrive.org/overview.html",
        "size_hours": "-",
        "size_storage": "3041.71",
        "frames": "100000",
        "numberOfScenes": "100",
        "samplingRate": "-",
        "lengthOfScenes": "100",
        "sensors": "camera, lidar, radar, gps/imu",
        "sensorDetail": "5x color cameras 1920x720 10Hz, 5x depth cameras 1920x720 10Hz, 3x lidar 64/800/1200 channels 360° 10Hz, 1x SPAD-LiDAR, 4x radar 360° 10Hz, 1x gps/imu 10Hz",
        "recordingPerspective": "Bird’s Eye",
        "dataType": "Real",
        "benchmark": "3d object detection, trajectory forecasting",
        "annotations": "2d/3d bounding boxes, object category and attributes, 2d-3d semantic, instance and panoptic segmentation",
        "licensing": "freely available for both commercial and non-commercial purposes",
        "relatedDatasets": "-",
        "publishDate": "2021-04-06",
        "lastUpdate": "-",
        "paperTitle": "All-In-One Drive: A Comprehensive Perception Dataset with High-Density Long-Range Point Clouds",
        "relatedPaper": "https://www.xinshuoweng.com/papers/AIODrive/arXiv.pdf",
        "location": "CARLA environment",
        "rawData": "-",
        "DOI": "10.13140/RG.2.2.21621.81122",
        "citationCount": 0,
        "completionStatus": "complete"
    },
    {
        "id": "Steet Hazards Dataset",
        "href": "https://once-for-auto-driving.github.io/index.html",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "RoadObstacle21",
        "href": "https://segmentmeifyoucan.com/datasets",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Astyx Dataset",
        "href": "https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/astyx_dataset.html",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "A Parametric Top-View Representation of Complex Road Scenes",
        "href": "https://www.nec-labs.com/~mas/BEV/",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "VITRO",
        "href": "https://vitro-testing.com/test-data/dashcam-annotations/",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "UDrive Dataset",
        "href": "https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjqnfKIoK3uAhUOuaQKHZwcDEgQFjASegQIFBAC&url=https%3A%2F%2Ferticonetwork.com%2Fwp-content%2Fuploads%2F2017%2F12%2FUDRIVE-D41.1-UDrive-dataset-and-key-analysis-results-with-annotation-codebook.pdf&usg=AOvVaw17NgwnPrIal53hUYco9klG",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "PolySync Dataset",
        "href": "http://selfracingcars.com/blog/2016/7/26/polysync",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "DriveSeg (MANUAL)",
        "href": "https://agelab.mit.edu/driveseg",
        "size_storage": "2.98",
        "size_hours": "0.03",
        "frames": "5000",
        "numberOfScenes": "1",
        "samplingRate": "30",
        "lengthOfScenes": "167",
        "sensors": "camera",
        "sensorDetail": "1x FDR-AX53 camera 1080P 1920x1080 30Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "semantic segmentation",
        "licensing": "freely available to academic and nonacademic entities for non-commercial purposes",
        "relatedDatasets": "DriveSeg (Semi-auto)",
        "publishDate": "2020-04-06",
        "lastUpdate": "-",
        "paperTitle": "MIT DriveSeg (Manual) Dataset for Dynamic Driving Scene Segmentation",
        "relatedPaper": "https://ieee-dataport.s3.amazonaws.com/docs/25911/MIT_DriveSeg_Semiauto.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20220522%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220522T125535Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=7e2583baac0a4ad83560a6945835066bead73730d912e573c360d500a5802022",
        "location": "-",
        "rawData": "-",
        "DOI": "10.21227/mmke-dv03",
        "citationCount": 0,
        "completionStatus": "complete"
    },
    {
        "id": "DriveSeg (Semi-auto)",
        "href": "https://agelab.mit.edu/driveseg",
        "size_storage": "13.46",
        "size_hours": "0.186",
        "frames": "20100",
        "numberOfScenes": "67",
        "samplingRate": "30",
        "lengthOfScenes": "10",
        "sensors": "camera",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "pixel-wise semantic annotation",
        "licensing": "-",
        "relatedDatasets": "DriveSeg (MANUAL), MIT-AVT Clustered Driving Scene Dataset",
        "publishDate": "2020-04-06",
        "lastUpdate": "-",
        "paperTitle": "MIT DriveSeg (Semi-auto) Dataset: Large-scale Semi-automated Annotation of Semantic Driving Scenes",
        "relatedPaper": "https://ieee-dataport.s3.amazonaws.com/docs/25911/MIT_DriveSeg_Semiauto.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20220522%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220522T125535Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=7e2583baac0a4ad83560a6945835066bead73730d912e573c360d500a5802022",
        "location": "-",
        "rawData": "-",
        "DOI": "10.21227/nb3n-kk46",
        "citationCount": 0,
        "completionStatus": "complete"
    },
    {
        "id": "KUL Belgium Traffic Sign dataset",
        "href": "https://people.ee.ethz.ch/~timofter/traffic_signs/",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "comma10k",
        "href": "https://github.com/commaai/comma10k",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "SemanticPOSS",
        "href": "http://www.poss.pku.edu.cn/semanticposs.html",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "SemanticUSL",
        "href": "https://unmannedlab.github.io/semanticusl",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "ELEKTRA",
        "href": "http://adas.cvc.uab.es/elektra/datasets/",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "LUMPI",
        "href": "https://data.uni-hannover.de/ne/dataset/lumpi",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Highway Work Zones",
        "href": "http://www.andrew.cmu.edu/user/jonghole/workzone/data/",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "NEXET",
        "href": "https://blog.getnexar.com/https-medium-com-itayklein-intro-nexet-50e9b596d0e5",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "RDD2020",
        "href": "https://data.mendeley.com/datasets/5ty2wb6gvg/2",
        "size_storage": "1.76",
        "size_hours": "-",
        "frames": "26336",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "2d bounding boxes",
        "licensing": "Attribution-NonCommercial 3.0 Unported (CC BY NC 3.0)",
        "relatedDatasets": "-",
        "publishDate": "2021-03-18",
        "lastUpdate": "2021-09-27",
        "paperTitle": "RDD2020: An annotated image dataset for automatic road damage detection using deep learning",
        "relatedPaper": "https://www.sciencedirect.com/science/article/pii/S2352340921004170#bib0001",
        "location": "India, Japan and Czech Republic",
        "rawData": "-",
        "DOI": "j.dib.2021.107133",
        "citationCount": 0,
        "completionStatus": "complete"
    },
    {
        "id": "CarlaScenes",
        "href": "https://github.com/CarlaScenes/CarlaSence",
        "relatedPaper": "https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Kloukiniotis_CarlaScenes_A_Synthetic_Dataset_for_Odometry_in_Autonomous_Driving_CVPRW_2022_paper.pdf",
        "DOI": "10.1109/OJITS.2022.3142612",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "K-Lane",
        "href": "https://github.com/kaist-avelab/k-lane",
        "relatedPaper": "https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Paek_K-Lane_Lidar_Lane_Dataset_and_Benchmark_for_Urban_Roads_and_CVPRW_2022_paper.pdf",
        "citationCount": 0,
        "completionStatus": "incomplete"
    },
    {
        "id": "Ithaca365",
        "href": "https://ithaca365.mae.cornell.edu/",
        "relatedPaper": "https://openaccess.thecvf.com/content/CVPR2022/papers/Diaz-Ruiz_Ithaca365_Dataset_and_Driving_Perception_Under_Repeated_and_Challenging_Weather_CVPR_2022_paper.pdf",
        "citationCount": 0,
        "completionStatus": "incomplete"
    },
    {
        "id": "TuSimple",
        "href": "https://www.tusimple.com/",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Astyx HiRes 2019",
        "href": "https://www.astyx.com/fileadmin/redakteur/dokumente/Astyx_Dataset_HiRes2019_specification.pdf",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Daimler Urban Segmentation",
        "href": "https://computervisiononline.com/dataset/1105138608",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Toronto City",
        "href": "http://www.cs.toronto.edu/~byang/papers/Tcity_iccv17.pdf",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "CrashD",
        "href": "https://crashd-cars.github.io/",
        "relatedPaper": "https://arxiv.org/abs/2112.04764",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Raincouver",
        "href": "https://ieeexplore.ieee.org/document/7970170",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "CCSAD",
        "href": "https://www.researchgate.net/publication/277476726_Towards_Ubiquitous_Autonomous_Driving_The_CCSAD_Dataset",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Cheddar Gorge Dataset",
        "href": "https://www.researchgate.net/publication/228428941_The_Cheddar_Gorge_Data_Set",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "The Annotated Laser Dataset",
        "href": "https://journals.sagepub.com/doi/pdf/10.1177/0278364910389840",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "DDD 17",
        "href": "https://www.paperswithcode.com/dataset/ddd17",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "FLIR Thermal Dataset",
        "href": "https://www.flir.com/oem/adas/adas-dataset-form/",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Multispectral Object Detection",
        "href": "https://deepai.org/publication/multispectral-object-detection-with-deep-learning",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "CityPersons",
        "href": "https://arxiv.org/abs/1702.05693",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "ETH Pedestrian",
        "href": "https://paperswithcode.com/dataset/eth",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "RoadSaW",
        "href": "https://www.viscoda.com/index.php/downloads/roadsaw-dataset",
        "relatedPaper": "https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Cordes_RoadSaW_A_Large-Scale_Dataset_for_Camera-Based_Road_Surface_and_Wetness_CVPRW_2022_paper.pdf",
        "citationCount": 0,
        "completionStatus": "incomplete"
    },
    {
        "id": "Daimler Pedestrian",
        "href": "http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/daimler_pedestrian_benchmark_d.html",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "TTI Core",
        "href": "https://www.toyota-ti.ac.jp/Lab/Denshi/COIN/Ontology/TTICore-0.03/",
        "relatedPaper": "http://ceur-ws.org/Vol-1486/paper_9.pdf",
        "citationCount": 0,
        "completionStatus": "incomplete"
    },
    {
        "id": "exiD",
        "href": "https://www.exid-dataset.com/",
        "size_storage": "-",
        "size_hours": "16.1",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "25",
        "lengthOfScenes": "-",
        "sensors": "camera",
        "sensorDetail": "1x DJI Matrice 200 Serie V2 equipped with a DJI Zenmuse gimbal camera 4096x2160 25Hz",
        "recordingPerspective": "top-view",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "2d bounding boxes for objects, trajectory tracking information",
        "licensing": "Freely available for non-commercial use only upon registration",
        "relatedDatasets": "highD, inD, rounD and uniD datasets",
        "publishDate": "2022-06-04",
        "lastUpdate": "-",
        "paperTitle": "The exiD Dataset: A Real-World Trajectory Dataset of Highly Interactive Highway Scenarios in Germany",
        "relatedPaper": "https://ieeexplore.ieee.org/document/9827305",
        "location": "Germany",
        "rawData": "-",
        "DOI": "10.1109/IV51971.2022.9827305",
        "citationCount": 0,
        "completionStatus": "complete"
    },
    {
        "id": "K-Radar",
        "href": "https://github.com/kaist-avelab/K-Radar",
        "relatedPaper": "https://arxiv.org/abs/2206.08171",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "Road Scene Graph",
        "href": "https://github.com/TianYafu/road-status-graph-dataset",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "R3 Driving Dataset",
        "href": "https://github.com/rllab-snu/R3-Driving-Dataset",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "62755",
        "numberOfScenes": "382",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar, gps/imu",
        "sensorDetail": "1x Mobileye ELD camera, 3x Velodyne VLP-16 Hi-Res lidar, 1x Inertial Lab INS D gnss/imu sensor",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Complete dataset available on request",
        "relatedDatasets": "-",
        "publishDate": "2021-09-16",
        "lastUpdate": "-",
        "paperTitle": "Towards Defensive Autonomous Driving: Collecting and Probing Driving Demonstrations of Mixed Qualities",
        "relatedPaper": "https://arxiv.org/pdf/2109.07995.pdf",
        "location": "Gwanak (Seoul), Siheung, and Incheon, Korea",
        "rawData": "-",
        "DOI": "10.48550/arXiv.2109.07995",
        "citationCount": 0,
        "completionStatus": "complete"
    },
    {
        "id": "EISATS",
        "href": "https://ccv.wordpress.fos.auckland.ac.nz/eisats/",
        "size_storage": "",
        "size_hours": "",
        "frames": "",
        "numberOfScenes": "",
        "samplingRate": "",
        "lengthOfScenes": "",
        "sensors": "",
        "sensorDetail": "",
        "benchmark": "",
        "annotations": "",
        "licensing": "",
        "relatedDatasets": "",
        "publishDate": " ",
        "lastUpdate": "",
        "relatedPaper": "",
        "citationCount": 0,
        "completionStatus": "incomplete"
    },
    {
        "id": "uniD",
        "href": "https://www.unid-dataset.com/",
        "size_storage": "",
        "size_hours": "",
        "frames": "",
        "numberOfScenes": "",
        "samplingRate": "",
        "lengthOfScenes": "",
        "sensors": "",
        "sensorDetail": "",
        "benchmark": "",
        "annotations": "",
        "licensing": "",
        "relatedDatasets": "",
        "publishDate": " ",
        "lastUpdate": "",
        "relatedPaper": "",
        "citationCount": 0,
        "completionStatus": "incomplete"
    },
    {
        "id": "Argoverse 2 Sensor Dataset",
        "href": "https://www.argoverse.org/av2.html",
        "size_storage": "1000",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "1000",
        "samplingRate": "10",
        "lengthOfScenes": "15",
        "sensors": "camera, lidar, gps",
        "sensorDetail": "7x high-resolution ring cameras 2048x1550 combined 360° 20Hz, 2x front-view facing stereo cameras 20Hz, 2x VLP-32C lidar sensors 360° 10Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "3d cuboids/bounding boxes around objects",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (“CC BY-NC-SA 4.0”)",
        "relatedDatasets": "Argoverse 1 and Argoverse 2 series of datasets",
        "publishDate": "2021-08-21",
        "lastUpdate": "-",
        "paperTitle": "Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting",
        "relatedPaper": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/4734ba6f3de83d861c3176a6273cac6d-Paper-round2.pdf",
        "location": "Austin, Detroit, Miami, Palo Alto, Pittsburgh, and Washington D.C, USA",
        "rawData": "Yes",
        "DOI": "-",
        "citationCount": 0,
        "completionStatus": "complete"
    },
    {
        "id": "Argoverse 2 Motion Forecasting Dataset",
        "href": "https://www.argoverse.org/av2.html",
        "size_storage": "58",
        "size_hours": "763",
        "frames": "-",
        "numberOfScenes": "250000",
        "samplingRate": "10",
        "lengthOfScenes": "11",
        "sensors": "camera, lidar, gps",
        "sensorDetail": "7x high-resolution ring cameras 2048x1550 combined 360° 20Hz, 2x front-view facing stereo cameras 20Hz, 2x VLP-32C lidar sensors 360° 10Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "trajectory tracking information",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (“CC BY-NC-SA 4.0”)",
        "relatedDatasets": "Argoverse 1 and Argoverse 2 series of datasets",
        "publishDate": "2021-08-21",
        "lastUpdate": "-",
        "paperTitle": "Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting",
        "relatedPaper": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/4734ba6f3de83d861c3176a6273cac6d-Paper-round2.pdf",
        "location": "Pittsburgh and Miami, USA",
        "rawData": "Yes",
        "DOI": "-",
        "citationCount": 0,
        "completionStatus": "complete"
    },
    {
        "id": "Argoverse 2 Lidar Dataset",
        "href": "https://www.argoverse.org/av2.html",
        "size_storage": "5000",
        "size_hours": "-",
        "frames": "6000000",
        "numberOfScenes": "20000",
        "samplingRate": "10",
        "lengthOfScenes": "30",
        "sensors": "camera, lidar, gps",
        "sensorDetail": "7x high-resolution ring cameras 2048x1550 combined 360° 20Hz, 2x front-view facing stereo cameras 20Hz, 2x VLP-32C lidar sensors 360° 10Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "-",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (“CC BY-NC-SA 4.0”)",
        "relatedDatasets": "Argoverse 1 and Argoverse 2 series of datasets",
        "publishDate": "2021-08-21",
        "lastUpdate": "-",
        "paperTitle": "Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting",
        "relatedPaper": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/4734ba6f3de83d861c3176a6273cac6d-Paper-round2.pdf",
        "location": "Austin, Detroit, Miami, Palo Alto, Pittsburgh, and Washington D.C, USA",
        "rawData": "Yes",
        "DOI": "-",
        "citationCount": 0,
        "completionStatus": "complete"
    },
    {
        "id": "Argoverse 2 Map Change Dataset",
        "href": "https://www.argoverse.org/av2.html",
        "size_storage": "1000",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "1000",
        "samplingRate": "-",
        "lengthOfScenes": "45",
        "sensors": "camera, lidar, gps",
        "sensorDetail": "7x high-resolution ring cameras 2048x1550 combined 360° 20Hz, 2x front-view facing stereo cameras 20Hz, 2x VLP-32C lidar sensors 360° 10Hz",
        "recordingPerspective": "ego-perspective",
        "dataType": "Real",
        "mapData": "Yes",
        "benchmark": "-",
        "annotations": "3D lane boundaries",
        "licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (“CC BY-NC-SA 4.0”)",
        "relatedDatasets": "Argoverse 1 and Argoverse 2 series of datasets",
        "publishDate": "2021-08-21",
        "lastUpdate": "-",
        "paperTitle": "Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting",
        "relatedPaper": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/4734ba6f3de83d861c3176a6273cac6d-Paper-round2.pdf",
        "location": "Austin, Detroit, Miami, Palo Alto, Pittsburgh and Washington, D.C, USA",
        "rawData": "Yes",
        "DOI": "-",
        "citationCount": 0,
        "completionStatus": "complete"
    },
    {
        "id": "Rope3D",
        "href": "https://thudair.baai.ac.cn/rope",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "50009",
        "numberOfScenes": "-",
        "samplingRate": "-",
        "lengthOfScenes": "-",
        "sensors": "camera, lidar",
        "sensorDetail": "1x HESAI Pandar 40P 40 laser beams lidar 360° 10/20Hz, 1x Jaguar Prime from Innovusion 300 beams lidar 6-20Hz, RGB cameras 1920x1080 30-60Hz",
        "recordingPerspective": "ego-perspective, bird's eye",
        "dataType": "Real",
        "mapData": "-",
        "benchmark": "-",
        "annotations": "2D and 3D bounding boxes of the obstacle objects as well as their category attributes, occlusions, and truncated states",
        "licensing": "Available upon registration",
        "relatedDatasets": "DAIR-V2X",
        "publishDate": "2022-03-25",
        "lastUpdate": "-",
        "paperTitle": "Rope3D: The Roadside Perception Dataset for Autonomous Driving and Monocular 3D Object Detection",
        "relatedPaper": "https://arxiv.org/pdf/2203.13608.pdf",
        "location": "China",
        "rawData": "-",
        "DOI": "10.48550/arXiv.2203.13608",
        "citationCount": 0,
        "completionStatus": "complete"
    },
    {
        "id": "AugKITTI",
        "relatedPaper": "https://arxiv.org/pdf/2203.00214.pdf",
        "citationCount": 0,
        "completionStatus": "incomplete"
    },
    {
        "id": "On Salience-Sensitive Sign Classification in Autonomous Vehicle Path Planning",
        "relatedPaper": "https://arxiv.org/pdf/2112.00942.pdf",
        "citationCount": 0,
        "completionStatus": "incomplete"
    },
    {
        "id": "HDBD",
        "href": "https://usa.honda-ri.com/hdbd",
        "size_storage": "-",
        "size_hours": "-",
        "frames": "-",
        "numberOfScenes": "-",
        "samplingRate": "10",
        "lengthOfScenes": "-",
        "sensors": "camera, eye tracker",
        "sensorDetail": "-",
        "recordingPerspective": "ego-perspective",
        "dataType": "Synthetic",
        "mapData": "No",
        "benchmark": "-",
        "annotations": "semantic segmentation of images, driver gaze map, vehice speed, throtle and steering data",
        "licensing": "Available for research for people affiliated with a university upon registration",
        "relatedDatasets": "HAD, HDD, HEV-I, HSD, LOKI, TITAN and EPOSH",
        "publishDate": "2022-05-23",
        "lastUpdate": "-",
        "paperTitle": "Incorporating Gaze Behavior Using Joint Embedding With Scene Context for Driver Takeover Detection",
        "relatedPaper": "https://ieeexplore.ieee.org/document/9747779",
        "location": "Unreal Engine",
        "rawData": "-",
        "DOI": "10.1109/ICASSP43922.2022.9747779",
        "citationCount": 0,
        "completionStatus": "complete"
    },
    {
        "id": "TITAN",
        "href": "https://usa.honda-ri.com/titan",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    },
    {
        "id": "EPOSH",
        "href": "https://usa.honda-ri.com/eposh",
        "citationCount": 0,
        "completionStatus": "partially Complete"
    }
]